"""
ç»Ÿä¸€æ•°æ®åº“åˆå§‹åŒ–å™¨

å‚è€ƒ emby-toolkit é¡¹ç›®çš„è®¾è®¡ç†å¿µï¼Œæ•´åˆä»¥ä¸‹åŠŸèƒ½ï¼š
1. è¡¨åˆ›å»ºï¼ˆåŸºäº SQLAlchemy ORM æ¨¡å‹ï¼‰
2. å­—æ®µå‡çº§ï¼ˆå£°æ˜å¼é…ç½® + è‡ªåŠ¨æ£€æµ‹ï¼‰
3. ç´¢å¼•ç®¡ç†ï¼ˆé›†ä¸­å£°æ˜å¼å®šä¹‰ï¼‰
4. åºŸå¼ƒå¯¹è±¡æ¸…ç†ï¼ˆä¸»åŠ¨æ¸…ç†è¿‡æ—¶çš„è¡¨å’Œå­—æ®µï¼‰

ä¼˜åŠ¿ï¼š
- å•ä¸€å…¥å£ï¼Œæ˜“äºç»´æŠ¤
- å£°æ˜å¼é…ç½®ï¼Œç»“æ„æ¸…æ™°
- å¹‚ç­‰æ€§æ“ä½œï¼Œå¯é‡å¤æ‰§è¡Œ
- å®Œæ•´çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†
"""

import logging
from typing import Dict, List, Set
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncConnection

from .orm_models import Base
from .db_maintainer import sync_database_schema
from .migrations import run_migrations

logger = logging.getLogger(__name__)


# ============================================================
# ğŸ“Š å£°æ˜å¼é…ç½®åŒºåŸŸ
# ============================================================

# ğŸ”¹ å­—æ®µå‡çº§é…ç½®ï¼ˆå¯é€‰ï¼Œä¸ db_maintainer é…åˆä½¿ç”¨ï¼‰
# æ ¼å¼: {'è¡¨å': {'å­—æ®µå': 'å­—æ®µç±»å‹'}}
SCHEMA_UPGRADES: Dict[str, Dict[str, str]] = {
    # ç¤ºä¾‹ï¼ˆå·²ç”± db_maintainer è‡ªåŠ¨å¤„ç†ï¼Œè¿™é‡Œä»…ä½œå¤‡ä»½è®°å½•ï¼‰:
    # 'anime': {
    #     'new_field': 'VARCHAR(255)',
    # },
}

# ğŸ”¹ åºŸå¼ƒè¡¨æ¸…ç†é…ç½®
# æ ¼å¼: ['è¡¨å1', 'è¡¨å2']
DEPRECATED_TABLES: List[str] = [
    # ç¤ºä¾‹ï¼š
    # 'old_cache_table',  # å·²è¢« cache_data è¡¨æ›¿ä»£
    # 'legacy_logs',      # å·²è¿ç§»åˆ°æ–°çš„æ—¥å¿—ç³»ç»Ÿ
]

# ğŸ”¹ åºŸå¼ƒå­—æ®µæ¸…ç†é…ç½®
# æ ¼å¼: {'è¡¨å': ['å­—æ®µå1', 'å­—æ®µå2']}
DEPRECATED_COLUMNS: Dict[str, List[str]] = {
    # ç¤ºä¾‹ï¼š
    # 'anime': [
    #     'old_status_field',  # å·²è¢« new_status æ›¿ä»£
    # ],
    # 'api_tokens': [
    #     'legacy_permissions',  # å·²è¿ç§»åˆ°æ–°çš„æƒé™ç³»ç»Ÿ
    # ],
}

# ğŸ”¹ ç´¢å¼•ç®¡ç†é…ç½®
# æ ¼å¼: ['CREATE INDEX IF NOT EXISTS ...', ...]
# æ³¨æ„ï¼šåŸºç¡€ç´¢å¼•å·²åœ¨ ORM æ¨¡å‹ä¸­å®šä¹‰ï¼Œè¿™é‡Œåªæ·»åŠ é¢å¤–çš„ä¼˜åŒ–ç´¢å¼•
ADDITIONAL_INDEXES: List[str] = [
    # ç¤ºä¾‹ï¼šå¤åˆç´¢å¼•ã€é™åºç´¢å¼•ã€éƒ¨åˆ†ç´¢å¼•ç­‰
    # MySQL ç¤ºä¾‹ï¼š
    # "CREATE INDEX IF NOT EXISTS idx_anime_year_type ON anime(year, type)",
    # "CREATE INDEX IF NOT EXISTS idx_episodes_anime_index ON episodes(anime_id, episode_index)",

    # PostgreSQL ç‰¹æœ‰ç¤ºä¾‹ï¼ˆGIN ç´¢å¼•ç”¨äº JSONBï¼‰ï¼š
    # "CREATE INDEX IF NOT EXISTS idx_metadata_json_gin ON anime_metadata USING GIN(metadata_json)",
]


# ============================================================
# ğŸ”§ æ ¸å¿ƒåˆå§‹åŒ–å‡½æ•°
# ============================================================

async def init_database_schema(conn: AsyncConnection, db_type: str, db_name: str):
    """
    ã€ç»Ÿä¸€æ•°æ®åº“åˆå§‹åŒ–å…¥å£ã€‘

    æŒ‰ä»¥ä¸‹é¡ºåºæ‰§è¡Œæ‰€æœ‰æ•°æ®åº“åˆå§‹åŒ–ä»»åŠ¡ï¼š
    1. åŸºäº ORM æ¨¡å‹åˆ›å»ºæ‰€æœ‰è¡¨
    2. è‡ªåŠ¨æ£€æµ‹å¹¶è¡¥å……ç¼ºå¤±çš„å­—æ®µï¼ˆdb_maintainerï¼‰
    3. æ‰§è¡Œéœ€è¦æ•°æ®è½¬æ¢çš„å¤æ‚è¿ç§»ï¼ˆmigrationsï¼‰
    4. åˆ›å»ºé¢å¤–çš„ä¼˜åŒ–ç´¢å¼•
    5. æ¸…ç†åºŸå¼ƒçš„è¡¨å’Œå­—æ®µ

    Args:
        conn: æ•°æ®åº“è¿æ¥
        db_type: æ•°æ®åº“ç±»å‹ ('mysql' æˆ– 'postgresql')
        db_name: æ•°æ®åº“åç§°
    """
    logger.info("="*60)
    logger.info("å¼€å§‹æ•°æ®åº“åˆå§‹åŒ–æµç¨‹...")
    logger.info("="*60)

    # âœ… é˜¶æ®µ 1: åˆ›å»ºæ‰€æœ‰åŸºäº ORM æ¨¡å‹çš„è¡¨
    logger.info("ğŸ“‹ [é˜¶æ®µ 1/5] æ­£åœ¨åŒæ­¥ ORM æ¨¡å‹ï¼Œåˆ›å»ºæ–°è¡¨...")
    await conn.run_sync(Base.metadata.create_all)
    logger.info("âœ“ ORM æ¨¡å‹åŒæ­¥å®Œæˆã€‚")

    # âœ… é˜¶æ®µ 2: è‡ªåŠ¨æ£€æµ‹å¹¶è¡¥å……ç¼ºå¤±çš„å­—æ®µ
    logger.info("ğŸ” [é˜¶æ®µ 2/5] æ­£åœ¨æ£€æµ‹å¹¶è¡¥å……ç¼ºå¤±çš„å­—æ®µ...")
    await sync_database_schema(conn, db_type)
    logger.info("âœ“ å­—æ®µåŒæ­¥å®Œæˆã€‚")

    # âœ… é˜¶æ®µ 3: æ‰§è¡Œå¤æ‚çš„æ•°æ®è¿ç§»ä»»åŠ¡
    logger.info("ğŸ”„ [é˜¶æ®µ 3/5] æ­£åœ¨æ‰§è¡Œæ•°æ®åº“è¿ç§»ä»»åŠ¡...")
    await run_migrations(conn, db_type, db_name)
    logger.info("âœ“ è¿ç§»ä»»åŠ¡å®Œæˆã€‚")

    # âœ… é˜¶æ®µ 4: åˆ›å»ºé¢å¤–çš„ä¼˜åŒ–ç´¢å¼•
    logger.info("ğŸ“Š [é˜¶æ®µ 4/5] æ­£åœ¨åˆ›å»ºé¢å¤–çš„ä¼˜åŒ–ç´¢å¼•...")
    await _create_additional_indexes(conn, db_type)
    logger.info("âœ“ ç´¢å¼•åˆ›å»ºå®Œæˆã€‚")

    # âœ… é˜¶æ®µ 5: æ¸…ç†åºŸå¼ƒçš„è¡¨å’Œå­—æ®µ
    logger.info("ğŸ§¹ [é˜¶æ®µ 5/5] æ­£åœ¨æ¸…ç†åºŸå¼ƒçš„æ•°æ®åº“å¯¹è±¡...")
    await _cleanup_deprecated_objects(conn, db_type)
    logger.info("âœ“ æ¸…ç†å®Œæˆã€‚")

    logger.info("="*60)
    logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–æµç¨‹å…¨éƒ¨å®Œæˆï¼")
    logger.info("="*60)


# ============================================================
# ğŸ› ï¸ è¾…åŠ©å‡½æ•°
# ============================================================

async def _create_additional_indexes(conn: AsyncConnection, db_type: str):
    """åˆ›å»ºé¢å¤–çš„ä¼˜åŒ–ç´¢å¼•"""
    if not ADDITIONAL_INDEXES:
        logger.info("   æ— éœ€åˆ›å»ºé¢å¤–ç´¢å¼•ã€‚")
        return

    created_count = 0
    for index_sql in ADDITIONAL_INDEXES:
        try:
            await conn.execute(text(index_sql))
            # æå–ç´¢å¼•åç”¨äºæ—¥å¿—
            index_name = index_sql.split("IF NOT EXISTS")[1].split("ON")[0].strip() if "IF NOT EXISTS" in index_sql else "æœªçŸ¥"
            logger.info(f"   âœ“ åˆ›å»ºç´¢å¼•: {index_name}")
            created_count += 1
        except Exception as e:
            logger.warning(f"   âš ï¸ åˆ›å»ºç´¢å¼•å¤±è´¥ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")

    logger.info(f"   æˆåŠŸåˆ›å»º {created_count} ä¸ªç´¢å¼•ã€‚")




async def _cleanup_deprecated_objects(conn: AsyncConnection, db_type: str):
    """æ¸…ç†åºŸå¼ƒçš„è¡¨å’Œå­—æ®µ"""
    cleanup_stats = {
        'tables_dropped': 0,
        'columns_dropped': 0,
        'warnings': 0
    }

    # æ¸…ç†åºŸå¼ƒçš„è¡¨
    if DEPRECATED_TABLES:
        logger.info(f"   æ­£åœ¨æ¸…ç† {len(DEPRECATED_TABLES)} ä¸ªåºŸå¼ƒè¡¨...")
        for table_name in DEPRECATED_TABLES:
            try:
                if await _check_table_exists(conn, db_type, table_name):
                    # ä½¿ç”¨ CASCADE ç¡®ä¿æ¸…ç†å¤–é”®å…³è”
                    await conn.execute(text(f"DROP TABLE IF EXISTS {table_name} CASCADE"))
                    logger.info(f"   âœ“ åˆ é™¤åºŸå¼ƒè¡¨: {table_name}")
                    cleanup_stats['tables_dropped'] += 1
            except Exception as e:
                logger.error(f"   âœ— åˆ é™¤è¡¨ {table_name} å¤±è´¥: {e}")
                cleanup_stats['warnings'] += 1

    # æ¸…ç†åºŸå¼ƒçš„å­—æ®µ
    if DEPRECATED_COLUMNS:
        logger.info(f"   æ­£åœ¨æ¸…ç†åºŸå¼ƒå­—æ®µ...")
        for table_name, columns in DEPRECATED_COLUMNS.items():
            # å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            if not await _check_table_exists(conn, db_type, table_name):
                logger.warning(f"   âš ï¸ è¡¨ {table_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡å­—æ®µæ¸…ç†ã€‚")
                continue

            for column_name in columns:
                try:
                    # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
                    if await _check_column_exists(conn, db_type, table_name, column_name):
                        await conn.execute(text(f"ALTER TABLE {table_name} DROP COLUMN IF EXISTS {column_name}"))
                        logger.info(f"   âœ“ åˆ é™¤åºŸå¼ƒå­—æ®µ: {table_name}.{column_name}")
                        cleanup_stats['columns_dropped'] += 1
                except Exception as e:
                    logger.error(f"   âœ— åˆ é™¤å­—æ®µ {table_name}.{column_name} å¤±è´¥: {e}")
                    cleanup_stats['warnings'] += 1

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    if cleanup_stats['tables_dropped'] > 0 or cleanup_stats['columns_dropped'] > 0:
        logger.info(f"   æ¸…ç†ç»Ÿè®¡: åˆ é™¤äº† {cleanup_stats['tables_dropped']} ä¸ªè¡¨, {cleanup_stats['columns_dropped']} ä¸ªå­—æ®µã€‚")
    else:
        logger.info("   æ— éœ€æ¸…ç†ä»»ä½•å¯¹è±¡ã€‚")

    if cleanup_stats['warnings'] > 0:
        logger.warning(f"   æ¸…ç†è¿‡ç¨‹ä¸­æœ‰ {cleanup_stats['warnings']} ä¸ªè­¦å‘Šã€‚")


async def _check_table_exists(conn: AsyncConnection, db_type: str, table_name: str) -> bool:
    """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
    if db_type == "mysql":
        sql = text("SELECT 1 FROM information_schema.tables WHERE table_schema = DATABASE() AND table_name = :table_name")
    else:  # postgresql
        sql = text("SELECT 1 FROM information_schema.tables WHERE table_schema = current_schema() AND table_name = :table_name")

    result = await conn.execute(sql, {"table_name": table_name})
    return result.scalar_one_or_none() is not None


async def _check_column_exists(conn: AsyncConnection, db_type: str, table_name: str, column_name: str) -> bool:
    """æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨"""
    if db_type == "mysql":
        sql = text("""
            SELECT 1 FROM information_schema.columns
            WHERE table_schema = DATABASE()
            AND table_name = :table_name
            AND column_name = :column_name
        """)
    else:  # postgresql
        sql = text("""
            SELECT 1 FROM information_schema.columns
            WHERE table_schema = current_schema()
            AND table_name = :table_name
            AND column_name = :column_name
        """)

    result = await conn.execute(sql, {"table_name": table_name, "column_name": column_name})
    return result.scalar_one_or_none() is not None


# ============================================================
# ğŸ“ ä½¿ç”¨ç¤ºä¾‹
# ============================================================
"""
åœ¨ main.py ä¸­ç®€åŒ–è°ƒç”¨ï¼š

# ä¿®æ”¹å‰ï¼ˆåˆ†æ•£åœ¨å¤šä¸ªæ­¥éª¤ï¼‰:
async with engine.begin() as conn:
    await conn.run_sync(Base.metadata.create_all)
    await sync_database_schema(conn, db_type)
    await run_migrations(conn, db_type, db_name)

# ä¿®æ”¹åï¼ˆç»Ÿä¸€å…¥å£ï¼‰:
async with engine.begin() as conn:
    from .database_initializer import init_database_schema
    await init_database_schema(conn, db_type, db_name)
"""

