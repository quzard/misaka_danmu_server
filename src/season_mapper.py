"""å­£åº¦æ˜ å°„æ¨¡å— - é€šè¿‡å…ƒæ•°æ®æºè·å–å­£åº¦åç§°"""
import asyncio
import hashlib
import logging
import re
from typing import Optional, List, Any

from pydantic import Field
from . import models, crud
from .ai.ai_prompts import SEASON_KEYWORDS

logger = logging.getLogger(__name__)


def calculate_similarity(str1: str, str2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ (0-100)
    ä½¿ç”¨ thefuzz åº“çš„å¤šç§ç®—æ³•ç»¼åˆè¯„åˆ†

    Args:
        str1: ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²
        str2: ç¬¬äºŒä¸ªå­—ç¬¦ä¸²

    Returns:
        ç›¸ä¼¼åº¦ç™¾åˆ†æ¯” (0-100)
    """
    if not str1 or not str2:
        return 0.0

    from thefuzz import fuzz

    # è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
    s1 = str1.lower().strip()
    s2 = str2.lower().strip()

    # ä½¿ç”¨å¤šç§ç®—æ³•è®¡ç®—ç›¸ä¼¼åº¦,å–æœ€é«˜å€¼
    # 1. ç®€å•ç›¸ä¼¼åº¦ - é€‚åˆå®Œå…¨åŒ¹é…
    simple_ratio = fuzz.ratio(s1, s2)

    # 2. éƒ¨åˆ†ç›¸ä¼¼åº¦ - é€‚åˆå­ä¸²åŒ¹é… (å¦‚ "æ— é™åˆ—è½¦ç¯‡" åœ¨ "é¬¼ç­ä¹‹åˆƒ æ— é™åˆ—è½¦ç¯‡" ä¸­)
    partial_ratio = fuzz.partial_ratio(s1, s2)

    # 3. Tokenæ’åºç›¸ä¼¼åº¦ - å¿½ç•¥è¯åº (å¦‚ "é¬¼ç­ä¹‹åˆƒ æ— é™åˆ—è½¦ç¯‡" vs "æ— é™åˆ—è½¦ç¯‡ é¬¼ç­ä¹‹åˆƒ")
    token_sort_ratio = fuzz.token_sort_ratio(s1, s2)

    # 4. Tokené›†åˆç›¸ä¼¼åº¦ - å¿½ç•¥é‡å¤è¯å’Œè¯åº
    token_set_ratio = fuzz.token_set_ratio(s1, s2)

    # å–æœ€é«˜åˆ†
    max_similarity = max(simple_ratio, partial_ratio, token_sort_ratio, token_set_ratio)

    return float(max_similarity)


def title_contains_season_name(title: str, season_number: int, season_name: str, season_aliases: List[str] = None, threshold: float = 60.0) -> float:
    """
    åˆ¤æ–­æ ‡é¢˜æ˜¯å¦åŒ…å«å­£åº¦åç§°å¹¶è®¡ç®—ç›¸ä¼¼åº¦
    ä½¿ç”¨å¤šç§ç­–ç•¥è¿›è¡ŒåŒ¹é…,é€‚åˆä¸­æ–‡åŠ¨æ¼«æ ‡é¢˜

    Args:
        title: æœç´¢ç»“æœæ ‡é¢˜ (å¦‚ "é¬¼ç­ä¹‹åˆƒ æ— é™åˆ—è½¦ç¯‡")
        season_number: å­£åº¦ç¼–å· (å¦‚ 2)
        season_name: å­£åº¦åç§° (å¦‚ "æ— é™åˆ—è½¦ç¯‡", "ç¬¬2å­£ æ— é™åˆ—è½¦ç¯‡")
        season_aliases: å­£åº¦åˆ«ååˆ—è¡¨ (å¦‚ ["æ— é™åˆ—è½¦ç¯‡", "Mugen Train Arc"])
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (é»˜è®¤60%)

    Returns:
        ç›¸ä¼¼åº¦ç™¾åˆ†æ¯” (0-100), å¦‚æœä¸åŒ¹é…è¿”å›0.0
    """
    if not title or not season_name:
        return 0.0

    from thefuzz import fuzz

    title_lower = title.lower().strip()
    season_name_lower = season_name.lower().strip()

    # åˆå§‹åŒ–æœ€é«˜ç›¸ä¼¼åº¦
    max_similarity = 0.0

    # ç­–ç•¥1: ç›´æ¥å­ä¸²åŒ…å« (æœ€ç²¾ç¡®)
    if season_name_lower in title_lower:
        max_similarity = max(max_similarity, 95.0)

    # ç­–ç•¥2: ç§»é™¤å¸¸è§å‰ç¼€ååŒ…å«
    # ç§»é™¤ "ç¬¬Xå­£"ã€"Season X"ã€"S0X" ç­‰å‰ç¼€
    season_name_cleaned = re.sub(r'^(ç¬¬\d+å­£|season\s*\d+|s\d+)\s*', '', season_name_lower, flags=re.IGNORECASE)
    if season_name_cleaned and season_name_cleaned in title_lower:
        max_similarity = max(max_similarity, 90.0)

    # ç­–ç•¥3: éƒ¨åˆ†åŒ¹é… - ä½¿ç”¨ thefuzz çš„ partial_ratio
    # é€‚åˆ "æ— é™åˆ—è½¦ç¯‡" åœ¨ "é¬¼ç­ä¹‹åˆƒ æ— é™åˆ—è½¦ç¯‡" ä¸­çš„åœºæ™¯
    partial_similarity = fuzz.partial_ratio(season_name_cleaned or season_name_lower, title_lower)
    if partial_similarity >= 90:  # éƒ¨åˆ†åŒ¹é…è¦æ±‚æ›´é«˜çš„ç›¸ä¼¼åº¦
        max_similarity = max(max_similarity, float(partial_similarity))

    # ç­–ç•¥4: Token é›†åˆåŒ¹é… - æ£€æŸ¥å­£åº¦åç§°çš„å…³é”®è¯æ˜¯å¦éƒ½åœ¨æ ‡é¢˜ä¸­
    # ä¾‹å¦‚: "æ— é™åˆ—è½¦ç¯‡" çš„æ‰€æœ‰å­—ç¬¦éƒ½åœ¨ "é¬¼ç­ä¹‹åˆƒ æ— é™åˆ—è½¦ç¯‡" ä¸­
    token_set_similarity = fuzz.token_set_ratio(season_name_cleaned or season_name_lower, title_lower)
    if token_set_similarity >= threshold:
        max_similarity = max(max_similarity, float(token_set_similarity))

    # ç­–ç•¥5: åˆ†è¯åŒ¹é… - æ£€æŸ¥å­£åº¦åç§°çš„ä¸»è¦è¯æ±‡æ˜¯å¦åœ¨æ ‡é¢˜ä¸­
    # ä¾‹å¦‚: "æ— é™" "åˆ—è½¦" "ç¯‡" éƒ½åœ¨æ ‡é¢˜ä¸­
    # è¿‡æ»¤æ‰å•å­—å’Œå¸¸è§è¯
    common_words = {'ç¬¬', 'å­£', 'season', 's', 'çš„', 'ä¹‹', 'ä¸', 'å’Œ', 'the', 'and', 'or'}
    words = [w for w in re.split(r'\s+', season_name_cleaned or season_name_lower) if len(w) > 1 and w not in common_words]
    if words:
        # è‡³å°‘70%çš„å…³é”®è¯åœ¨æ ‡é¢˜ä¸­
        matched_words = sum(1 for word in words if word in title_lower)
        word_similarity = (matched_words / len(words)) * 100
        if word_similarity >= 70:
            max_similarity = max(max_similarity, word_similarity)

    # ç­–ç•¥6: å­£åº¦å·ç›´æ¥åŒ¹é…
    season_patterns = [
        rf'ç¬¬{season_number}å­£',
        rf'season\s*{season_number}',
        rf's{season_number}\b',
        rf'ç¬¬{season_number}éƒ¨',
        rf'part\s*{season_number}'
    ]
    for pattern in season_patterns:
        if re.search(pattern, title_lower, flags=re.IGNORECASE):
            max_similarity = max(max_similarity, 85.0)
            break

    # ç­–ç•¥7: åˆ«ååŒ¹é…
    if season_aliases:
        for alias in season_aliases:
            alias_similarity = fuzz.token_set_ratio(alias.lower(), title_lower)
            if alias_similarity >= threshold:
                max_similarity = max(max_similarity, float(alias_similarity))

    return max_similarity


class SeasonInfo(models.BaseModel):
    """é€šç”¨å­£åº¦ä¿¡æ¯æ¨¡å‹"""
    season_number: int
    name: Optional[str] = None
    episode_count: int = 0
    air_date: Optional[str] = None
    overview: Optional[str] = None
    aliases: Optional[List[str]] = Field(default=[], description="å­£åº¦åˆ«ååˆ—è¡¨")


class MetadataSearchCandidate(models.BaseModel):
    """å…ƒæ•°æ®æœç´¢å€™é€‰ç»“æœ"""
    source: str  # 'tmdb', 'tvdb', 'bangumi', etc.
    id: str  # æºçš„ID
    title: str
    original_title: Optional[str] = None
    year: Optional[int] = None
    media_type: str  # 'tv' or 'movie'
    overview: Optional[str] = None


class SeasonMapper:
    """å­£åº¦æ˜ å°„å™¨ - é€šè¿‡å…ƒæ•°æ®æºè·å–å­£åº¦åç§°"""
    
    def __init__(self, metadata_manager, session_factory):
        """
        åˆå§‹åŒ–å­£åº¦æ˜ å°„å™¨
        
        Args:
            metadata_manager: MetadataSourceManagerå®ä¾‹
            session_factory: æ•°æ®åº“ä¼šè¯å·¥å‚
        """
        self.metadata_manager = metadata_manager
        self._session_factory = session_factory
        self.logger = logger
    
    async def get_season_name(
        self,
        title: str,
        season_number: int,
        year: Optional[int] = None,
        sources: Optional[List[str]] = None,
        ai_matcher: Optional[Any] = None,
        user: Optional[models.User] = None,
        custom_prompt: Optional[str] = None
    ) -> Optional[str]:
        """
        é€šè¿‡å…ƒæ•°æ®æºè·å–æŒ‡å®šå­£åº¦çš„åç§°(é€šç”¨æ–¹æ³•)

        Args:
            title: å‰§é›†æ ‡é¢˜
            season_number: å­£åº¦ç¼–å·
            year: å¹´ä»½(å¯é€‰)
            sources: è¦æœç´¢çš„å…ƒæ•°æ®æºåˆ—è¡¨,Noneè¡¨ç¤ºä½¿ç”¨é»˜è®¤æº
            ai_matcher: AIåŒ¹é…å™¨(å¯é€‰)
            user: ç”¨æˆ·å¯¹è±¡
            custom_prompt: è‡ªå®šä¹‰AIæç¤ºè¯(å¯é€‰)

        Returns:
            å­£åº¦åç§°,å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
        """
        # æ£€æŸ¥ç¼“å­˜
        sources_str = "_".join(sources) if sources else "default"
        cache_key = f"season_name_{title}_{season_number}_{year or 'any'}_{sources_str}"
        async with self._session_factory() as session:
            cached_result = await crud.get_cache(session, cache_key)
            if cached_result:
                self.logger.info(f"å­£åº¦åç§°ç¼“å­˜å‘½ä¸­: {title} S{season_number:02d}")
                return cached_result.get("season_name")
        
        # ç¬¬1æ­¥: æœç´¢æ‰€æœ‰å…ƒæ•°æ®æº
        candidates = await self.search_all_metadata_sources(title, year, "tv", sources, user)
        if not candidates:
            self.logger.info(f"æœªæ‰¾åˆ°ä»»ä½•å…ƒæ•°æ®: {title}")
            return None
        
        # ç¬¬2æ­¥: ä½¿ç”¨AIé€‰æ‹©æœ€ä½³åŒ¹é…(å¦‚æœæœ‰AIåŒ¹é…å™¨)
        selected_candidate = None
        if ai_matcher and len(candidates) > 1:
            try:
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä¾›AIä½¿ç”¨
                candidates_dict = [c.model_dump() for c in candidates]
                selected_index = await ai_matcher.select_metadata_result(
                    title,
                    year,
                    candidates_dict,
                    season=season_number,
                    custom_prompt=custom_prompt
                )

                if selected_index is not None and 0 <= selected_index < len(candidates):
                    selected_candidate = candidates[selected_index]
                    self.logger.info(f"AIé€‰æ‹©å…ƒæ•°æ®: {selected_candidate.source}:{selected_candidate.id}")
            except Exception as e:
                self.logger.warning(f"AIé€‰æ‹©å…ƒæ•°æ®å¤±è´¥: {e}, ä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ")
        
        # å¦‚æœAIæœªé€‰æ‹©æˆ–åªæœ‰ä¸€ä¸ªå€™é€‰,ä½¿ç”¨ç¬¬ä¸€ä¸ª
        if not selected_candidate:
            selected_candidate = candidates[0]
            self.logger.info(f"ä½¿ç”¨ç¬¬ä¸€ä¸ªå…ƒæ•°æ®ç»“æœ: {selected_candidate.source}:{selected_candidate.id}")
        
        # ç¬¬3æ­¥: è·å–å­£åº¦åˆ—è¡¨
        seasons = await self.get_seasons_from_source(
            selected_candidate.source,
            selected_candidate.id,
            "tv"
        )
        if not seasons:
            return None
        
        # ç¬¬4æ­¥: æ‰¾åˆ°å¯¹åº”å­£åº¦
        target_season = None
        for season in seasons:
            if season.season_number == season_number:
                target_season = season
                break
        
        if not target_season or not target_season.name:
            return None
        
        # ç¼“å­˜ç»“æœ(7å¤©)
        async with self._session_factory() as session:
            await crud.set_cache(
                session,
                cache_key,
                {"season_name": target_season.name, "source": selected_candidate.source},
                ttl_seconds=604800,  # 7å¤©
                provider=selected_candidate.source
            )
        
        self.logger.info(f"è·å–å­£åº¦åç§°æˆåŠŸ: {title} S{season_number:02d} â†’ {target_season.name} (æ¥æº: {selected_candidate.source})")
        return target_season.name

    async def search_all_metadata_sources(
        self,
        title: str,
        year: Optional[int] = None,
        media_type: str = "tv",
        sources: Optional[List[str]] = None,
        user: Optional[models.User] = None
    ) -> List[MetadataSearchCandidate]:
        """
        æœç´¢æ‰€æœ‰å…ƒæ•°æ®æº,è¿”å›å€™é€‰åˆ—è¡¨

        Args:
            title: æœç´¢æ ‡é¢˜
            year: å¹´ä»½(å¯é€‰)
            media_type: åª’ä½“ç±»å‹ ('tv' or 'movie')
            sources: è¦æœç´¢çš„æºåˆ—è¡¨,Noneè¡¨ç¤ºæœç´¢æ‰€æœ‰å¯ç”¨çš„æº
            user: ç”¨æˆ·å¯¹è±¡

        Returns:
            å€™é€‰ç»“æœåˆ—è¡¨
        """
        if not user:
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç”¨æˆ·å¯¹è±¡ç”¨äºAPIè°ƒç”¨
            user = models.User(id=0, username="system", isAdmin=True)

        # ç¡®å®šè¦æœç´¢çš„æº
        if sources is None:
            sources = ["tmdb"]  # é»˜è®¤åªæœç´¢TMDB,åç»­å¯æ‰©å±•

        all_candidates = []

        # å¹¶å‘æœç´¢æ‰€æœ‰æº
        for source_name in sources:
            try:
                source = self.metadata_manager.get_source(source_name)
                if not source:
                    self.logger.warning(f"å…ƒæ•°æ®æº '{source_name}' æœªæ‰¾åˆ°")
                    continue

                # è°ƒç”¨æºçš„æœç´¢æ–¹æ³•
                search_results = await source.search(title, user, mediaType=media_type)

                if not search_results:
                    self.logger.info(f"{source_name} æœç´¢æ— ç»“æœ: {title}")
                    continue

                # è½¬æ¢ä¸ºé€šç”¨æ ¼å¼
                for result in search_results[:10]:  # æ¯ä¸ªæºæœ€å¤šå–10ä¸ªç»“æœ
                    candidate = MetadataSearchCandidate(
                        source=source_name,
                        id=result.tmdbId or result.id,
                        title=result.title,
                        original_title=getattr(result, 'originalTitle', None),
                        year=result.year,
                        media_type=media_type,
                        overview=getattr(result, 'overview', None)
                    )
                    all_candidates.append(candidate)

                self.logger.info(f"{source_name} æœç´¢æˆåŠŸ: {title}, æ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")

            except Exception as e:
                self.logger.error(f"{source_name} æœç´¢å¤±è´¥: {title}, é”™è¯¯: {e}")
                continue

        return all_candidates

    async def get_seasons_from_source(
        self,
        source: str,
        id: str,
        media_type: str = "tv"
    ) -> List[SeasonInfo]:
        """
        ä»æŒ‡å®šå…ƒæ•°æ®æºè·å–å­£åº¦ä¿¡æ¯

        Args:
            source: å…ƒæ•°æ®æºåç§° ('tmdb', 'tvdb', etc.)
            id: æºçš„ID
            media_type: åª’ä½“ç±»å‹ ('tv' or 'movie')

        Returns:
            å­£åº¦ä¿¡æ¯åˆ—è¡¨
        """
        if media_type != "tv":
            return []

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{source}_seasons_{id}"
        async with self._session_factory() as session:
            cached_result = await crud.get_cache(session, cache_key)
            if cached_result:
                self.logger.info(f"{source} å­£åº¦ä¿¡æ¯ç¼“å­˜å‘½ä¸­: {id}")
                return [SeasonInfo(**s) for s in cached_result]

        # æ ¹æ®æºç±»å‹è°ƒç”¨ä¸åŒçš„API
        try:
            if source == "tmdb":
                seasons = await self._get_tmdb_seasons(id)
            # åç»­å¯æ‰©å±•å…¶ä»–æº
            # elif source == "tvdb":
            #     seasons = await self._get_tvdb_seasons(id)
            else:
                self.logger.warning(f"ä¸æ”¯æŒçš„å…ƒæ•°æ®æº: {source}")
                return []

            # ç¼“å­˜ç»“æœ(30å¤©)
            async with self._session_factory() as session:
                await crud.set_cache(
                    session,
                    cache_key,
                    [s.model_dump() for s in seasons],
                    ttl_seconds=2592000,  # 30å¤©
                    provider=source
                )

            self.logger.info(f"è·å–{source}å­£åº¦ä¿¡æ¯æˆåŠŸ: {id}, å…±{len(seasons)}å­£")
            return seasons

        except Exception as e:
            self.logger.error(f"è·å–{source}å­£åº¦ä¿¡æ¯å¤±è´¥: {id}, é”™è¯¯: {e}")
            return []

    async def _get_tmdb_seasons(self, tmdb_id: str) -> List[SeasonInfo]:
        """è·å–TMDBå­£åº¦ä¿¡æ¯(å†…éƒ¨æ–¹æ³•)"""
        tmdb_source = self.metadata_manager.get_source("tmdb")
        # ç›´æ¥è°ƒç”¨TMDB APIè·å–å­£åº¦ä¿¡æ¯
        async with await tmdb_source._create_client() as client:
            response = await client.get(f"/tv/{tmdb_id}")
            response.raise_for_status()
            data = response.json()

            seasons_data = data.get("seasons", [])
            seasons = []

            for season_data in seasons_data:
                # è·³è¿‡ç‰¹åˆ«ç¯‡(season 0)
                season_number = season_data.get("season_number", 0)
                if season_number == 0:
                    continue

                # è·å–å­£åº¦åˆ«å
                season_aliases = await self._get_season_aliases(
                    tmdb_id, season_number, season_data.get("name", "")
                )

                seasons.append(SeasonInfo(
                    season_number=season_number,
                    name=season_data.get("name"),
                    episode_count=season_data.get("episode_count", 0),
                    air_date=season_data.get("air_date"),
                    overview=season_data.get("overview"),
                    aliases=season_aliases
                ))

            return seasons

    async def _get_season_aliases(
        self,
        tmdb_id: str,
        season_number: int,
        season_name: str
    ) -> List[str]:
        """
        è·å–å­£åº¦åˆ«åï¼ŒåŒ…æ‹¬å¸¸è§çš„ä¸­è‹±æ–‡è¡¨è¾¾

        Args:
            tmdb_id: TMDB ID
            season_number: å­£åº¦å·
            season_name: å­£åº¦åç§°

        Returns:
            å­£åº¦åˆ«ååˆ—è¡¨
        """
        aliases = set()

        # æ·»åŠ åŸå§‹åç§°
        if season_name:
            aliases.add(season_name)

        # åŸºäºå­£åº¦å·ç”Ÿæˆå¸¸è§åˆ«å
        season_aliases_map = {
            1: ["ç¬¬ä¸€å­£", "ç¬¬1å­£", "Season 1", "S1", "ç¬¬ä¸€éƒ¨", "Part 1"],
            2: ["ç¬¬äºŒå­£", "ç¬¬2å­£", "Season 2", "S2", "ç¬¬äºŒéƒ¨", "Part 2", "II", "â…±"],
            3: ["ç¬¬ä¸‰å­£", "ç¬¬3å­£", "Season 3", "S3", "ç¬¬ä¸‰éƒ¨", "Part 3", "III", "â…²"],
            4: ["ç¬¬å››å­£", "ç¬¬4å­£", "Season 4", "S4", "ç¬¬å››éƒ¨", "Part 4", "IV", "â…³"],
            5: ["ç¬¬äº”å­£", "ç¬¬5å­£", "Season 5", "S5", "ç¬¬äº”éƒ¨", "Part 5", "V", "â…´"],
            6: ["ç¬¬å…­å­£", "ç¬¬6å­£", "Season 6", "S6", "ç¬¬å…­éƒ¨", "Part 6", "VI", "â…µ"],
        }

        # æ·»åŠ åŸºäºå­£åº¦å·çš„åˆ«å
        if season_number in season_aliases_map:
            aliases.update(season_aliases_map[season_number])

        # åŸºäºå­£åº¦åç§°ç”Ÿæˆç‰¹æ®Šåˆ«åï¼ˆç§»é™¤ç¡¬ç¼–ç ï¼Œè®©TMDBæ•°æ®è‡ªå·±è¯´è¯ï¼‰
        if season_name:
            # è‡ªåŠ¨ä»å­£åº¦åç§°ä¸­æå–å…³é”®è¯ä½œä¸ºåˆ«å
            import re

            # æå–å­£åº¦åç§°ä¸­çš„ç‹¬ç‰¹éƒ¨åˆ†ï¼ˆå»é™¤é€šç”¨å‰ç¼€ï¼‰
            cleaned_name = re.sub(r'^(ç¬¬?\d+å­£|Season\s*\d+|S\d+|[A-Z]+)\s*', '', season_name.strip())
            if cleaned_name:
                # æ·»åŠ æ¸…ç†åçš„åç§°ä½œä¸ºåˆ«å
                aliases.add(cleaned_name)

                # æ·»åŠ å®Œæ•´å­£åº¦åç§°çš„å˜ä½“
                base_name = re.sub(r'^(é¬¼ç­ä¹‹åˆƒ|åˆ€å‰‘ç¥åŸŸ|è¿›å‡»çš„å·¨äºº|Demon Slayer|Sword Art Online|Attack on Titan)\s*', '', cleaned_name.strip())
                if base_name and base_name != cleaned_name:
                    aliases.add(base_name)

        return list(aliases)


async def ai_season_mapping_and_correction(
    search_title: str,
    search_results: list,
    metadata_manager,
    ai_matcher,
    logger,
    similarity_threshold: float = 60.0
) -> list:
    """
    AIå­£åº¦æ˜ å°„ä¸ä¿®æ­£å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå¹¶è¡Œè®¡ç®—ï¼‰

    Args:
        search_title: æ ‡å‡†åŒ–çš„æœç´¢æ ‡é¢˜
        search_results: æœç´¢ç»“æœåˆ—è¡¨
        metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨
        ai_matcher: AIåŒ¹é…å™¨
        logger: æ—¥å¿—è®°å½•å™¨
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

    Returns:
        list: ä¿®æ­£ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ä¿®æ­£ä¿¡æ¯
    """
    try:
        # 1. é€šè¿‡æ ‡é¢˜æœç´¢TMDBè·å–å­£åº¦ä¿¡æ¯ï¼ˆç”¨äºæ˜ å°„ä¿®æ­£ï¼‰
        tmdb_results = await _get_cached_tmdb_search(search_title, metadata_manager, logger)
        if not tmdb_results:
            logger.info(f"â—‹ AIå­£åº¦æ˜ å°„: æœªæ‰¾åˆ° '{search_title}' çš„TMDBä¿¡æ¯")
            return []

        # 2. å¦‚æœTMDBè¿”å›å¤šä¸ªç»“æœï¼Œä½¿ç”¨AIé€‰æ‹©æœ€ä½³TMDBåŒ¹é…
        if len(tmdb_results) == 1:
            best_tmdb_match = tmdb_results[0]
            logger.info(f"â—‹ AIå­£åº¦æ˜ å°„: å”¯ä¸€TMDBåŒ¹é…: {best_tmdb_match.title} (ç±»å‹: {best_tmdb_match.type})")
        else:
            # å¤šä¸ªTMDBç»“æœæ—¶ï¼ŒAIé€‰æ‹©æœ€ä½³åŒ¹é…
            try:
                # è½¬æ¢MetadataDetailsResponseä¸ºProviderSearchInfoæ ¼å¼ä¾›AIä½¿ç”¨
                provider_results = []
                for r in tmdb_results:
                    provider_results.append(models.ProviderSearchInfo(
                        provider="tmdb",
                        mediaId=r.tmdbId or r.id,
                        title=r.title,
                        type=r.type or "unknown",
                        season=1,  # TMDBæœç´¢ç»“æœæ²¡æœ‰å­£åº¦ä¿¡æ¯ï¼Œé»˜è®¤ä¸º1
                        year=r.year,
                        imageUrl=r.imageUrl,
                        episodeCount=None
                    ))

                query_info = {
                    "title": search_title,
                    "season": None,
                    "episode": None,
                    "year": None,
                    "type": None
                }

                selected_index = await ai_matcher.select_best_match(
                    query_info, provider_results, {}
                )

                if selected_index is not None and 0 <= selected_index < len(tmdb_results):
                    best_tmdb_match = tmdb_results[selected_index]
                    logger.info(f"âœ“ AIå­£åº¦æ˜ å°„: AIé€‰æ‹©TMDBåŒ¹é…: {best_tmdb_match.title} (ç±»å‹: {best_tmdb_match.type}, ID: {best_tmdb_match.id})")
                else:
                    logger.error(f"âš  AIå­£åº¦æ˜ å°„: AIé€‰æ‹©TMDBåŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ")
                    best_tmdb_match = tmdb_results[0]

            except Exception as e:
                logger.error(f"âš  AIå­£åº¦æ˜ å°„: TMDBåŒ¹é…é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ: {e}")
                best_tmdb_match = tmdb_results[0]

        # 3. è·å–TMDBå­£åº¦ä¿¡æ¯ç”¨äºåç»­ä¿®æ­£
        # ç¡®ä¿é€‰æ‹©çš„æ˜¯TVç±»å‹ï¼Œå¦åˆ™æ— æ³•è·å–å­£åº¦ä¿¡æ¯
        if best_tmdb_match.type != 'tv':
            logger.warning(f"âš  AIå­£åº¦æ˜ å°„: é€‰æ‹©çš„TMDBç»“æœä¸æ˜¯TVç±»å‹ ({best_tmdb_match.type})ï¼Œæ— æ³•è·å–å­£åº¦ä¿¡æ¯")
            # å°è¯•ä»TMDBç»“æœä¸­æ‰¾åˆ°TVç±»å‹çš„ç»“æœ
            tv_result = None
            for result in tmdb_results:
                if result.type == 'tv':
                    tv_result = result
                    logger.info(f"âœ“ AIå­£åº¦æ˜ å°„: æ‰¾åˆ°TVç±»å‹ç»“æœ: {tv_result.title} (ID: {tv_result.id})")
                    break

            if not tv_result:
                logger.error(f"âš  AIå­£åº¦æ˜ å°„: TMDBç»“æœä¸­æ²¡æœ‰TVç±»å‹ï¼Œæ— æ³•è·å–å­£åº¦ä¿¡æ¯")
                return []

            best_tmdb_match = tv_result

        try:
            seasons_info = await metadata_manager.get_seasons("tmdb", best_tmdb_match.id)
        except Exception as e:
            logger.error(f"è·å–tmdbå­£åº¦ä¿¡æ¯å¤±è´¥: {best_tmdb_match.id}, é”™è¯¯: {e}")
            return []

        if not seasons_info or len(seasons_info) <= 1:
            logger.info(f"â—‹ AIå­£åº¦æ˜ å°„: '{search_title}' åªæœ‰1ä¸ªå­£åº¦æˆ–æ— å­£åº¦ä¿¡æ¯ï¼Œè·³è¿‡")
            return []

        logger.info(f"âœ“ AIå­£åº¦æ˜ å°„: è·å–åˆ° '{search_title}' çš„TMDBå­£åº¦ä¿¡æ¯ï¼Œå…± {len(seasons_info)} ä¸ªå­£åº¦")
        for season in seasons_info:
            season_name = season.name or f"ç¬¬{season.season_number}å­£"
            logger.info(f"  - ç¬¬{season.season_number}å­£: {season_name}")
        # 4. å¯¹æ‰€æœ‰æœç´¢ç»“æœè¿›è¡ŒAIå­£åº¦ä¿®æ­£ï¼ˆä¸æ˜¯é€‰æ‹©æœ€ä½³åŒ¹é…ï¼‰
        tv_results = [item for item in search_results if item.type == 'tv_series']
        if not tv_results:
            logger.info(f"â—‹ AIå­£åº¦æ˜ å°„: æ²¡æœ‰TVç»“æœéœ€è¦ä¿®æ­£")
            return []

        logger.info(f"â—‹ å¼€å§‹AIå­£åº¦ä¿®æ­£ï¼Œæ£€æŸ¥ {len(tv_results)} ä¸ªTVç»“æœ...")

        # è°ƒè¯•ï¼šå¼ºåˆ¶æ‰“å°TMDBå­£åº¦ä¿¡æ¯
        logger.info("ğŸ” TMDBå­£åº¦ä¿¡æ¯è¯¦æƒ…:")
        for season in seasons_info:
            aliases_str = ', '.join(season.aliases[:5]) if season.aliases else 'æ— '
            logger.info(f"  S{season.season_number}: {season.name} (åˆ«å: {aliases_str})")

        # 5. ä½¿ç”¨æ‰¹é‡AIåˆ«åæ˜ å°„è¿›è¡Œå­£åº¦ä¿®æ­£
        # æå–æ‰€æœ‰æ ‡é¢˜ä½œä¸ºåˆ«ååˆ—è¡¨è¿›è¡Œæ‰¹é‡æ˜ å°„
        all_titles = [item.title for item in tv_results]
        logger.info(f"ğŸ”„ æå–åˆ° {len(all_titles)} ä¸ªæ ‡é¢˜è¿›è¡Œæ‰¹é‡AIåˆ«åæ˜ å°„...")

        # æ‰¹é‡AIåˆ«åæ˜ å°„
        alias_mapping = await _batch_alias_season_mapping(
            search_title, all_titles, seasons_info, ai_matcher, logger
        )

            # åº”ç”¨æ‰¹é‡æ˜ å°„ç»“æœ
        corrected_results = []
        for item in tv_results:
            mapped_season = alias_mapping.get(item.title)
            if mapped_season and mapped_season.startswith('S'):
                # æå–å­£åº¦å·
                season_num = int(mapped_season[1:])
                if item.season != season_num:
                    # æ‰¾åˆ°å¯¹åº”çš„å­£åº¦åç§°
                    season_name = "æœªçŸ¥å­£åº¦"
                    for season in seasons_info:
                        if season.season_number == season_num:
                            season_name = season.name or f"ç¬¬{season_num}å­£"
                            break

                    correction = {
                        'item': item,
                        'original_season': item.season,
                        'corrected_season': season_num,
                        'confidence': 95.0,  # AIæ‰¹é‡æ˜ å°„ç»™äºˆé«˜ç½®ä¿¡åº¦
                        'tmdb_season_name': season_name
                    }
                    corrected_results.append(correction)
                    logger.info(f"  âœ“ AIæ‰¹é‡ä¿®æ­£: '{item.title}' S{item.season} â†’ S{season_num} ({season_name}) (ç½®ä¿¡åº¦: 95.0%)")
                else:
                    logger.debug(f"  â—‹ '{item.title}' å­£åº¦å·²æ­£ç¡®: S{season_num}")
            elif mapped_season == 'å¤–ä¼ ':
                logger.info(f"  â—‹ '{item.title}' è¯†åˆ«ä¸ºå¤–ä¼ ï¼Œè·³è¿‡å­£åº¦æ˜ å°„")
            else:
                logger.debug(f"  â—‹ '{item.title}' æœªæ‰¾åˆ°æ˜ å°„ï¼Œä¿æŒåŸå­£åº¦ S{item.season}")

        logger.info(f"âœ“ AIå­£åº¦æ˜ å°„å®Œæˆ: ä¿®æ­£äº† {len(corrected_results)} ä¸ªç»“æœçš„å­£åº¦ä¿¡æ¯")
        return corrected_results

    except Exception as e:
        logger.warning(f"AIå­£åº¦æ˜ å°„å¤±è´¥: {e}")
        return []

            


async def _batch_alias_season_mapping(
    search_title: str,
    alias_list: list,
    tmdb_seasons_info: list,
    ai_matcher,
    logger
) -> dict:
    """
    æ‰¹é‡AIåˆ«åå­£åº¦æ˜ å°„

    Args:
        search_title: æœç´¢æ ‡é¢˜
        alias_list: åˆ«ååˆ—è¡¨
        tmdb_seasons_info: TMDBå­£åº¦ä¿¡æ¯
        ai_matcher: AIåŒ¹é…å™¨
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        åˆ«ååˆ°å­£åº¦çš„æ˜ å°„å­—å…¸
    """
    logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡AIåˆ«åå­£åº¦æ˜ å°„ï¼Œå…± {len(alias_list)}ä¸ªåˆ«å")

    # æ„å»ºå­£åº¦é€‰é¡¹
    season_options = []
    for season in tmdb_seasons_info:
        season_options.append({
            'season_number': season.season_number,
            'name': season.name,
            'aliases': season.aliases or []
        })

    # æ„å»ºAIæç¤ºè¯
    options_text = ""
    for i, option in enumerate(season_options):
        aliases_str = ', '.join(option['aliases'][:3]) if option['aliases'] else 'æ— '
        options_text += f"S{option['season_number']}: {option['name']} (åˆ«å: {aliases_str})\n"

    # æ‰¹é‡å¤„ç†åˆ«å
    alias_mapping = {}

    # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡é•¿
    batch_size = 10
    for i in range(0, len(alias_list), batch_size):
        batch = alias_list[i:i + batch_size]

        # æ„å»ºæ‰¹é‡è¯·æ±‚æç¤ºè¯ï¼ˆå¤ç”¨ç°æœ‰é…ç½®ï¼‰
        batch_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­£åº¦è¯†åˆ«åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æåŠ¨æ¼«æ ‡é¢˜ä¸­çš„å­£åº¦ä¿¡æ¯ã€‚

è¯·åˆ†æä»¥ä¸‹åˆ«ååˆ—è¡¨ï¼Œå°†æ¯ä¸ªåˆ«åæ˜ å°„åˆ°æ­£ç¡®çš„å­£åº¦ï¼š

æœç´¢ä½œå“ï¼š{search_title}

å­£åº¦é€‰é¡¹ï¼š
{options_text}

åˆ«ååˆ—è¡¨ï¼š
{chr(10).join(f"{j+1}. {alias}" for j, alias in enumerate(batch))}

**åˆ†æè§„åˆ™**:
1. ä¼˜å…ˆè¯†åˆ«æ ‡é¢˜ä¸­æ˜ç¡®çš„å­£åº¦å…³é”®è¯ï¼š
   - ä¸­æ–‡ï¼šç¬¬1å­£ã€ç¬¬2å­£ã€ç¬¬ä¸€å­£ã€ç¬¬äºŒå­£ç­‰
   - è‹±æ–‡ï¼šSeason 1ã€Season 2ã€S1ã€S2ç­‰
   - ç½—é©¬æ•°å­—ï¼šI=1, II=2, III=3, IV=4, V=5, VI=6
   - ç‰¹æ®Šè¡¨è¾¾ï¼šæœ€ç»ˆå­£=æœ€åä¸€å­£ï¼Œç‰¹åˆ«ç¯‡=ç¬¬0å­£

2. åˆ«ååŒ¹é…ï¼š
   - æ³¨æ„æ¯ä¸ªå­£åº¦é€‰é¡¹éƒ½æœ‰åˆ«åï¼Œæ ‡é¢˜ä¸­çš„ä»»ä½•åˆ«åéƒ½åº”è¯¥åŒ¹é…å¯¹åº”å­£åº¦
   - ä¾‹å¦‚ï¼š"åˆ€å‰‘ç¥åŸŸ Alicizationç¯‡" åº”è¯¥åŒ¹é…åŒ…å«"Alicization"åˆ«åçš„å­£åº¦
   - ä¾‹å¦‚ï¼š"åˆ€å‰‘ç¥åŸŸ çˆ±ä¸½ä¸ç¯‡" åº”è¯¥åŒ¹é…åŒ…å«"çˆ±ä¸½ä¸ç¯‡"åˆ«åçš„å­£åº¦

3. è¯­ä¹‰ç†è§£ï¼š
   - "åˆ€å‰‘ç¥åŸŸ Sword Art Online ç¬¬äºŒå­£" åº”è¯¥åŒ¹é…ç¬¬2å­£
   - "é¬¼ç­ä¹‹åˆƒ é”»åˆ€æ‘ç¯‡" éœ€è¦æ ¹æ®ä½œå“å®é™…å­£åº¦åˆ¤æ–­
   - "è¿›å‡»çš„å·¨äºº æœ€ç»ˆå­£" åŒ¹é…æœ€åä¸€å­£

4. å¤–ä¼ å¤„ç†ï¼š
   - å¤–ä¼ ã€ç‰¹åˆ«ç¯‡ã€å‰§åœºç‰ˆé€šå¸¸ä¸å±äºä¸»çº¿å­£åº¦ï¼Œè¿”å›"å¤–ä¼ "
   - åŒ…å«"å¤–ä¼ "ã€"ç‰¹åˆ«ç¯‡"ã€"Extra"ã€"SP"ã€"OVA"ã€"OAD"ç­‰å…³é”®è¯çš„è¯†åˆ«ä¸ºå¤–ä¼ 

**è¾“å‡ºæ ¼å¼**:
æ¯è¡Œä¸€ä¸ªæ˜ å°„ï¼Œæ ¼å¼ï¼šåˆ«å -> SX æˆ– åˆ«å -> å¤–ä¼ 
ä¾‹å¦‚ï¼š
åˆ€å‰‘ç¥åŸŸ ç¬¬äºŒå­£ -> S2
åˆ€å‰‘ç¥åŸŸ Alicization -> S3
åˆ€å‰‘ç¥åŸŸå¤–ä¼  -> å¤–ä¼ 

ä¸è¦è¿”å›ä»»ä½•è§£é‡Šæˆ–å…¶ä»–æ–‡æœ¬ã€‚"""

        try:
            # è°ƒç”¨AIè¿›è¡Œæ‰¹é‡æ˜ å°„
            response = ai_matcher.client.chat.completions.create(
                model=ai_matcher.model,
                messages=[{"role": "user", "content": batch_prompt}],
                temperature=0.1,
                max_tokens=1000
            )

            ai_result = response.choices[0].message.content.strip()
            logger.info(f"ğŸ¤– AIæ‰¹é‡æ˜ å°„ç»“æœ (æ‰¹æ¬¡ {i//batch_size + 1}):")
            logger.info(ai_result)

            # è§£æAIç»“æœ
            for line in ai_result.split('\n'):
                if '->' in line:
                    alias_part, season_part = line.split('->', 1)
                    alias = alias_part.strip()
                    season = season_part.strip()

                    if alias in batch:
                        alias_mapping[alias] = season
                        logger.info(f"  âœ“ {alias} -> {season}")

        except Exception as e:
            logger.error(f"âŒ AIæ‰¹é‡æ˜ å°„å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ä½¿ç”¨ç®€å•è§„åˆ™å…œåº•
            for alias in batch:
                if 'å¤–ä¼ ' in alias or 'ç‰¹åˆ¥ç¯‡' in alias or 'Extra' in alias:
                    alias_mapping[alias] = 'å¤–ä¼ '
                elif any(keyword in alias for keyword in ['ç¬¬äºŒå­£', 'ç¬¬2å­£', 'â…¡', 'II']):
                    alias_mapping[alias] = 'S2'
                elif any(keyword in alias for keyword in ['çˆ±ä¸½ä¸ç¯‡', 'Alicization', 'ç¬¬ä¸‰å­£', 'ç¬¬3å­£']):
                    alias_mapping[alias] = 'S3'
                elif any(keyword in alias for keyword in ['å¼‚ç•Œæˆ˜äº‰', 'War of Underworld', 'ç¬¬å››å­£', 'ç¬¬4å­£']):
                    alias_mapping[alias] = 'S4'
                else:
                    alias_mapping[alias] = 'S1'  # é»˜è®¤ç¬¬1å­£

    logger.info(f"âœ… æ‰¹é‡AIåˆ«åæ˜ å°„å®Œæˆï¼Œå…±æ˜ å°„ {len(alias_mapping)} ä¸ªåˆ«å")
    return alias_mapping


async def ai_type_and_season_mapping_and_correction(
    search_title: str,
    search_results: list,
    metadata_manager,
    ai_matcher,
    logger,
    similarity_threshold: float = 60.0
) -> dict:
    """
    ç»Ÿä¸€çš„AIç±»å‹å’Œå­£åº¦æ˜ å°„ä¸ä¿®æ­£å‡½æ•°

    é€‚ç”¨äºæ‰€æœ‰å…­ä¸ªæµç¨‹ï¼š
    1. ä¸»é¡µæœç´¢
    2. å…¨è‡ªåŠ¨å¯¼å…¥
    3. Webhookå¤„ç†
    4. åå¤‡æœç´¢
    5. åå¤‡åŒ¹é…
    6. å¤–éƒ¨æ§åˆ¶æœç´¢/å¯¼å…¥

    Args:
        search_title: æ ‡å‡†åŒ–çš„æœç´¢æ ‡é¢˜
        search_results: æœç´¢ç»“æœåˆ—è¡¨
        metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨
        ai_matcher: AIåŒ¹é…å™¨
        logger: æ—¥å¿—è®°å½•å™¨
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

    Returns:
        dict: åŒ…å«ç±»å‹ä¿®æ­£å’Œå­£åº¦ä¿®æ­£çš„ç»“æœ
        {
            'type_corrections': list,  # ç±»å‹ä¿®æ­£ç»“æœ
            'season_corrections': list,  # å­£åº¦ä¿®æ­£ç»“æœ
            'total_corrections': int,   # æ€»ä¿®æ­£æ•°
            'corrected_results': list   # ä¿®æ­£åçš„å®Œæ•´ç»“æœåˆ—è¡¨
        }
    """
    try:
        logger.info(f"â—‹ å¼€å§‹ç»Ÿä¸€AIæ˜ å°„ä¿®æ­£: '{search_title}' ({len(search_results)} ä¸ªç»“æœ)")

        # åˆå§‹åŒ–ç»“æœ
        type_corrections = []
        season_corrections = []
        corrected_results = []

        # 1. ç±»å‹ä¿®æ­£ï¼ˆå°†æ‰€æœ‰ç»“æœä¿®æ­£ä¸ºæ­£ç¡®çš„åª’ä½“ç±»å‹ï¼‰
        logger.info(f"â—‹ å¼€å§‹ç±»å‹ä¿®æ­£...")
        for item in search_results:
            original_type = item.type
            corrected_type = original_type

            # ä½¿ç”¨AIåˆ¤æ–­æ­£ç¡®çš„ç±»å‹ï¼ˆè¿™é‡Œå¯ä»¥æ‰©å±•AIç±»å‹åˆ¤æ–­é€»è¾‘ï¼‰
            # ç›®å‰æš‚æ—¶ä¿æŒåŸç±»å‹ï¼Œåç»­å¯ä»¥æ·»åŠ AIç±»å‹åˆ¤æ–­
            if original_type != corrected_type:
                type_corrections.append({
                    'item': item,
                    'original_type': original_type,
                    'corrected_type': corrected_type
                })
                item.type = corrected_type
                logger.info(f"  âœ“ ç±»å‹ä¿®æ­£: '{item.title}' {original_type} â†’ {corrected_type}")

        logger.info(f"âœ“ ç±»å‹ä¿®æ­£å®Œæˆ: ä¿®æ­£äº† {len(type_corrections)} ä¸ªç»“æœçš„ç±»å‹ä¿¡æ¯")

        # 2. å­£åº¦ä¿®æ­£ï¼ˆåªå¯¹ç”µè§†å‰§è¿›è¡Œï¼‰
        tv_results = [item for item in search_results if item.type == 'tv_series']
        if tv_results:
            season_corrections = await ai_season_mapping_and_correction(
                search_title=search_title,
                search_results=search_results,
                metadata_manager=metadata_manager,
                ai_matcher=ai_matcher,
                logger=logger,
                similarity_threshold=similarity_threshold
            )

            # åº”ç”¨å­£åº¦ä¿®æ­£åˆ°åŸå§‹æœç´¢ç»“æœ
            for correction in season_corrections:
                item = correction['item']
                item.season = correction['corrected_season']
                logger.info(f"  âœ“ å­£åº¦ä¿®æ­£åº”ç”¨: '{item.title}' â†’ S{item.season}")

        # 3. æ„å»ºä¿®æ­£åçš„ç»“æœåˆ—è¡¨
        corrected_results = search_results.copy()

        total_corrections = len(type_corrections) + len(season_corrections)
        logger.info(f"âœ“ ç»Ÿä¸€AIæ˜ å°„ä¿®æ­£å®Œæˆ: ç±»å‹ä¿®æ­£ {len(type_corrections)} ä¸ª, å­£åº¦ä¿®æ­£ {len(season_corrections)} ä¸ª, æ€»è®¡ {total_corrections} ä¸ª")

        return {
            'type_corrections': type_corrections,
            'season_corrections': season_corrections,
            'total_corrections': total_corrections,
            'corrected_results': corrected_results
        }

    except Exception as e:
        logger.warning(f"ç»Ÿä¸€AIæ˜ å°„ä¿®æ­£å¤±è´¥: {e}")
        return {
            'type_corrections': [],
            'season_corrections': [],
            'total_corrections': 0,
            'corrected_results': search_results
        }


async def _get_cached_tmdb_search(search_title: str, metadata_manager, logger) -> List[models.MetadataDetailsResponse]:
    """
    è·å–TMDBæœç´¢ç»“æœï¼Œå¸¦6å°æ—¶ç¼“å­˜

    Args:
        search_title: æœç´¢æ ‡é¢˜
        metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨
        logger: æ—¥å¿—è®°å½•å™¨

    Returns:
        List[models.MetadataDetailsResponse]: TMDBæœç´¢ç»“æœ
    """

    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = f"tmdb_search_{hashlib.md5(search_title.encode('utf-8')).hexdigest()}"

    # æ£€æŸ¥ç¼“å­˜
    async with metadata_manager._session_factory() as session:
        cached_result = await crud.get_cache(session, cache_key)
        if cached_result:
            logger.info(f"TMDBæœç´¢ç¼“å­˜å‘½ä¸­: {search_title}")
            return [models.MetadataDetailsResponse(**r) for r in cached_result]

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæœç´¢
    logger.debug(f"TMDBæœç´¢ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæœç´¢: {search_title}")
    try:
        tmdb_results = await metadata_manager.search("tmdb", search_title, None, mediaType='multi')

        # ç¼“å­˜ç»“æœï¼ˆ6å°æ—¶ = 21600ç§’ï¼‰
        async with metadata_manager._session_factory() as session:
            await crud.set_cache(
                session,
                cache_key,
                [r.model_dump() for r in tmdb_results],
                ttl_seconds=21600,  # 6å°æ—¶
                provider="tmdb"
            )
            logger.info(f"TMDBæœç´¢ç»“æœå·²ç¼“å­˜: {search_title} (6å°æ—¶)")

        return tmdb_results
    except Exception as e:
        logger.error(f"TMDBæœç´¢å¤±è´¥: {search_title}, é”™è¯¯: {e}")
        return []




