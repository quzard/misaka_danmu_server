"""å­£åº¦æ˜ å°„æ¨¡å— - V2.1.6é£æ ¼é‡æ„ç‰ˆæœ¬"""
import hashlib
import logging
import re
from typing import Optional, List, Any, Dict
from difflib import SequenceMatcher

from pydantic import Field
from . import models, crud

logger = logging.getLogger(__name__)


# ============================================================================
# å¤–ä¼ /è¡ç”Ÿä½œå“æ£€æµ‹ (V2.1.6æ–°å¢)
# ============================================================================

# å¤–ä¼ /è¡ç”Ÿä½œå“çš„è¯†åˆ«å…³é”®è¯
SPINOFF_KEYWORDS = [
    # ä¸­æ–‡
    "å¤–ä¼ ", "ç•ªå¤–", "ç‰¹åˆ«ç¯‡", "å‰§åœºç‰ˆ", "OVA", "OAD", "SP",
    # æ—¥æ–‡
    "å¤–ä¼", "ç•ªå¤–ç·¨", "ç‰¹åˆ¥ç·¨",
    # è‹±æ–‡
    "spin-off", "spinoff", "side story", "gaiden",
    "special", "movie", "film", "ova", "oad",
]

# é¢„ç¼–è¯‘çš„å¤–ä¼ æ£€æµ‹æ­£åˆ™ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
SPINOFF_PATTERN = re.compile(
    r'(?:' + '|'.join(re.escape(kw) for kw in SPINOFF_KEYWORDS) + r')',
    re.IGNORECASE
)


def is_spinoff_title(title: str, base_title: str) -> bool:
    """
    æ£€æµ‹æ ‡é¢˜æ˜¯å¦ä¸ºå¤–ä¼ /è¡ç”Ÿä½œå“

    Args:
        title: è¦æ£€æµ‹çš„æ ‡é¢˜
        base_title: åŸä½œåŸºç¡€æ ‡é¢˜

    Returns:
        True å¦‚æœæ˜¯å¤–ä¼ /è¡ç”Ÿä½œå“
    """
    if not title:
        return False

    title_lower = title.lower()

    # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«å¤–ä¼ å…³é”®è¯
    if SPINOFF_PATTERN.search(title):
        return True

    # 2. æ£€æŸ¥æ˜¯å¦ä¸º "XXXå¤–ä¼ ï¼šYYY" æ ¼å¼
    if base_title:
        base_lower = base_title.lower()
        # å¦‚æœæ ‡é¢˜åŒ…å«åŸºç¡€æ ‡é¢˜ï¼Œä½†åé¢æœ‰é¢å¤–å†…å®¹ä¸”ä¸æ˜¯å­£åº¦æ ‡è¯†
        if base_lower in title_lower:
            suffix = title_lower.replace(base_lower, "").strip()
            # æ’é™¤çº¯å­£åº¦æ ‡è¯†ï¼ˆå¦‚ "ç¬¬äºŒå­£"ã€"II"ã€"2"ï¼‰
            if suffix and not re.match(r'^[:\s]*(?:ç¬¬?\d+å­£|[â…°â…±â…²â…³â…´â…µâ…¶â…·â…¸â…¹]+|[ivx]+|season\s*\d+|\d+)$', suffix, re.IGNORECASE):
                # æ£€æŸ¥åç¼€æ˜¯å¦çœ‹èµ·æ¥åƒå¤–ä¼ æ ‡é¢˜ï¼ˆæœ‰å†’å·åè·Ÿä¸åŒåç§°ï¼‰
                if re.match(r'^[:\sï¼š]+[^ç¬¬å­£\d]+', suffix):
                    return True

    return False


# ============================================================================
# æ ‡é¢˜ä¸­æ˜ç¡®å­£åº¦ä¿¡æ¯æå– (V2.1.7æ–°å¢)
# ============================================================================

# ä¸­æ–‡æ•°å­—åˆ°é˜¿æ‹‰ä¼¯æ•°å­—çš„æ˜ å°„
CHINESE_NUM_MAP = {
    'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
    'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
    'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15,
}

# ç½—é©¬æ•°å­—åˆ°é˜¿æ‹‰ä¼¯æ•°å­—çš„æ˜ å°„
ROMAN_NUM_MAP = {
    'i': 1, 'ii': 2, 'iii': 3, 'iv': 4, 'v': 5,
    'vi': 6, 'vii': 7, 'viii': 8, 'ix': 9, 'x': 10,
    'â…°': 1, 'â…±': 2, 'â…²': 3, 'â…³': 4, 'â…´': 5,
    'â…µ': 6, 'â…¶': 7, 'â…·': 8, 'â…¸': 9, 'â…¹': 10,
}


def _extract_explicit_season_from_title(title: str) -> Optional[int]:
    """
    ä»æ ‡é¢˜ä¸­æå–æ˜ç¡®çš„å­£åº¦ä¿¡æ¯ (V2.1.7æ–°å¢)

    è¯†åˆ«ä»¥ä¸‹æ¨¡å¼:
    - "ç¬¬äºŒå­£"ã€"ç¬¬ä¸‰å­£" ç­‰ä¸­æ–‡å­£åº¦
    - "Season 2"ã€"Season 3" ç­‰è‹±æ–‡å­£åº¦
    - "S2"ã€"S3" ç­‰ç¼©å†™ï¼ˆéœ€åœ¨æ ‡é¢˜æœ«å°¾æˆ–ç©ºæ ¼åï¼‰
    - ç½—é©¬æ•°å­—å¦‚ "II"ã€"III" ç­‰ï¼ˆéœ€åœ¨æ ‡é¢˜æœ«å°¾ï¼‰

    Args:
        title: æ ‡é¢˜å­—ç¬¦ä¸²

    Returns:
        å­£åº¦æ•°å­—ï¼Œå¦‚æœæ²¡æœ‰æ˜ç¡®å­£åº¦ä¿¡æ¯åˆ™è¿”å› None
    """
    if not title:
        return None

    title_clean = title.strip()

    # æ¨¡å¼1: ä¸­æ–‡ "ç¬¬Nå­£" (Nä¸ºä¸­æ–‡æˆ–é˜¿æ‹‰ä¼¯æ•°å­—)
    match = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+|\d+)å­£', title_clean)
    if match:
        num_str = match.group(1)
        if num_str.isdigit():
            return int(num_str)
        elif num_str in CHINESE_NUM_MAP:
            return CHINESE_NUM_MAP[num_str]

    # æ¨¡å¼2: è‹±æ–‡ "Season N"
    match = re.search(r'Season\s*(\d+)', title_clean, re.IGNORECASE)
    if match:
        return int(match.group(1))

    # æ¨¡å¼3: ç¼©å†™ "S2"ã€"S3" ç­‰ï¼ˆåœ¨ç©ºæ ¼åæˆ–æœ«å°¾ï¼‰
    match = re.search(r'(?:^|\s)S(\d+)(?:\s|$)', title_clean, re.IGNORECASE)
    if match:
        return int(match.group(1))

    # æ¨¡å¼4: ç½—é©¬æ•°å­—ï¼ˆåœ¨æœ«å°¾ï¼Œå¦‚ "åˆ€å‰‘ç¥åŸŸ II"ï¼‰
    match = re.search(r'\s+(I{1,3}|IV|VI{0,3}|IX|X|[â…°â…±â…²â…³â…´â…µâ…¶â…·â…¸â…¹])\s*$', title_clean, re.IGNORECASE)
    if match:
        roman = match.group(1).lower()
        if roman in ROMAN_NUM_MAP:
            return ROMAN_NUM_MAP[roman]

    # æ¨¡å¼5: æ ‡é¢˜æœ«å°¾çš„é˜¿æ‹‰ä¼¯æ•°å­—ï¼ˆå¦‚ "æš´é£ä¹‹é“³2"ã€"é­”æ³•å°‘å¥³å°åœ†3"ï¼‰
    # åŒ¹é…æœ«å°¾çš„æ•°å­—ï¼Œä½†æ’é™¤å¹´ä»½ï¼ˆ4ä½æ•°å­—ï¼‰å’Œåˆ†è¾¨ç‡ï¼ˆå¦‚1080ï¼‰
    match = re.search(r'[^\d](\d{1,2})\s*$', title_clean)
    if match:
        num = int(match.group(1))
        # åªæ¥å—åˆç†çš„å­£åº¦èŒƒå›´ (1-20)
        if 1 <= num <= 20:
            return num

    return None


# ============================================================================
# æ ¸å¿ƒç›¸ä¼¼åº¦è®¡ç®—å‡½æ•° (V2.1.6é£æ ¼ï¼Œä¸ä¾èµ–thefuzz)
# ============================================================================

def calculate_similarity(str1: str, str2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ (0-100)
    V2.1.6é£æ ¼ï¼šä½¿ç”¨å†…ç½®difflibï¼Œä¸ä¾èµ–thefuzz

    Args:
        str1: ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²
        str2: ç¬¬äºŒä¸ªå­—ç¬¦ä¸²

    Returns:
        ç›¸ä¼¼åº¦ç™¾åˆ†æ¯” (0-100)
    """
    if not str1 or not str2:
        return 0.0

    s1 = str1.lower().strip()
    s2 = str2.lower().strip()

    # 1. ç®€å•ç›¸ä¼¼åº¦
    simple = SequenceMatcher(None, s1, s2).ratio() * 100

    # 2. éƒ¨åˆ†ç›¸ä¼¼åº¦ - å­ä¸²åŒ¹é…
    partial = 0.0
    shorter, longer = (s1, s2) if len(s1) <= len(s2) else (s2, s1)
    if shorter in longer:
        partial = len(shorter) / len(longer) * 100

    # 3. Tokenç›¸ä¼¼åº¦
    s1_tokens = set(s1.split())
    s2_tokens = set(s2.split())
    if s1_tokens and s2_tokens:
        intersection = len(s1_tokens & s2_tokens)
        union = len(s1_tokens | s2_tokens)
        token_sim = (intersection / union) * 100 if union > 0 else 0
    else:
        token_sim = 0

    return float(max(simple, partial, token_sim))


def title_contains_season_name(title: str, season_number: int, season_name: str, season_aliases: List[str] = None, threshold: float = 60.0) -> float:
    """
    åˆ¤æ–­æ ‡é¢˜æ˜¯å¦åŒ…å«å­£åº¦åç§°å¹¶è®¡ç®—ç›¸ä¼¼åº¦
    V2.1.6é£æ ¼ï¼šä¸ä¾èµ–thefuzz

    Args:
        title: æœç´¢ç»“æœæ ‡é¢˜
        season_number: å­£åº¦ç¼–å·
        season_name: å­£åº¦åç§°
        season_aliases: å­£åº¦åˆ«ååˆ—è¡¨
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

    Returns:
        ç›¸ä¼¼åº¦ç™¾åˆ†æ¯” (0-100)
    """
    if not title or not season_name:
        return 0.0

    title_lower = title.lower().strip()
    season_name_lower = season_name.lower().strip()
    max_similarity = 0.0

    # ç­–ç•¥1: ç›´æ¥å­ä¸²åŒ…å«
    if season_name_lower in title_lower:
        return 95.0

    # ç­–ç•¥2: ç§»é™¤å‰ç¼€ååŒ…å«
    season_cleaned = re.sub(r'^(ç¬¬\d+å­£|season\s*\d+|s\d+)\s*', '', season_name_lower, flags=re.IGNORECASE)
    if season_cleaned and season_cleaned in title_lower:
        return 90.0

    # ç­–ç•¥3: å­£åº¦å·ç›´æ¥åŒ¹é…
    season_patterns = [
        rf'ç¬¬{season_number}å­£', rf'season\s*{season_number}',
        rf's{season_number}\b', rf'ç¬¬{season_number}éƒ¨'
    ]
    for pattern in season_patterns:
        if re.search(pattern, title_lower, flags=re.IGNORECASE):
            max_similarity = max(max_similarity, 85.0)
            break

    # ç­–ç•¥4: ç›¸ä¼¼åº¦è®¡ç®—
    sim = calculate_similarity(season_cleaned or season_name_lower, title_lower)
    max_similarity = max(max_similarity, sim)

    # ç­–ç•¥5: åˆ«ååŒ¹é…
    if season_aliases:
        for alias in season_aliases:
            alias_sim = calculate_similarity(alias.lower(), title_lower)
            max_similarity = max(max_similarity, alias_sim)

    return max_similarity if max_similarity >= threshold else 0.0


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def _build_title_alias_equivalence_map(tv_results: List, seasons_info: List, log) -> Dict[str, Dict]:
    """
    æ„å»ºæ ‡é¢˜åˆ«åç­‰ä»·æ˜ å°„è¡¨
    å¦‚æœæœç´¢ç»“æœæ ‡é¢˜ä¸TMDBå­£åº¦åˆ«åç›¸åŒï¼Œåˆ™å»ºç«‹ç­‰ä»·å…³ç³»
    """
    equivalence_map = {}

    # æ”¶é›†æ‰€æœ‰TMDBå­£åº¦çš„åˆ«å
    tmdb_aliases = {}
    for season in seasons_info:
        aliases = set()
        if season.name:
            aliases.add(season.name.lower().strip())
        if season.aliases:
            for alias in season.aliases:
                aliases.add(alias.lower().strip())
        # æ·»åŠ å­£åº¦ç¼–å·åˆ«å
        aliases.add(f"s{season.season_number}")
        aliases.add(f"ç¬¬{season.season_number}å­£")

        tmdb_aliases[season.season_number] = {
            'season': season.season_number,
            'name': season.name or f"ç¬¬{season.season_number}å­£",
            'aliases': aliases
        }

    # æ£€æŸ¥æ¯ä¸ªæœç´¢ç»“æœæ ‡é¢˜æ˜¯å¦ä¸TMDBåˆ«åç­‰ä»·
    for item in tv_results:
        title_normalized = item.title.lower().strip()
        for season_num, info in tmdb_aliases.items():
            if title_normalized in info['aliases']:
                equivalence_map[item.title] = {
                    'season': season_num,
                    'name': info['name']
                }
                break

    if equivalence_map:
        log.info(f"ğŸ“‹ åˆ«åç­‰ä»·æ˜ å°„: æ‰¾åˆ° {len(equivalence_map)} ä¸ªç›´æ¥åŒ¹é…")

    return equivalence_map


def _calculate_season_similarity(title: str, season_name: str, season_aliases: List[str] = None) -> float:
    """
    è®¡ç®—æ ‡é¢˜ä¸å­£åº¦çš„ç›¸ä¼¼åº¦ (V2.1.6æ ¸å¿ƒç®—æ³•)
    """
    if not title or not season_name:
        return 0.0

    title_clean = title.lower().strip()
    season_clean = season_name.lower().strip()

    # ç›´æ¥å­ä¸²åŒ…å«
    if season_clean in title_clean:
        return 95.0

    # ç§»é™¤å‰ç¼€ååŒ…å«
    season_no_prefix = re.sub(r'^(ç¬¬\d+å­£|season\s*\d+|s\d+)\s*', '', season_clean, flags=re.IGNORECASE)
    if season_no_prefix and season_no_prefix in title_clean:
        return 90.0

    # ç›¸ä¼¼åº¦è®¡ç®—
    max_sim = calculate_similarity(season_no_prefix or season_clean, title_clean)

    # åˆ«ååŒ¹é…
    if season_aliases:
        for alias in season_aliases:
            alias_sim = calculate_similarity(alias.lower(), title_clean)
            max_sim = max(max_sim, alias_sim)

    return float(max_sim)


class SeasonInfo(models.BaseModel):
    """é€šç”¨å­£åº¦ä¿¡æ¯æ¨¡å‹"""
    season_number: int
    name: Optional[str] = None
    episode_count: int = 0
    air_date: Optional[str] = None
    overview: Optional[str] = None
    aliases: Optional[List[str]] = Field(default=[], description="å­£åº¦åˆ«ååˆ—è¡¨")


class MetadataSearchCandidate(models.BaseModel):
    """å…ƒæ•°æ®æœç´¢å€™é€‰ç»“æœ"""
    source: str  # 'tmdb', 'tvdb', 'bangumi', etc.
    id: str  # æºçš„ID
    title: str
    original_title: Optional[str] = None
    year: Optional[int] = None
    media_type: str  # 'tv' or 'movie'
    overview: Optional[str] = None


class SeasonMapper:
    """å­£åº¦æ˜ å°„å™¨ - é€šè¿‡å…ƒæ•°æ®æºè·å–å­£åº¦åç§°"""
    
    def __init__(self, metadata_manager, session_factory):
        """
        åˆå§‹åŒ–å­£åº¦æ˜ å°„å™¨
        
        Args:
            metadata_manager: MetadataSourceManagerå®ä¾‹
            session_factory: æ•°æ®åº“ä¼šè¯å·¥å‚
        """
        self.metadata_manager = metadata_manager
        self._session_factory = session_factory
        self.logger = logger
    
    async def get_season_name(
        self,
        title: str,
        season_number: int,
        year: Optional[int] = None,
        sources: Optional[List[str]] = None,
        ai_matcher: Optional[Any] = None,
        user: Optional[models.User] = None,
        custom_prompt: Optional[str] = None
    ) -> Optional[str]:
        """
        é€šè¿‡å…ƒæ•°æ®æºè·å–æŒ‡å®šå­£åº¦çš„åç§°(é€šç”¨æ–¹æ³•)

        Args:
            title: å‰§é›†æ ‡é¢˜
            season_number: å­£åº¦ç¼–å·
            year: å¹´ä»½(å¯é€‰)
            sources: è¦æœç´¢çš„å…ƒæ•°æ®æºåˆ—è¡¨,Noneè¡¨ç¤ºä½¿ç”¨é»˜è®¤æº
            ai_matcher: AIåŒ¹é…å™¨(å¯é€‰)
            user: ç”¨æˆ·å¯¹è±¡
            custom_prompt: è‡ªå®šä¹‰AIæç¤ºè¯(å¯é€‰)

        Returns:
            å­£åº¦åç§°,å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
        """
        # æ£€æŸ¥ç¼“å­˜
        sources_str = "_".join(sources) if sources else "default"
        cache_key = f"season_name_{title}_{season_number}_{year or 'any'}_{sources_str}"
        async with self._session_factory() as session:
            cached_result = await crud.get_cache(session, cache_key)
            if cached_result:
                self.logger.info(f"å­£åº¦åç§°ç¼“å­˜å‘½ä¸­: {title} S{season_number:02d}")
                return cached_result.get("season_name")
        
        # ç¬¬1æ­¥: æœç´¢æ‰€æœ‰å…ƒæ•°æ®æº
        candidates = await self.search_all_metadata_sources(title, year, "tv", sources, user)
        if not candidates:
            self.logger.info(f"æœªæ‰¾åˆ°ä»»ä½•å…ƒæ•°æ®: {title}")
            return None
        
        # ç¬¬2æ­¥: ä½¿ç”¨AIé€‰æ‹©æœ€ä½³åŒ¹é…(å¦‚æœæœ‰AIåŒ¹é…å™¨)
        selected_candidate = None
        if ai_matcher and len(candidates) > 1:
            try:
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä¾›AIä½¿ç”¨
                candidates_dict = [c.model_dump() for c in candidates]
                selected_index = await ai_matcher.select_metadata_result(
                    title,
                    year,
                    candidates_dict,
                    season=season_number,
                    custom_prompt=custom_prompt
                )

                if selected_index is not None and 0 <= selected_index < len(candidates):
                    selected_candidate = candidates[selected_index]
                    self.logger.info(f"AIé€‰æ‹©å…ƒæ•°æ®: {selected_candidate.source}:{selected_candidate.id}")
            except Exception as e:
                self.logger.warning(f"AIé€‰æ‹©å…ƒæ•°æ®å¤±è´¥: {e}, ä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ")
        
        # å¦‚æœAIæœªé€‰æ‹©æˆ–åªæœ‰ä¸€ä¸ªå€™é€‰,ä½¿ç”¨ç¬¬ä¸€ä¸ª
        if not selected_candidate:
            selected_candidate = candidates[0]
            self.logger.info(f"ä½¿ç”¨ç¬¬ä¸€ä¸ªå…ƒæ•°æ®ç»“æœ: {selected_candidate.source}:{selected_candidate.id}")
        
        # ç¬¬3æ­¥: è·å–å­£åº¦åˆ—è¡¨
        seasons = await self.get_seasons_from_source(
            selected_candidate.source,
            selected_candidate.id,
            "tv"
        )
        if not seasons:
            return None
        
        # ç¬¬4æ­¥: æ‰¾åˆ°å¯¹åº”å­£åº¦
        target_season = None
        for season in seasons:
            if season.season_number == season_number:
                target_season = season
                break
        
        if not target_season or not target_season.name:
            return None
        
        # ç¼“å­˜ç»“æœ(7å¤©)
        async with self._session_factory() as session:
            await crud.set_cache(
                session,
                cache_key,
                {"season_name": target_season.name, "source": selected_candidate.source},
                ttl_seconds=604800,  # 7å¤©
                provider=selected_candidate.source
            )
        
        self.logger.info(f"è·å–å­£åº¦åç§°æˆåŠŸ: {title} S{season_number:02d} â†’ {target_season.name} (æ¥æº: {selected_candidate.source})")
        return target_season.name

    async def search_all_metadata_sources(
        self,
        title: str,
        year: Optional[int] = None,
        media_type: str = "tv",
        sources: Optional[List[str]] = None,
        user: Optional[models.User] = None
    ) -> List[MetadataSearchCandidate]:
        """
        æœç´¢æ‰€æœ‰å…ƒæ•°æ®æº,è¿”å›å€™é€‰åˆ—è¡¨

        Args:
            title: æœç´¢æ ‡é¢˜
            year: å¹´ä»½(å¯é€‰)
            media_type: åª’ä½“ç±»å‹ ('tv' or 'movie')
            sources: è¦æœç´¢çš„æºåˆ—è¡¨,Noneè¡¨ç¤ºæœç´¢æ‰€æœ‰å¯ç”¨çš„æº
            user: ç”¨æˆ·å¯¹è±¡

        Returns:
            å€™é€‰ç»“æœåˆ—è¡¨
        """
        if not user:
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç”¨æˆ·å¯¹è±¡ç”¨äºAPIè°ƒç”¨
            user = models.User(id=0, username="system", isAdmin=True)

        # ç¡®å®šè¦æœç´¢çš„æº
        if sources is None:
            sources = ["tmdb"]  # é»˜è®¤åªæœç´¢TMDB,åç»­å¯æ‰©å±•

        all_candidates = []

        # å¹¶å‘æœç´¢æ‰€æœ‰æº
        for source_name in sources:
            try:
                source = self.metadata_manager.get_source(source_name)
                if not source:
                    self.logger.warning(f"å…ƒæ•°æ®æº '{source_name}' æœªæ‰¾åˆ°")
                    continue

                # è°ƒç”¨æºçš„æœç´¢æ–¹æ³•
                search_results = await source.search(title, user, mediaType=media_type)

                if not search_results:
                    self.logger.info(f"{source_name} æœç´¢æ— ç»“æœ: {title}")
                    continue

                # è½¬æ¢ä¸ºé€šç”¨æ ¼å¼
                for result in search_results[:10]:  # æ¯ä¸ªæºæœ€å¤šå–10ä¸ªç»“æœ
                    candidate = MetadataSearchCandidate(
                        source=source_name,
                        id=result.tmdbId or result.id,
                        title=result.title,
                        original_title=getattr(result, 'originalTitle', None),
                        year=result.year,
                        media_type=media_type,
                        overview=getattr(result, 'overview', None)
                    )
                    all_candidates.append(candidate)

                self.logger.info(f"{source_name} æœç´¢æˆåŠŸ: {title}, æ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")

            except Exception as e:
                self.logger.error(f"{source_name} æœç´¢å¤±è´¥: {title}, é”™è¯¯: {e}")
                continue

        return all_candidates

    async def get_seasons_from_source(
        self,
        source: str,
        id: str,
        media_type: str = "tv"
    ) -> List[SeasonInfo]:
        """
        ä»æŒ‡å®šå…ƒæ•°æ®æºè·å–å­£åº¦ä¿¡æ¯

        Args:
            source: å…ƒæ•°æ®æºåç§° ('tmdb', 'tvdb', etc.)
            id: æºçš„ID
            media_type: åª’ä½“ç±»å‹ ('tv' or 'movie')

        Returns:
            å­£åº¦ä¿¡æ¯åˆ—è¡¨
        """
        if media_type != "tv":
            return []

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{source}_seasons_{id}"
        async with self._session_factory() as session:
            cached_result = await crud.get_cache(session, cache_key)
            if cached_result:
                self.logger.info(f"{source} å­£åº¦ä¿¡æ¯ç¼“å­˜å‘½ä¸­: {id}")
                return [SeasonInfo(**s) for s in cached_result]

        # æ ¹æ®æºç±»å‹è°ƒç”¨ä¸åŒçš„API
        try:
            if source == "tmdb":
                seasons = await self._get_tmdb_seasons(id)
            # åç»­å¯æ‰©å±•å…¶ä»–æº
            # elif source == "tvdb":
            #     seasons = await self._get_tvdb_seasons(id)
            else:
                self.logger.warning(f"ä¸æ”¯æŒçš„å…ƒæ•°æ®æº: {source}")
                return []

            # ç¼“å­˜ç»“æœ(30å¤©)
            async with self._session_factory() as session:
                await crud.set_cache(
                    session,
                    cache_key,
                    [s.model_dump() for s in seasons],
                    ttl_seconds=2592000,  # 30å¤©
                    provider=source
                )

            self.logger.info(f"è·å–{source}å­£åº¦ä¿¡æ¯æˆåŠŸ: {id}, å…±{len(seasons)}å­£")
            return seasons

        except Exception as e:
            self.logger.error(f"è·å–{source}å­£åº¦ä¿¡æ¯å¤±è´¥: {id}, é”™è¯¯: {e}")
            return []

    async def _get_tmdb_seasons(self, tmdb_id: str) -> List[SeasonInfo]:
        """è·å–TMDBå­£åº¦ä¿¡æ¯(å†…éƒ¨æ–¹æ³•)"""
        tmdb_source = self.metadata_manager.get_source("tmdb")
        # ç›´æ¥è°ƒç”¨TMDB APIè·å–å­£åº¦ä¿¡æ¯
        async with await tmdb_source._create_client() as client:
            response = await client.get(f"/tv/{tmdb_id}")
            response.raise_for_status()
            data = response.json()

            seasons_data = data.get("seasons", [])
            seasons = []

            for season_data in seasons_data:
                # è·³è¿‡ç‰¹åˆ«ç¯‡(season 0)
                season_number = season_data.get("season_number", 0)
                if season_number == 0:
                    continue

                # è·å–å­£åº¦åˆ«å
                season_aliases = await self._get_season_aliases(
                    tmdb_id, season_number, season_data.get("name", "")
                )

                seasons.append(SeasonInfo(
                    season_number=season_number,
                    name=season_data.get("name"),
                    episode_count=season_data.get("episode_count", 0),
                    air_date=season_data.get("air_date"),
                    overview=season_data.get("overview"),
                    aliases=season_aliases
                ))

            return seasons

    async def _get_season_aliases(
        self,
        tmdb_id: str,
        season_number: int,
        season_name: str
    ) -> List[str]:
        """
        è·å–å­£åº¦åˆ«åï¼ŒåŒ…æ‹¬å¸¸è§çš„ä¸­è‹±æ–‡è¡¨è¾¾

        Args:
            tmdb_id: TMDB ID
            season_number: å­£åº¦å·
            season_name: å­£åº¦åç§°

        Returns:
            å­£åº¦åˆ«ååˆ—è¡¨
        """
        aliases = set()

        # æ·»åŠ åŸå§‹åç§°
        if season_name:
            aliases.add(season_name)

        # åŸºäºå­£åº¦å·ç”Ÿæˆå¸¸è§åˆ«å
        season_aliases_map = {
            1: ["ç¬¬ä¸€å­£", "ç¬¬1å­£", "Season 1", "S1", "ç¬¬ä¸€éƒ¨", "Part 1"],
            2: ["ç¬¬äºŒå­£", "ç¬¬2å­£", "Season 2", "S2", "ç¬¬äºŒéƒ¨", "Part 2", "II", "â…±"],
            3: ["ç¬¬ä¸‰å­£", "ç¬¬3å­£", "Season 3", "S3", "ç¬¬ä¸‰éƒ¨", "Part 3", "III", "â…²"],
            4: ["ç¬¬å››å­£", "ç¬¬4å­£", "Season 4", "S4", "ç¬¬å››éƒ¨", "Part 4", "IV", "â…³"],
            5: ["ç¬¬äº”å­£", "ç¬¬5å­£", "Season 5", "S5", "ç¬¬äº”éƒ¨", "Part 5", "V", "â…´"],
            6: ["ç¬¬å…­å­£", "ç¬¬6å­£", "Season 6", "S6", "ç¬¬å…­éƒ¨", "Part 6", "VI", "â…µ"],
        }

        # æ·»åŠ åŸºäºå­£åº¦å·çš„åˆ«å
        if season_number in season_aliases_map:
            aliases.update(season_aliases_map[season_number])

        # åŸºäºå­£åº¦åç§°ç”Ÿæˆç‰¹æ®Šåˆ«åï¼ˆç§»é™¤ç¡¬ç¼–ç ï¼Œè®©TMDBæ•°æ®è‡ªå·±è¯´è¯ï¼‰
        if season_name:
            # è‡ªåŠ¨ä»å­£åº¦åç§°ä¸­æå–å…³é”®è¯ä½œä¸ºåˆ«å
            import re

            # æå–å­£åº¦åç§°ä¸­çš„ç‹¬ç‰¹éƒ¨åˆ†ï¼ˆå»é™¤é€šç”¨å‰ç¼€ï¼‰
            cleaned_name = re.sub(r'^(ç¬¬?\d+å­£|Season\s*\d+|S\d+|[A-Z]+)\s*', '', season_name.strip())
            if cleaned_name:
                # æ·»åŠ æ¸…ç†åçš„åç§°ä½œä¸ºåˆ«å
                aliases.add(cleaned_name)

                # æ·»åŠ å®Œæ•´å­£åº¦åç§°çš„å˜ä½“
                base_name = re.sub(r'^(é¬¼ç­ä¹‹åˆƒ|åˆ€å‰‘ç¥åŸŸ|è¿›å‡»çš„å·¨äºº|Demon Slayer|Sword Art Online|Attack on Titan)\s*', '', cleaned_name.strip())
                if base_name and base_name != cleaned_name:
                    aliases.add(base_name)

        return list(aliases)


async def ai_season_mapping_and_correction(
    search_title: str,
    search_results: list,
    metadata_manager,
    ai_matcher,
    logger,
    similarity_threshold: float = 60.0,
    prefetched_metadata_results: list = None,  # ğŸš€ V2.1.6: é¢„å–çš„å…ƒæ•°æ®ç»“æœ
    metadata_source: str = "tmdb"  # å…ƒæ•°æ®æº
) -> list:
    """
    AIå­£åº¦æ˜ å°„ä¸ä¿®æ­£å‡½æ•° - V2.1.6ä¼˜åŒ–ç‰ˆæœ¬

    Args:
        search_title: æ ‡å‡†åŒ–çš„æœç´¢æ ‡é¢˜
        search_results: æœç´¢ç»“æœåˆ—è¡¨
        metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨
        ai_matcher: AIåŒ¹é…å™¨
        logger: æ—¥å¿—è®°å½•å™¨
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        prefetched_metadata_results: é¢„å–çš„å…ƒæ•°æ®æœç´¢ç»“æœï¼ˆç”¨äºå¹¶è¡Œä¼˜åŒ–ï¼‰
        metadata_source: å…ƒæ•°æ®æºåç§° (é»˜è®¤: tmdb)

    Returns:
        list: ä¿®æ­£ç»“æœåˆ—è¡¨
    """
    try:
        # 1. ä½¿ç”¨é¢„å–çš„å…ƒæ•°æ®ç»“æœæˆ–é‡æ–°æŸ¥è¯¢
        if prefetched_metadata_results:
            metadata_results = prefetched_metadata_results
            logger.info(f"âœ“ AIå­£åº¦æ˜ å°„: ä½¿ç”¨é¢„å–çš„[{metadata_source}]ç»“æœ ({len(metadata_results)} ä¸ª)")
        else:
            metadata_results = await _get_cached_metadata_search(search_title, metadata_manager, logger, metadata_source)

        if not metadata_results:
            logger.info(f"â—‹ AIå­£åº¦æ˜ å°„: æœªæ‰¾åˆ° '{search_title}' çš„[{metadata_source}]ä¿¡æ¯")
            return []

        # 2. å¦‚æœè¿”å›å¤šä¸ªç»“æœï¼Œä½¿ç”¨AIé€‰æ‹©æœ€ä½³åŒ¹é…
        if len(metadata_results) == 1:
            best_match = metadata_results[0]
            logger.info(f"â—‹ AIå­£åº¦æ˜ å°„: å”¯ä¸€[{metadata_source}]åŒ¹é…: {best_match.title} (ç±»å‹: {best_match.type})")
        else:
            # å¤šä¸ªç»“æœæ—¶ï¼ŒAIé€‰æ‹©æœ€ä½³åŒ¹é…
            try:
                provider_results = []
                for r in metadata_results:
                    provider_results.append(models.ProviderSearchInfo(
                        provider=metadata_source,
                        mediaId=r.tmdbId or r.id,
                        title=r.title,
                        type=r.type or "unknown",
                        season=1,
                        year=r.year,
                        imageUrl=r.imageUrl,
                        episodeCount=None
                    ))

                query_info = {
                    "title": search_title,
                    "season": None,
                    "episode": None,
                    "year": None,
                    "type": None
                }

                selected_index = await ai_matcher.select_best_match(
                    query_info, provider_results, {}
                )

                if selected_index is not None and 0 <= selected_index < len(metadata_results):
                    best_match = metadata_results[selected_index]
                    logger.info(f"âœ“ AIå­£åº¦æ˜ å°„: AIé€‰æ‹©[{metadata_source}]åŒ¹é…: {best_match.title} (ç±»å‹: {best_match.type}, ID: {best_match.id})")
                else:
                    logger.error(f"âš  AIå­£åº¦æ˜ å°„: AIé€‰æ‹©åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ")
                    best_match = metadata_results[0]

            except Exception as e:
                logger.error(f"âš  AIå­£åº¦æ˜ å°„: åŒ¹é…é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœ: {e}")
                best_match = metadata_results[0]

        # 3. è·å–å­£åº¦ä¿¡æ¯ç”¨äºåç»­ä¿®æ­£
        # ç¡®ä¿é€‰æ‹©çš„æ˜¯TVç±»å‹ï¼Œå¦åˆ™æ— æ³•è·å–å­£åº¦ä¿¡æ¯
        if best_match.type != 'tv':
            logger.warning(f"âš  AIå­£åº¦æ˜ å°„: é€‰æ‹©çš„ç»“æœä¸æ˜¯TVç±»å‹ ({best_match.type})ï¼Œæ— æ³•è·å–å­£åº¦ä¿¡æ¯")
            # å°è¯•æ‰¾åˆ°TVç±»å‹çš„ç»“æœ
            tv_result = None
            for result in metadata_results:
                if result.type == 'tv':
                    tv_result = result
                    logger.info(f"âœ“ AIå­£åº¦æ˜ å°„: æ‰¾åˆ°TVç±»å‹ç»“æœ: {tv_result.title} (ID: {tv_result.id})")
                    break

            if not tv_result:
                logger.error(f"âš  AIå­£åº¦æ˜ å°„: ç»“æœä¸­æ²¡æœ‰TVç±»å‹ï¼Œæ— æ³•è·å–å­£åº¦ä¿¡æ¯")
                return []

            best_match = tv_result

        try:
            seasons_info = await metadata_manager.get_seasons(metadata_source, best_match.id)
        except Exception as e:
            logger.error(f"è·å–[{metadata_source}]å­£åº¦ä¿¡æ¯å¤±è´¥: {best_match.id}, é”™è¯¯: {e}")
            return []

        if not seasons_info or len(seasons_info) <= 1:
            logger.info(f"â—‹ AIå­£åº¦æ˜ å°„: '{search_title}' åªæœ‰1ä¸ªå­£åº¦æˆ–æ— å­£åº¦ä¿¡æ¯ï¼Œè·³è¿‡")
            return []

        # èšåˆæ—¥å¿—æ”¶é›†
        log_lines = []
        log_lines.append(f"âœ“ AIå­£åº¦æ˜ å°„: è·å–åˆ° '{search_title}' çš„[{metadata_source}]å­£åº¦ä¿¡æ¯ï¼Œå…± {len(seasons_info)} ä¸ªå­£åº¦")
        for season in seasons_info:
            season_name = season.name or f"ç¬¬{season.season_number}å­£"
            log_lines.append(f"  - ç¬¬{season.season_number}å­£: {season_name}")

        # 4. å¯¹æ‰€æœ‰æœç´¢ç»“æœè¿›è¡Œå­£åº¦ä¿®æ­£
        tv_results = [item for item in search_results if item.type == 'tv_series']
        if not tv_results:
            log_lines.append(f"â—‹ AIå­£åº¦æ˜ å°„: æ²¡æœ‰TVç»“æœéœ€è¦ä¿®æ­£")
            logger.info("\n".join(log_lines))
            return []

        log_lines.append(f"â—‹ å¼€å§‹å­£åº¦ä¿®æ­£ï¼Œæ£€æŸ¥ {len(tv_results)} ä¸ªTVç»“æœ...")

        # è°ƒè¯•ï¼šæ‰“å°å­£åº¦ä¿¡æ¯è¯¦æƒ…
        log_lines.append(f"ğŸ” [{metadata_source}]å­£åº¦ä¿¡æ¯è¯¦æƒ…:")
        for season in seasons_info:
            aliases_str = ', '.join(season.aliases[:5]) if season.aliases else 'æ— '
            log_lines.append(f"  S{season.season_number}: {season.name} (åˆ«å: {aliases_str})")

        # 5. V2.1.6å¢å¼ºæ–¹æ¡ˆï¼šç®—æ³•ä¼˜å…ˆ + åˆ«åç­‰ä»·åŒ¹é…
        corrected_results = []

        # æ„å»ºæ ‡é¢˜åˆ«åç­‰ä»·æ˜ å°„
        title_alias_mapping = _build_title_alias_equivalence_map(tv_results, seasons_info, logger)

        # è·å–åŸºç¡€æ ‡é¢˜ï¼ˆç”¨äºå¤–ä¼ æ£€æµ‹ï¼‰
        base_title = search_title

        for item in tv_results:
            item_title = item.title
            best_season = item.season or 1
            best_confidence = 0.0
            best_season_name = ""
            best_method = "åŸå§‹"

            # V2.1.6æ–°å¢: å¤–ä¼ /è¡ç”Ÿä½œå“æ£€æµ‹ - è·³è¿‡å¤–ä¼ ä½œå“çš„å­£åº¦æ˜ å°„
            if is_spinoff_title(item_title, base_title):
                logger.debug(f"  â—‹ è·³è¿‡å¤–ä¼ ä½œå“: '{item_title}' (ä¿æŒåŸå­£åº¦ S{best_season})")
                continue

            logger.debug(f"  â—‹ æ£€æŸ¥ '{item_title}' çš„å­£åº¦åŒ¹é…...")

            # V2.1.7æ–°å¢: æ ‡é¢˜ä¸­æ˜ç¡®å­£åº¦ä¿¡æ¯ä¿æŠ¤
            # å¦‚æœæ ‡é¢˜å·²æ˜ç¡®åŒ…å«"ç¬¬Nå­£"ç­‰ä¿¡æ¯ï¼Œä¸”ä¸å½“å‰seasonä¸€è‡´ï¼Œåˆ™è·³è¿‡ä¿®æ­£
            explicit_season = _extract_explicit_season_from_title(item_title)
            if explicit_season is not None and explicit_season == item.season:
                logger.debug(f"  â—‹ æ ‡é¢˜å·²æ˜ç¡®åŒ…å«å­£åº¦ä¿¡æ¯: '{item_title}' â†’ S{explicit_season}ï¼Œè·³è¿‡ä¿®æ­£")
                continue

            # ç­–ç•¥1: åˆ«åç­‰ä»·åŒ¹é… (æœ€å¿«)
            equivalent_info = title_alias_mapping.get(item_title)
            if equivalent_info:
                best_season = equivalent_info['season']
                best_confidence = 98.0  # åˆ«åç­‰ä»·ç»™äºˆé«˜ç½®ä¿¡åº¦
                best_season_name = equivalent_info['name']
                best_method = "åˆ«åç­‰ä»·"
                logger.debug(f"    ğŸ¯ åˆ«åç­‰ä»·åŒ¹é…: S{best_season} ({best_season_name})")
            else:
                # ç­–ç•¥2: V2.1.6ç®—æ³•ç›¸ä¼¼åº¦åŒ¹é…
                for season in seasons_info:
                    season_num = season.season_number
                    season_name = season.name or f"ç¬¬{season_num}å­£"
                    season_aliases = season.aliases or []

                    # V2.1.6ï¼šä½¿ç”¨ç›¸ä¼¼åº¦è®¡ç®—
                    confidence = _calculate_season_similarity(
                        item_title,
                        season_name,
                        season_aliases
                    )

                    logger.debug(f"    - S{season_num} ({season_name}): ç›¸ä¼¼åº¦ {confidence:.1f}%")

                    # æ›´æ–°æœ€ä½³åŒ¹é…
                    if confidence > best_confidence and confidence >= similarity_threshold:
                        best_season = season_num
                        best_confidence = confidence
                        best_season_name = season_name
                        best_method = "ç®—æ³•ç›¸ä¼¼åº¦"

                # ç­–ç•¥3: AIè¾…åŠ© (ä»…å½“ç®—æ³•ç½®ä¿¡åº¦åœ¨æ¨¡ç³ŠåŒºé—´ 60-75% æ—¶)
                if similarity_threshold <= best_confidence < 75 and ai_matcher:
                    try:
                        # æ„å»ºå€™é€‰åˆ—è¡¨ä¾›AIé€‰æ‹©
                        candidates = [
                            {"season": s.season_number, "name": s.name or f"ç¬¬{s.season_number}å­£"}
                            for s in seasons_info
                        ]
                        ai_result = await ai_matcher.select_best_season_for_title(
                            item_title, candidates
                        )
                        if ai_result and ai_result.get('confidence', 0) > best_confidence:
                            best_season = ai_result['season']
                            best_confidence = ai_result['confidence']
                            best_season_name = ai_result.get('name', f"ç¬¬{best_season}å­£")
                            best_method = "AIè¾…åŠ©"
                            logger.debug(f"    ğŸ¤– AIè¾…åŠ©ç¡®è®¤: S{best_season} ({best_season_name})")
                    except Exception as e:
                        logger.debug(f"    AIè¾…åŠ©è·³è¿‡: {e}")

            # è®°å½•æœ€ç»ˆé€‰æ‹©
            if best_confidence >= similarity_threshold and item.season != best_season:
                correction = {
                    'item': item,
                    'original_season': item.season,
                    'corrected_season': best_season,
                    'confidence': best_confidence,
                    'tmdb_season_name': best_season_name,
                    'method': best_method
                }
                corrected_results.append(correction)
                log_lines.append(f"  âœ“ {best_method}ä¿®æ­£: '{item_title}' S{item.season or '?'} â†’ S{best_season} ({best_season_name}) (ç½®ä¿¡åº¦: {best_confidence:.1f}%)")
            elif best_confidence >= similarity_threshold:
                logger.debug(f"  â—‹ æ— éœ€ä¿®æ­£: '{item_title}' å·²æ˜¯æ­£ç¡®å­£åº¦ S{best_season} ({best_method}, ç½®ä¿¡åº¦: {best_confidence:.1f}%)")
            else:
                logger.debug(f"  â—‹ ç›¸ä¼¼åº¦ä¸è¶³: '{item_title}' ä¿æŒåŸå­£åº¦ S{item.season or '?'} (æœ€é«˜ç›¸ä¼¼åº¦: {best_confidence:.1f}% < {similarity_threshold}%)")

        log_lines.append(f"âœ“ å­£åº¦æ˜ å°„å®Œæˆ: ä¿®æ­£äº† {len(corrected_results)} ä¸ªç»“æœçš„å­£åº¦ä¿¡æ¯")
        # èšåˆå¼æ‰“å°æ‰€æœ‰æ—¥å¿—
        logger.info("\n".join(log_lines))
        return corrected_results

    except Exception as e:
        logger.warning(f"AIå­£åº¦æ˜ å°„å¤±è´¥: {e}")
        return []

            





async def ai_type_and_season_mapping_and_correction(
    search_title: str,
    search_results: list,
    metadata_manager,
    ai_matcher,
    logger,
    similarity_threshold: float = 60.0,
    prefetched_metadata_results: list = None,  # ğŸš€ V2.1.6: é¢„å–çš„å…ƒæ•°æ®ç»“æœ
    metadata_source: str = "tmdb"  # å…ƒæ•°æ®æº
) -> dict:
    """
    ç»Ÿä¸€çš„AIç±»å‹å’Œå­£åº¦æ˜ å°„ä¸ä¿®æ­£å‡½æ•°

    Args:
        search_title: æ ‡å‡†åŒ–çš„æœç´¢æ ‡é¢˜
        search_results: æœç´¢ç»“æœåˆ—è¡¨
        metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨
        ai_matcher: AIåŒ¹é…å™¨
        logger: æ—¥å¿—è®°å½•å™¨
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        prefetched_metadata_results: é¢„å–çš„å…ƒæ•°æ®æœç´¢ç»“æœï¼ˆå¯é€‰ï¼Œç”¨äºå¹¶è¡Œä¼˜åŒ–ï¼‰
        metadata_source: å…ƒæ•°æ®æºåç§° (é»˜è®¤: tmdb)

    Returns:
        dict: åŒ…å«ç±»å‹ä¿®æ­£å’Œå­£åº¦ä¿®æ­£çš„ç»“æœ
    """
    try:
        # èšåˆæ—¥å¿—æ”¶é›†
        unified_log_lines = []
        unified_log_lines.append(f"â—‹ å¼€å§‹ç»Ÿä¸€AIæ˜ å°„ä¿®æ­£: '{search_title}' ({len(search_results)} ä¸ªç»“æœ)")

        # åˆå§‹åŒ–ç»“æœ
        type_corrections = []
        season_corrections = []

        # 1. ç±»å‹ä¿®æ­£ï¼ˆç›®å‰ä¿æŒåŸç±»å‹ï¼‰
        unified_log_lines.append(f"â—‹ å¼€å§‹ç±»å‹ä¿®æ­£...")
        for item in search_results:
            original_type = item.type
            corrected_type = original_type
            if original_type != corrected_type:
                type_corrections.append({
                    'item': item,
                    'original_type': original_type,
                    'corrected_type': corrected_type
                })
                item.type = corrected_type
                unified_log_lines.append(f"  âœ“ ç±»å‹ä¿®æ­£: '{item.title}' {original_type} â†’ {corrected_type}")

        unified_log_lines.append(f"âœ“ ç±»å‹ä¿®æ­£å®Œæˆ: ä¿®æ­£äº† {len(type_corrections)} ä¸ªç»“æœçš„ç±»å‹ä¿¡æ¯")

        # 2. å­£åº¦ä¿®æ­£ï¼ˆåªå¯¹ç”µè§†å‰§è¿›è¡Œï¼‰
        tv_results = [item for item in search_results if item.type == 'tv_series']
        if tv_results:
            season_corrections = await ai_season_mapping_and_correction(
                search_title=search_title,
                search_results=search_results,
                metadata_manager=metadata_manager,
                ai_matcher=ai_matcher,
                logger=logger,
                similarity_threshold=similarity_threshold,
                prefetched_metadata_results=prefetched_metadata_results,
                metadata_source=metadata_source
            )

            # åº”ç”¨å­£åº¦ä¿®æ­£åˆ°åŸå§‹æœç´¢ç»“æœ
            for correction in season_corrections:
                item = correction['item']
                item.season = correction['corrected_season']
                unified_log_lines.append(f"  âœ“ å­£åº¦ä¿®æ­£åº”ç”¨: '{item.title}' â†’ S{item.season}")

        # 3. æ„å»ºä¿®æ­£åçš„ç»“æœåˆ—è¡¨
        corrected_results = search_results.copy()

        total_corrections = len(type_corrections) + len(season_corrections)
        unified_log_lines.append(f"âœ“ ç»Ÿä¸€AIæ˜ å°„ä¿®æ­£å®Œæˆ: ç±»å‹ä¿®æ­£ {len(type_corrections)} ä¸ª, å­£åº¦ä¿®æ­£ {len(season_corrections)} ä¸ª, æ€»è®¡ {total_corrections} ä¸ª")

        # èšåˆå¼æ‰“å°æ‰€æœ‰æ—¥å¿—
        logger.info("\n".join(unified_log_lines))

        return {
            'type_corrections': type_corrections,
            'season_corrections': season_corrections,
            'total_corrections': total_corrections,
            'corrected_results': corrected_results
        }

    except Exception as e:
        logger.warning(f"ç»Ÿä¸€AIæ˜ å°„ä¿®æ­£å¤±è´¥: {e}")
        return {
            'type_corrections': [],
            'season_corrections': [],
            'total_corrections': 0,
            'corrected_results': search_results
        }


async def _get_cached_metadata_search(
    search_title: str,
    metadata_manager,
    logger,
    source: str = "tmdb"  # æ”¯æŒå…¶ä»–å…ƒæ•°æ®æº
) -> List[models.MetadataDetailsResponse]:
    """
    è·å–å…ƒæ•°æ®æœç´¢ç»“æœï¼Œå¸¦6å°æ—¶ç¼“å­˜

    Args:
        search_title: æœç´¢æ ‡é¢˜
        metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨
        logger: æ—¥å¿—è®°å½•å™¨
        source: å…ƒæ•°æ®æºåç§° (é»˜è®¤: tmdbï¼Œå¯é€‰: bangumi ç­‰)

    Returns:
        List[models.MetadataDetailsResponse]: å…ƒæ•°æ®æœç´¢ç»“æœ
    """

    # ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŒ…å«æºåç§°ï¼‰
    cache_key = f"{source}_search_{hashlib.md5(search_title.encode('utf-8')).hexdigest()}"

    # æ£€æŸ¥ç¼“å­˜
    async with metadata_manager._session_factory() as session:
        cached_result = await crud.get_cache(session, cache_key)
        if cached_result:
            logger.info(f"[{source}] æœç´¢ç¼“å­˜å‘½ä¸­: {search_title}")
            return [models.MetadataDetailsResponse(**r) for r in cached_result]

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæœç´¢
    logger.debug(f"[{source}] æœç´¢ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæœç´¢: {search_title}")
    try:
        results = await metadata_manager.search(source, search_title, None, mediaType='multi')

        # ç¼“å­˜ç»“æœï¼ˆ6å°æ—¶ = 21600ç§’ï¼‰
        async with metadata_manager._session_factory() as session:
            await crud.set_cache(
                session,
                cache_key,
                [r.model_dump() for r in results],
                ttl_seconds=21600,  # 6å°æ—¶
                provider=source
            )
            logger.info(f"[{source}] æœç´¢ç»“æœå·²ç¼“å­˜: {search_title} (6å°æ—¶)")

        return results
    except Exception as e:
        logger.error(f"[{source}] æœç´¢å¤±è´¥: {search_title}, é”™è¯¯: {e}")
        return []


# ä¿æŒå‘åå…¼å®¹çš„åˆ«å
_get_cached_tmdb_search = _get_cached_metadata_search




