import json
import logging
import re
import secrets
from datetime import datetime, timedelta, timezone
from typing import List, Optional, Dict, Any, Type, Tuple
import xml.etree.ElementTree as ET
from pathlib import Path

from sqlalchemy import select, func, delete, update, and_, or_, text, distinct, case, exc, String
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import selectinload, joinedload, aliased, DeclarativeBase
from sqlalchemy.dialects.mysql import insert as mysql_insert
from sqlalchemy.dialects.postgresql import insert as postgresql_insert
from sqlalchemy.sql.elements import ColumnElement

from . import models
from . import orm_models
from .orm_models import ( # noqa: F401
    Anime, AnimeSource, Episode, User, Scraper, AnimeMetadata, Config, CacheData, ApiToken, TokenAccessLog, UaRule, BangumiAuth, OauthState, AnimeAlias, TmdbEpisodeMapping, ScheduledTask, TaskHistory, MetadataSource, ExternalApiLog, WebhookTask
, RateLimitState)
from .config import settings
from .timezone import get_now
from .danmaku_parser import parse_dandan_xml_to_comments
from .path_template import DanmakuPathTemplate, create_danmaku_context
from fastapi import Request

logger = logging.getLogger(__name__)

# --- æ–°å¢ï¼šæ–‡ä»¶å­˜å‚¨ç›¸å…³å¸¸é‡å’Œè¾…åŠ©å‡½æ•° ---
def _is_docker_environment():
    """æ£€æµ‹æ˜¯å¦åœ¨Dockerå®¹å™¨ä¸­è¿è¡Œ"""
    import os
    # æ–¹æ³•1: æ£€æŸ¥ /.dockerenv æ–‡ä»¶ï¼ˆDockeræ ‡å‡†åšæ³•ï¼‰
    if Path("/.dockerenv").exists():
        return True
    # æ–¹æ³•2: æ£€æŸ¥ç¯å¢ƒå˜é‡
    if os.getenv("DOCKER_CONTAINER") == "true" or os.getenv("IN_DOCKER") == "true":
        return True
    # æ–¹æ³•3: æ£€æŸ¥å½“å‰å·¥ä½œç›®å½•æ˜¯å¦ä¸º /app
    if Path.cwd() == Path("/app"):
        return True
    return False

def _get_base_dir():
    """è·å–åŸºç¡€ç›®å½•ï¼Œæ ¹æ®è¿è¡Œç¯å¢ƒè‡ªåŠ¨è°ƒæ•´"""
    if _is_docker_environment():
        return Path("/app")
    else:
        # æºç è¿è¡Œç¯å¢ƒï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
        return Path(".")

BASE_DIR = _get_base_dir()
DANMAKU_BASE_DIR = BASE_DIR / "config/danmaku"

def _generate_xml_from_comments(
    comments: List[Dict[str, Any]], 
    episode_id: int, 
    provider_name: Optional[str] = "misaka",
    chat_server: Optional[str] = "danmaku.misaka.org"
) -> str:
    """æ ¹æ®å¼¹å¹•å­—å…¸åˆ—è¡¨ç”Ÿæˆç¬¦åˆdandanplayæ ‡å‡†çš„XMLå­—ç¬¦ä¸²ã€‚"""
    root = ET.Element('i')
    ET.SubElement(root, 'chatserver').text = chat_server
    ET.SubElement(root, 'chatid').text = str(episode_id)
    ET.SubElement(root, 'mission').text = '0'
    ET.SubElement(root, 'maxlimit').text = '2000'
    ET.SubElement(root, 'source').text = 'k-v' # ä¿æŒä¸å®˜æ–¹æ ¼å¼ä¸€è‡´
    # æ–°å¢å­—æ®µ
    ET.SubElement(root, 'sourceprovider').text = provider_name
    ET.SubElement(root, 'datasize').text = str(len(comments))
    
    for comment in comments:
        p_attr = str(comment.get('p', ''))
        d = ET.SubElement(root, 'd', p=p_attr)
        d.text = comment.get('m', '')
    return ET.tostring(root, encoding='unicode', xml_declaration=True)

def _get_fs_path_from_web_path(web_path: Optional[str]) -> Optional[Path]:
    """
    å°†Webè·¯å¾„è½¬æ¢ä¸ºæ–‡ä»¶ç³»ç»Ÿè·¯å¾„ã€‚
    ç°åœ¨æ”¯æŒç»å¯¹è·¯å¾„æ ¼å¼ï¼ˆå¦‚ /app/config/danmaku/1/2.xmlï¼‰å’Œè‡ªå®šä¹‰è·¯å¾„ã€‚
    """
    if not web_path:
        return None

    # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
    if web_path.startswith('/app/'):
        # ç§»é™¤ /app/ å‰ç¼€ï¼Œè½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
        return Path(web_path[5:])  # ç§»é™¤ "/app/" å‰ç¼€
    elif web_path.startswith('/'):
        # å…¶ä»–ç»å¯¹è·¯å¾„ä¿æŒä¸å˜ï¼ˆç”¨æˆ·è‡ªå®šä¹‰çš„ç»å¯¹è·¯å¾„ï¼‰
        return Path(web_path)

    # å…¼å®¹æ—§çš„ç›¸å¯¹è·¯å¾„æ ¼å¼
    if '/danmaku/' in web_path:
        relative_part = web_path.split('/danmaku/', 1)[1]
        return DANMAKU_BASE_DIR / relative_part
    elif '/custom_danmaku/' in web_path:
        # å¤„ç†è‡ªå®šä¹‰è·¯å¾„
        relative_part = web_path.split('/custom_danmaku/', 1)[1]
        return Path(relative_part)

    logger.warning(f"æ— æ³•ä»Webè·¯å¾„ '{web_path}' è§£ææ–‡ä»¶ç³»ç»Ÿè·¯å¾„: {web_path}")
    return None

async def _generate_danmaku_path(session: AsyncSession, episode, config_manager=None) -> tuple[str, Path]:
    """
    ç”Ÿæˆå¼¹å¹•æ–‡ä»¶çš„Webè·¯å¾„å’Œæ–‡ä»¶ç³»ç»Ÿè·¯å¾„

    Returns:
        tuple: (web_path, absolute_path)
    """
    anime_id = episode.source.anime.id
    episode_id = episode.id

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªå®šä¹‰è·¯å¾„
    custom_path_enabled = False
    custom_template = None

    if config_manager:
        try:
            custom_path_enabled_str = await config_manager.get('customDanmakuPathEnabled', 'false')
            custom_path_enabled = custom_path_enabled_str.lower() == 'true'
            if custom_path_enabled:
                custom_template = await config_manager.get('customDanmakuPathTemplate', '')
        except Exception as e:
            logger.warning(f"è·å–è‡ªå®šä¹‰è·¯å¾„é…ç½®å¤±è´¥: {e}")

    if custom_path_enabled and custom_template:
        try:
            # åˆ›å»ºè·¯å¾„æ¨¡æ¿ä¸Šä¸‹æ–‡
            context = create_danmaku_context(
                anime_title=episode.source.anime.title,
                season=episode.source.anime.season or 1,
                episode_index=episode.episodeIndex,
                year=episode.source.anime.year,
                provider=episode.source.providerName,
                anime_id=anime_id,
                episode_id=episode_id,
                source_id=episode.source.id
            )

            # ç”Ÿæˆè‡ªå®šä¹‰è·¯å¾„
            path_template = DanmakuPathTemplate(custom_template)
            custom_path = path_template.generate_path(context)

            # è‡ªå®šä¹‰è·¯å¾„ä½¿ç”¨ç»å¯¹è·¯å¾„å­˜å‚¨
            web_path = str(custom_path)  # ç»å¯¹è·¯å¾„ç”¨äºæ•°æ®åº“å­˜å‚¨
            absolute_path = Path(custom_path)  # ç›´æ¥ä½¿ç”¨ç”Ÿæˆçš„è·¯å¾„

            logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„æ¨¡æ¿ç”Ÿæˆå¼¹å¹•è·¯å¾„: {absolute_path}")
            return web_path, absolute_path

        except Exception as e:
            logger.error(f"ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„æ¨¡æ¿å¤±è´¥: {e}ï¼Œå›é€€åˆ°é»˜è®¤è·¯å¾„")

    # é»˜è®¤è·¯å¾„é€»è¾‘ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„
    web_path = f"/app/config/danmaku/{anime_id}/{episode_id}.xml"  # ä¿æŒæ•°æ®åº“ä¸­çš„æ ¼å¼ä¸€è‡´æ€§
    absolute_path = DANMAKU_BASE_DIR / str(anime_id) / f"{episode_id}.xml"

    return web_path, absolute_path
# --- Anime & Library ---

async def get_library_anime(session: AsyncSession, keyword: Optional[str] = None, page: int = 1, page_size: int = -1) -> Dict[str, Any]:
    """è·å–åª’ä½“åº“ä¸­çš„æ‰€æœ‰ç•ªå‰§åŠå…¶å…³è”ä¿¡æ¯ï¼ˆå¦‚åˆ†é›†æ•°ï¼‰ï¼Œæ”¯æŒæœç´¢å’Œåˆ†é¡µã€‚"""
    stmt = (
        select(
            Anime.id.label("animeId"),
            Anime.localImagePath.label("localImagePath"),
            Anime.imageUrl.label("imageUrl"),
            Anime.title,
            Anime.type,
            Anime.season,
            Anime.year,
            func.coalesce(Anime.createdAt, func.now()).label("createdAt"),  # å¤„ç†NULLå€¼
            case(
                (Anime.type == 'movie', 1),
                else_=func.coalesce(func.max(Episode.episodeIndex), 0)
            ).label("episodeCount"),
            func.count(distinct(AnimeSource.id)).label("sourceCount")
        )
        .join(AnimeSource, Anime.id == AnimeSource.animeId, isouter=True)
        .join(Episode, AnimeSource.id == Episode.sourceId, isouter=True)
        .join(AnimeAlias, Anime.id == AnimeAlias.animeId, isouter=True)
        .group_by(Anime.id)
    )

    if keyword:
        clean_keyword = keyword.strip()
        if clean_keyword:
            normalized_like_keyword = f"%{clean_keyword.replace('ï¼š', ':').replace(' ', '')}%"
            like_conditions = [
                func.replace(func.replace(col, 'ï¼š', ':'), ' ', '').like(normalized_like_keyword)
                for col in [Anime.title, AnimeAlias.nameEn, AnimeAlias.nameJp, AnimeAlias.nameRomaji, AnimeAlias.aliasCn1, AnimeAlias.aliasCn2, AnimeAlias.aliasCn3]
            ]
            stmt = stmt.where(or_(*like_conditions))

    count_subquery = stmt.alias("count_subquery")
    count_stmt = select(func.count()).select_from(count_subquery)
    total_count = (await session.execute(count_stmt)).scalar_one()

    data_stmt = stmt.order_by(Anime.createdAt.desc())
    if page_size > 0:
        offset = (page - 1) * page_size
        data_stmt = data_stmt.offset(offset).limit(page_size)
    
    result = await session.execute(data_stmt)
    items = [dict(row) for row in result.mappings()]
    return {"total": total_count, "list": items}

async def get_library_anime_by_id(session: AsyncSession, anime_id: int) -> Optional[Dict[str, Any]]:
    """
    Gets a single anime from the library by its ID, with counts.
    """
    stmt = (
        select(
            Anime.id.label("animeId"),
            Anime.localImagePath.label("localImagePath"),
            Anime.imageUrl.label("imageUrl"),
            Anime.title,
            Anime.type,
            Anime.season,
            Anime.year,
            func.coalesce(Anime.createdAt, func.now()).label("createdAt"),  # å¤„ç†NULLå€¼
            case(
                (Anime.type == 'movie', 1),
                else_=func.coalesce(func.max(Episode.episodeIndex), 0)
            ).label("episodeCount"),
            func.count(distinct(AnimeSource.id)).label("sourceCount")
        )
        .join(AnimeSource, Anime.id == AnimeSource.animeId, isouter=True)
        .join(Episode, AnimeSource.id == Episode.sourceId, isouter=True)
        .where(Anime.id == anime_id)
        .group_by(Anime.id)
    )
    result = await session.execute(stmt)
    row = result.mappings().one_or_none()
    return dict(row) if row else None

async def get_last_episode_for_source(session: AsyncSession, sourceId: int) -> Optional[Dict[str, Any]]:
    """è·å–æŒ‡å®šæºçš„æœ€åä¸€ä¸ªåˆ†é›†ã€‚"""
    stmt = (
        select(Episode.episodeIndex.label("episodeIndex"))
        .where(Episode.sourceId == sourceId)
        .order_by(Episode.episodeIndex.desc())
        .limit(1)
    )
    result = await session.execute(stmt)
    row = result.mappings().first()
    return dict(row) if row else None

async def get_episode_for_refresh(session: AsyncSession, episodeId: int) -> Optional[Dict[str, Any]]:
    """è·å–ç”¨äºåˆ·æ–°çš„åˆ†é›†ä¿¡æ¯ã€‚"""
    stmt = (
        select(Episode.id, Episode.title, AnimeSource.providerName)
        .join(AnimeSource, Episode.sourceId == AnimeSource.id)
        .where(Episode.id == episodeId)
    )
    result = await session.execute(stmt)
    row = result.mappings().first()
    return dict(row) if row else None

async def get_or_create_anime(session: AsyncSession, title: str, media_type: str, season: int, image_url: Optional[str], local_image_path: Optional[str], year: Optional[int] = None, title_recognition_manager=None, source: Optional[str] = None) -> int:
    """é€šè¿‡æ ‡é¢˜ã€å­£åº¦å’Œå¹´ä»½æŸ¥æ‰¾ç•ªå‰§ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºã€‚å¦‚æœå­˜åœ¨ä½†ç¼ºå°‘æµ·æŠ¥ï¼Œåˆ™æ›´æ–°æµ·æŠ¥ã€‚è¿”å›å…¶IDã€‚
    ä¼˜å…ˆè¿›è¡Œå®Œå…¨åŒ¹é…ï¼Œåªæœ‰åœ¨æ²¡æœ‰æ‰¾åˆ°æ—¶æ‰åº”ç”¨è¯†åˆ«è¯è½¬æ¢ã€‚"""
    logger.info(f"å¼€å§‹å¤„ç†ç•ªå‰§: åŸå§‹æ ‡é¢˜='{title}', å­£æ•°={season}, å¹´ä»½={year}")

    original_title = title
    original_season = season

    # æ­¥éª¤1ï¼šå…ˆå°è¯•å®Œå…¨åŒ¹é…ï¼ˆä¸åº”ç”¨è¯†åˆ«è¯è½¬æ¢ï¼‰
    logger.info(f"ğŸ” æ•°æ®åº“æŸ¥æ‰¾ï¼ˆå®Œå…¨åŒ¹é…ï¼‰: title='{original_title}', season={original_season}, year={year}")
    stmt = select(Anime).where(Anime.title == original_title, Anime.season == original_season)
    if year:
        stmt = stmt.where(Anime.year == year)
    result = await session.execute(stmt)
    anime = result.scalar_one_or_none()

    if anime:
        logger.info(f"âœ“ å®Œå…¨åŒ¹é…æˆåŠŸ: ID={anime.id}, æ ‡é¢˜='{anime.title}', å­£æ•°={anime.season}, å¹´ä»½={anime.year}")
        # æ£€æŸ¥å¹¶æ›´æ–°æµ·æŠ¥
        if not anime.imageUrl and (image_url or local_image_path):
            if image_url:
                anime.imageUrl = image_url
                logger.info(f"æ›´æ–°æµ·æŠ¥URL: {image_url}")
            if local_image_path:
                anime.localImagePath = local_image_path
                logger.info(f"æ›´æ–°æœ¬åœ°æµ·æŠ¥è·¯å¾„: {local_image_path}")
            await session.commit()
        return anime.id

    # æ­¥éª¤2ï¼šå¦‚æœå®Œå…¨åŒ¹é…å¤±è´¥ï¼Œå°è¯•åº”ç”¨è¯†åˆ«è¯è½¬æ¢
    logger.info(f"â—‹ å®Œå…¨åŒ¹é…å¤±è´¥: æœªæ‰¾åˆ°åŒ¹é…çš„ç•ªå‰§")

    converted_title = original_title
    converted_season = original_season
    was_converted = False
    metadata_info = None

    if title_recognition_manager:
        # å…ˆå°è¯•ç”¨åŸå§‹æ ‡é¢˜è¿›è¡Œå…¥åº“åå¤„ç†ï¼ˆæŸ¥æ‰¾æ˜¯å¦æœ‰åŒ¹é…çš„åç§»è§„åˆ™ï¼‰
        converted_title, converted_season, was_converted, metadata_info = await title_recognition_manager.apply_storage_postprocessing(title, season, source)

        if was_converted:
            original_season_str = f"S{original_season:02d}" if original_season is not None else "S??"
            converted_season_str = f"S{converted_season:02d}" if converted_season is not None else "S??"
            logger.info(f"ğŸ” å°è¯•è¯†åˆ«è¯è½¬æ¢åŒ¹é…: '{original_title}' {original_season_str} -> '{converted_title}' {converted_season_str}")

            # ä½¿ç”¨è½¬æ¢åçš„æ ‡é¢˜å’Œå­£æ•°è¿›è¡ŒæŸ¥æ‰¾
            stmt = select(Anime).where(Anime.title == converted_title, Anime.season == converted_season)
            if year:
                stmt = stmt.where(Anime.year == year)
            result = await session.execute(stmt)
            anime = result.scalar_one_or_none()

            if anime:
                logger.info(f"âœ“ è¯†åˆ«è¯è½¬æ¢åŒ¹é…æˆåŠŸ: ID={anime.id}, æ ‡é¢˜='{anime.title}', å­£æ•°={anime.season}, å¹´ä»½={anime.year}")
                # æ£€æŸ¥å¹¶æ›´æ–°æµ·æŠ¥
                if not anime.imageUrl and (image_url or local_image_path):
                    if image_url:
                        anime.imageUrl = image_url
                        logger.info(f"æ›´æ–°æµ·æŠ¥URL: {image_url}")
                    if local_image_path:
                        anime.localImagePath = local_image_path
                        logger.info(f"æ›´æ–°æœ¬åœ°æµ·æŠ¥è·¯å¾„: {local_image_path}")
                    await session.commit()
                return anime.id
            else:
                logger.info(f"â—‹ è¯†åˆ«è¯è½¬æ¢åŒ¹é…ä¹Ÿå¤±è´¥: æœªæ‰¾åˆ°åŒ¹é…çš„ç•ªå‰§")
        else:
            original_season_str = f"S{original_season:02d}" if original_season is not None else "S??"
            logger.info(f"â—‹ æ ‡é¢˜è¯†åˆ«è½¬æ¢æœªç”Ÿæ•ˆ: '{original_title}' {original_season_str} (æ— åŒ¹é…è§„åˆ™)")

    # æ­¥éª¤3ï¼šå¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œåˆ›å»ºæ–°ç•ªå‰§
    # å¦‚æœè¯†åˆ«è¯è½¬æ¢ç”Ÿæ•ˆäº†ï¼Œä½¿ç”¨è½¬æ¢åçš„æ ‡é¢˜å’Œå­£æ•°ï¼›å¦åˆ™ä½¿ç”¨åŸå§‹æ ‡é¢˜
    final_title = converted_title if was_converted else original_title
    final_season = converted_season if was_converted else original_season

    logger.info(f"åˆ›å»ºæ–°ç•ªå‰§: æ ‡é¢˜='{final_title}', å­£æ•°={final_season}, ç±»å‹={media_type}")
    if was_converted:
        logger.info(f"âœ“ ä½¿ç”¨è¯†åˆ«è¯è½¬æ¢åçš„æ ‡é¢˜å’Œå­£æ•°åˆ›å»ºæ–°æ¡ç›®")

    from .timezone import get_now
    created_time = get_now()
    logger.info(f"è®¾ç½®åˆ›å»ºæ—¶é—´: {created_time}")
    new_anime = Anime(
        title=final_title,  # ä½¿ç”¨æœ€ç»ˆæ ‡é¢˜åˆ›å»º
        season=final_season,  # ä½¿ç”¨æœ€ç»ˆå­£æ•°åˆ›å»º
        type=media_type,
        year=year,
        imageUrl=image_url,
        localImagePath=local_image_path,
        createdAt=created_time  # è®¾ç½®åˆ›å»ºæ—¶é—´
    )
    session.add(new_anime)
    await session.flush()  # è·å–IDä½†ä¸æäº¤äº‹åŠ¡
    logger.info(f"æ–°ç•ªå‰§åˆ›å»ºå®Œæˆ: ID={new_anime.id}, æ ‡é¢˜='{new_anime.title}', å­£æ•°={new_anime.season}, createdAt={new_anime.createdAt}")
    return new_anime.id

async def create_anime(session: AsyncSession, anime_data: models.AnimeCreate) -> Anime:
    """
    Manually creates a new anime entry in the database, and automatically
    creates and links a default 'custom' source for it.
    """
    # ä¿®æ­£ï¼šåœ¨é‡å¤æ£€æŸ¥æ—¶ä¹ŸåŒ…å«å¹´ä»½
    existing_anime = await find_anime_by_title_season_year(
        session, anime_data.title, anime_data.season, anime_data.year
    )
    if existing_anime:
        raise ValueError(f"ä½œå“ '{anime_data.title}' (ç¬¬ {anime_data.season} å­£) å·²å­˜åœ¨ã€‚")

    created_time = get_now().replace(tzinfo=None)
    logger.info(f"create_anime: è®¾ç½®åˆ›å»ºæ—¶é—´: {created_time}")
    new_anime = Anime(
        title=anime_data.title,
        type=anime_data.type,
        season=anime_data.season,
        year=anime_data.year,
        imageUrl=anime_data.imageUrl,
        createdAt=created_time
    )
    session.add(new_anime)
    await session.flush()
    
    # Create associated metadata and alias records
    new_metadata = AnimeMetadata(animeId=new_anime.id)
    new_alias = AnimeAlias(animeId=new_anime.id)
    session.add_all([new_metadata, new_alias])
    
    # ä¿®æ­£ï¼šåœ¨åˆ›å»ºæ–°ä½œå“æ—¶ï¼Œè‡ªåŠ¨ä¸ºå…¶åˆ›å»ºä¸€ä¸ª'custom'æ•°æ®æºã€‚
    # è¿™ç®€åŒ–äº†ç”¨æˆ·æ“ä½œï¼Œå¹¶ä»æ ¹æºä¸Šç¡®ä¿äº†æ•°æ®å®Œæ•´æ€§ï¼Œ
    # å› ä¸º link_source_to_anime ä¼šè´Ÿè´£åœ¨ scrapers è¡¨ä¸­åˆ›å»ºå¯¹åº”çš„æ¡ç›®ã€‚
    logger.info(f"ä¸ºæ–°ä½œå“ '{anime_data.title}' è‡ªåŠ¨åˆ›å»º 'custom' æ•°æ®æºã€‚")
    custom_media_id = f"custom_{new_anime.id}"
    await link_source_to_anime(session, new_anime.id, "custom", custom_media_id)
    
    await session.flush()
    await session.refresh(new_anime)
    return new_anime

async def update_anime_aliases(session: AsyncSession, anime_id: int, payload: Any):
    """
    Updates the aliases for a given anime.
    The payload can be any object with the alias attributes.
    """
    stmt = select(AnimeAlias).where(AnimeAlias.animeId == anime_id)
    result = await session.execute(stmt)
    alias_record = result.scalar_one_or_none()

    if not alias_record:
        alias_record = AnimeAlias(animeId=anime_id)
        session.add(alias_record)
    
    alias_record.nameEn = getattr(payload, 'nameEn', alias_record.nameEn)
    alias_record.nameJp = getattr(payload, 'nameJp', alias_record.nameJp)
    alias_record.nameRomaji = getattr(payload, 'nameRomaji', alias_record.nameRomaji)
    alias_record.aliasCn1 = getattr(payload, 'aliasCn1', alias_record.aliasCn1)
    alias_record.aliasCn2 = getattr(payload, 'aliasCn2', alias_record.aliasCn2)
    alias_record.aliasCn3 = getattr(payload, 'aliasCn3', alias_record.aliasCn3)
    
    await session.flush()

async def update_anime_details(session: AsyncSession, anime_id: int, update_data: models.AnimeDetailUpdate) -> bool:
    """åœ¨äº‹åŠ¡ä¸­æ›´æ–°ç•ªå‰§çš„æ ¸å¿ƒä¿¡æ¯ã€å…ƒæ•°æ®å’Œåˆ«åã€‚"""
    anime = await session.get(Anime, anime_id, options=[selectinload(Anime.metadataRecord), selectinload(Anime.aliases)])
    if not anime:
        return False

    # Update Anime table
    anime.title = update_data.title
    anime.type = update_data.type
    anime.season = update_data.season
    anime.episodeCount = update_data.episodeCount
    anime.year = update_data.year
    anime.imageUrl = update_data.imageUrl

    # Update or create AnimeMetadata
    if not anime.metadataRecord:
        anime.metadataRecord = AnimeMetadata(animeId=anime_id)
    anime.metadataRecord.tmdbId = update_data.tmdbId
    anime.metadataRecord.tmdbEpisodeGroupId = update_data.tmdbEpisodeGroupId
    anime.metadataRecord.bangumiId = update_data.bangumiId
    anime.metadataRecord.tvdbId = update_data.tvdbId
    anime.metadataRecord.doubanId = update_data.doubanId
    anime.metadataRecord.imdbId = update_data.imdbId

    # Update or create AnimeAlias
    if not anime.aliases:
        anime.aliases = AnimeAlias(animeId=anime_id)
    anime.aliases.nameEn = update_data.nameEn
    anime.aliases.nameJp = update_data.nameJp
    anime.aliases.nameRomaji = update_data.nameRomaji
    anime.aliases.aliasCn1 = update_data.aliasCn1
    anime.aliases.aliasCn2 = update_data.aliasCn2
    anime.aliases.aliasCn3 = update_data.aliasCn3

    await session.commit()
    return True

async def delete_anime(session: AsyncSession, anime_id: int) -> bool:
    """åˆ é™¤ä¸€ä¸ªä½œå“åŠå…¶æ‰€æœ‰å…³è”æ•°æ®ï¼ˆé€šè¿‡çº§è”åˆ é™¤ï¼‰ã€‚"""
    import shutil
    anime = await session.get(Anime, anime_id)
    if anime:
        await session.delete(anime)
        await session.commit()
        return True
    return False

async def search_anime(session: AsyncSession, keyword: str) -> List[Dict[str, Any]]:
    """åœ¨æ•°æ®åº“ä¸­æœç´¢ç•ªå‰§ (ä½¿ç”¨FULLTEXTç´¢å¼•)"""
    sanitized_keyword = re.sub(r'[+\-><()~*@"]', ' ', keyword).strip()
    if not sanitized_keyword:
        return []

    # ä¿®æ­£ï¼šä½¿ç”¨ LIKE ä»£æ›¿ MATCH...AGAINST ä»¥å…¼å®¹ PostgreSQL
    # æ³¨æ„ï¼šè¿™ä¼šæ¯”å…¨æ–‡ç´¢å¼•æ…¢ï¼Œä½†æä¾›äº†è·¨æ•°æ®åº“çš„å…¼å®¹æ€§ã€‚
    # å¯¹äºé«˜æ€§èƒ½éœ€æ±‚ï¼Œå¯ä»¥è€ƒè™‘ä¸ºæ¯ä¸ªæ•°æ®åº“æ–¹è¨€å®ç°ç‰¹å®šçš„å…¨æ–‡æœç´¢æŸ¥è¯¢ã€‚
    stmt = select(Anime.id, Anime.title, Anime.type).where(
        Anime.title.like(f"%{sanitized_keyword}%")
    ).order_by(func.length(Anime.title)) # æŒ‰æ ‡é¢˜é•¿åº¦æ’åºï¼Œè¾ƒçŸ­çš„åŒ¹é…æ›´ç›¸å…³

    result = await session.execute(stmt)
    return [dict(row) for row in result.mappings()]

async def search_episodes_in_library(session: AsyncSession, anime_title: str, episode_number: Optional[int], season_number: Optional[int] = None) -> List[Dict[str, Any]]:
    """åœ¨æœ¬åœ°åº“ä¸­é€šè¿‡ç•ªå‰§æ ‡é¢˜å’Œå¯é€‰çš„é›†æ•°æœç´¢åŒ¹é…çš„åˆ†é›†ã€‚"""
    clean_title = anime_title.strip()
    if not clean_title:
        return []

    # Base query
    stmt = (
        select(
            Anime.id.label("animeId"),
            Anime.title.label("animeTitle"),
            Anime.type,
            Anime.imageUrl.label("imageUrl"),
            Anime.createdAt.label("startDate"),
            Episode.id.label("episodeId"),
            case((Anime.type == 'movie', func.concat(Scraper.providerName, ' æº')), else_=Episode.title).label("episodeTitle"),
            AnimeAlias.nameEn,
            AnimeAlias.nameJp,
            AnimeAlias.nameRomaji,
            AnimeAlias.aliasCn1,
            AnimeAlias.aliasCn2,
            AnimeAlias.aliasCn3,
            Scraper.displayOrder,
            AnimeSource.isFavorited.label("isFavorited"),
            AnimeMetadata.bangumiId.label("bangumiId")
        )
        .join(AnimeSource, Anime.id == AnimeSource.animeId)
        .join(Episode, AnimeSource.id == Episode.sourceId)
        .join(Scraper, AnimeSource.providerName == Scraper.providerName)
        .join(AnimeMetadata, Anime.id == AnimeMetadata.animeId, isouter=True)
        .join(AnimeAlias, Anime.id == AnimeAlias.animeId, isouter=True)
    )

    # Add conditions
    if episode_number is not None:
        stmt = stmt.where(Episode.episodeIndex == episode_number)
    if season_number is not None:
        stmt = stmt.where(Anime.season == season_number)

    # Title condition
    normalized_like_title = f"%{clean_title.replace('ï¼š', ':').replace(' ', '')}%"
    like_conditions = [
        func.replace(func.replace(col, 'ï¼š', ':'), ' ', '').like(normalized_like_title)
        for col in [Anime.title, AnimeAlias.nameEn, AnimeAlias.nameJp, AnimeAlias.nameRomaji,
                    AnimeAlias.aliasCn1, AnimeAlias.aliasCn2, AnimeAlias.aliasCn3]
    ]
    stmt = stmt.where(or_(*like_conditions))

    # Order and execute
    # ä¿®æ­£ï¼šæŒ‰é›†æ•°æ’åºï¼Œç¡®ä¿episodesæŒ‰æ­£ç¡®é¡ºåºè¿”å›
    stmt = stmt.order_by(func.length(Anime.title), Scraper.displayOrder, Episode.episodeIndex)
    result = await session.execute(stmt)
    return [dict(row) for row in result.mappings()]

async def find_anime_by_title_season_year(session: AsyncSession, title: str, season: int, year: Optional[int] = None, title_recognition_manager=None, source: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    é€šè¿‡æ ‡é¢˜ã€å­£åº¦å’Œå¯é€‰çš„å¹´ä»½æŸ¥æ‰¾ç•ªå‰§ï¼Œè¿”å›ä¸€ä¸ªç®€åŒ–çš„å­—å…¸æˆ–Noneã€‚
    ä¼˜å…ˆè¿›è¡Œå®Œå…¨åŒ¹é…ï¼Œåªæœ‰åœ¨æ²¡æœ‰æ‰¾åˆ°æ—¶æ‰åº”ç”¨è¯†åˆ«è¯è½¬æ¢ã€‚
    """
    original_title = title
    original_season = season

    # æ­¥éª¤1ï¼šå…ˆå°è¯•å®Œå…¨åŒ¹é…ï¼ˆä¸åº”ç”¨è¯†åˆ«è¯è½¬æ¢ï¼‰
    logger.info(f"ğŸ” æ•°æ®åº“æŸ¥æ‰¾: title='{original_title}', season={original_season}, year={year}")
    stmt = (
        select(
            Anime.id,
            Anime.title,
            Anime.season,
            Anime.year
        )
        .where(Anime.title == original_title, Anime.season == original_season)
        .limit(1)
    )
    if year:
        stmt = stmt.where(Anime.year == year)
    result = await session.execute(stmt)
    row = result.mappings().first()

    if row:
        original_season_str = f"S{original_season:02d}" if original_season is not None else "S??"
        logger.info(f"âœ“ å®Œå…¨åŒ¹é…æˆåŠŸ: æ‰¾åˆ°ä½œå“ '{original_title}' {original_season_str}")
        return dict(row)

    # æ­¥éª¤2ï¼šå¦‚æœå®Œå…¨åŒ¹é…å¤±è´¥ï¼Œå°è¯•åº”ç”¨è¯†åˆ«è¯è½¬æ¢
    logger.info(f"â—‹ å®Œå…¨åŒ¹é…å¤±è´¥: æœªæ‰¾åˆ°åŒ¹é…çš„ç•ªå‰§")

    if title_recognition_manager:
        converted_title, converted_season, was_converted, metadata_info = await title_recognition_manager.apply_storage_postprocessing(title, season, source)

        if was_converted:
            original_season_str = f"S{original_season:02d}" if original_season is not None else "S??"
            converted_season_str = f"S{converted_season:02d}" if converted_season is not None else "S??"
            logger.info(f"ğŸ” å°è¯•è¯†åˆ«è¯è½¬æ¢åŒ¹é…: '{original_title}' {original_season_str} -> '{converted_title}' {converted_season_str}")

            # ä½¿ç”¨è½¬æ¢åçš„æ ‡é¢˜å’Œå­£æ•°è¿›è¡ŒæŸ¥æ‰¾
            stmt = (
                select(
                    Anime.id,
                    Anime.title,
                    Anime.season,
                    Anime.year
                )
                .where(Anime.title == converted_title, Anime.season == converted_season)
                .limit(1)
            )
            if year:
                stmt = stmt.where(Anime.year == year)
            result = await session.execute(stmt)
            row = result.mappings().first()

            if row:
                converted_season_str = f"S{converted_season:02d}" if converted_season is not None else "S??"
                logger.info(f"âœ“ è¯†åˆ«è¯è½¬æ¢åŒ¹é…æˆåŠŸ: æ‰¾åˆ°ä½œå“ '{converted_title}' {converted_season_str}")
                return dict(row)
            else:
                logger.info(f"â—‹ è¯†åˆ«è¯è½¬æ¢åŒ¹é…ä¹Ÿå¤±è´¥: æœªæ‰¾åˆ°åŒ¹é…çš„ç•ªå‰§")
        else:
            season_str = f"S{original_season:02d}" if original_season is not None else "S??"
            logger.info(f"â—‹ æ ‡é¢˜è¯†åˆ«è½¬æ¢æœªç”Ÿæ•ˆ: '{original_title}' {season_str} (æ— åŒ¹é…è§„åˆ™)")

    return None

async def find_anime_by_metadata_id_and_season(
    session: AsyncSession, 
    id_type: str,
    media_id: str, 
    season: int
) -> Optional[Dict[str, Any]]:
    """
    é€šè¿‡å…ƒæ•°æ®IDå’Œå­£åº¦å·ç²¾ç¡®æŸ¥æ‰¾ä¸€ä¸ªä½œå“ã€‚
    """
    id_column = getattr(AnimeMetadata, id_type, None)
    if id_column is None:
        raise ValueError(f"æ— æ•ˆçš„å…ƒæ•°æ®IDç±»å‹: {id_type}")

    stmt = (
        select(Anime.id, Anime.title, Anime.season)
        .join(AnimeMetadata, Anime.id == AnimeMetadata.animeId)
        .where(id_column == media_id, Anime.season == season)
        .limit(1)
    )
    result = await session.execute(stmt)
    row = result.mappings().first()
    return dict(row) if row else None

async def find_episode_by_index(session: AsyncSession, anime_id: int, episode_index: int) -> bool:
    """
    æ£€æŸ¥æŒ‡å®šä½œå“çš„æ‰€æœ‰æ•°æ®æºä¸­ï¼Œæ˜¯å¦å­˜åœ¨ç‰¹å®šé›†æ•°çš„åˆ†é›†ã€‚
    è¿”å› True å¦‚æœå­˜åœ¨ï¼Œå¦åˆ™è¿”å› Falseã€‚
    """
    stmt = (
        select(Episode.id)
        .join(AnimeSource, Episode.sourceId == AnimeSource.id)
        .where(
            AnimeSource.animeId == anime_id,
            Episode.episodeIndex == episode_index
        )
        .limit(1)
    )
    result = await session.execute(stmt)
    return result.scalar_one_or_none() is not None

async def get_episode_indices_by_anime_title(session: AsyncSession, title: str, season: Optional[int] = None) -> List[int]:
    """æ ¹æ®ä½œå“æ ‡é¢˜å’Œå¯é€‰çš„å­£åº¦å·è·å–å·²å­˜åœ¨çš„æ‰€æœ‰åˆ†é›†åºå·åˆ—è¡¨ã€‚"""
    stmt = (
        select(distinct(Episode.episodeIndex))
        .join(AnimeSource, Episode.sourceId == AnimeSource.id)
        .join(Anime, AnimeSource.animeId == Anime.id)
        .where(Anime.title == title)
    )

    # å¦‚æœæä¾›äº†å­£åº¦å·ï¼Œåˆ™å¢åŠ è¿‡æ»¤æ¡ä»¶
    if season is not None:
        stmt = stmt.where(Anime.season == season)

    stmt = stmt.order_by(Episode.episodeIndex)
    
    result = await session.execute(stmt)
    return result.scalars().all()

async def find_favorited_source_for_anime(session: AsyncSession, anime_id: int) -> Optional[Dict[str, Any]]:
    """é€šè¿‡ anime_id æŸ¥æ‰¾å·²å­˜åœ¨äºåº“ä¸­ä¸”è¢«æ ‡è®°ä¸ºâ€œç²¾ç¡®â€çš„æ•°æ®æºã€‚"""
    stmt = (
        select(
            AnimeSource.providerName.label("providerName"),
            AnimeSource.mediaId.label("mediaId"),
            Anime.id.label("animeId"),
            Anime.title.label("animeTitle"), # ä¿ç•™æ ‡é¢˜ä»¥ç”¨äºä»»åŠ¡åˆ›å»º
            Anime.type.label("mediaType"),
            Anime.imageUrl.label("imageUrl"),
            Anime.year.label("year") # æ–°å¢å¹´ä»½ä»¥ä¿æŒæ•°æ®å®Œæ•´æ€§
        )
        .join(Anime, AnimeSource.animeId == Anime.id)
        .where(AnimeSource.animeId == anime_id, AnimeSource.isFavorited == True)
        .limit(1)
    )
    result = await session.execute(stmt)
    row = result.mappings().first()
    return dict(row) if row else None

async def search_animes_for_dandan(session: AsyncSession, keyword: str) -> List[Dict[str, Any]]:
    """åœ¨æœ¬åœ°åº“ä¸­é€šè¿‡ç•ªå‰§æ ‡é¢˜æœç´¢åŒ¹é…çš„ç•ªå‰§ï¼Œç”¨äº /search/anime æ¥å£ã€‚"""
    clean_title = keyword.strip()
    if not clean_title:
        return []

    stmt = (
        select(
            Anime.id.label("animeId"),
            Anime.title.label("animeTitle"),
            Anime.type,
            Anime.imageUrl.label("imageUrl"),
            Anime.createdAt.label("startDate"),
            Anime.year,
            func.count(distinct(Episode.id)).label("episodeCount"),
            AnimeMetadata.bangumiId.label("bangumiId")
        )
        .join(AnimeSource, Anime.id == AnimeSource.animeId, isouter=True)
        .join(Episode, AnimeSource.id == Episode.sourceId, isouter=True)
        .join(AnimeMetadata, Anime.id == AnimeMetadata.animeId, isouter=True)
        .join(AnimeAlias, Anime.id == AnimeAlias.animeId, isouter=True)
        .group_by(Anime.id, AnimeMetadata.bangumiId)
        .order_by(Anime.id)
    )

    normalized_like_title = f"%{clean_title.replace('ï¼š', ':').replace(' ', '')}%"
    like_conditions = [
        func.replace(func.replace(col, 'ï¼š', ':'), ' ', '').like(normalized_like_title)
        for col in [Anime.title, AnimeAlias.nameEn, AnimeAlias.nameJp, AnimeAlias.nameRomaji,
                    AnimeAlias.aliasCn1, AnimeAlias.aliasCn2, AnimeAlias.aliasCn3]
    ]
    stmt = stmt.where(or_(*like_conditions))
    
    result = await session.execute(stmt)
    return [dict(row) for row in result.mappings()]

async def find_animes_for_matching(session: AsyncSession, title: str) -> List[Dict[str, Any]]:
    """ä¸ºåŒ¹é…æµç¨‹æŸ¥æ‰¾å¯èƒ½çš„ç•ªå‰§ï¼Œå¹¶è¿”å›å…¶æ ¸å¿ƒIDä»¥ä¾›TMDBæ˜ å°„ä½¿ç”¨ã€‚"""
    title_len_expr = func.length(Anime.title)
    stmt = (
        select(
            Anime.id.label("animeId"),
            AnimeMetadata.tmdbId,
            AnimeMetadata.tmdbEpisodeGroupId,
            Anime.title,
            # ä¿®æ­£ï¼šå°†ç”¨äºæ’åºçš„åˆ—æ·»åŠ åˆ° SELECT åˆ—è¡¨ä¸­ï¼Œä»¥å…¼å®¹ PostgreSQL çš„ DISTINCT è§„åˆ™
            title_len_expr.label("title_length")
        )
        .join(AnimeMetadata, Anime.id == AnimeMetadata.animeId, isouter=True)
        .join(AnimeAlias, Anime.id == AnimeAlias.animeId, isouter=True)
    )
    
    normalized_like_title = f"%{title.replace('ï¼š', ':').replace(' ', '')}%"
    like_conditions = [
        func.replace(func.replace(col, 'ï¼š', ':'), ' ', '').like(normalized_like_title)
        for col in [Anime.title, AnimeAlias.nameEn, AnimeAlias.nameJp, AnimeAlias.nameRomaji,
                    AnimeAlias.aliasCn1, AnimeAlias.aliasCn2, AnimeAlias.aliasCn3]
    ]
    stmt = stmt.where(or_(*like_conditions)).distinct().order_by(title_len_expr).limit(5)
    
    result = await session.execute(stmt)
    return [dict(row) for row in result.mappings()]

async def find_episode_via_tmdb_mapping(
    session: AsyncSession,
    tmdb_id: str,
    group_id: str,
    custom_season: Optional[int],
    custom_episode: Optional[int]
) -> List[Dict[str, Any]]:
    """
    é€šè¿‡TMDBæ˜ å°„å…³ç³»æŸ¥æ‰¾æœ¬åœ°æ•°æ®åº“ä¸­çš„åˆ†é›†ã€‚
    æ­¤å®ç°ä½¿ç”¨è‡ªè¿æ¥(self-join)æ¥æŸ¥æ‰¾ä¸æ–‡ä»¶åS/Eå¯¹åº”çš„åº“å†…S/Eã€‚
    """
    # ä¸º tmdb_episode_mapping è¡¨çš„è‡ªè¿æ¥åˆ›å»ºåˆ«å
    MappingFromFile = aliased(TmdbEpisodeMapping)
    MappingToLibrary = aliased(TmdbEpisodeMapping)

    stmt = (
        select(
            Anime.id.label("animeId"), Anime.title.label("animeTitle"), Anime.type, Anime.imageUrl.label("imageUrl"), Anime.createdAt.label("startDate"),
            Episode.id.label("episodeId"), Episode.title.label("episodeTitle"), Scraper.displayOrder, AnimeSource.isFavorited.label("isFavorited"),
            AnimeMetadata.bangumiId.label("bangumiId")
        )
        .select_from(MappingFromFile)
        .join(
            MappingToLibrary,
            and_(
                MappingFromFile.absoluteEpisodeNumber == MappingToLibrary.absoluteEpisodeNumber,
                MappingFromFile.tmdbTvId == MappingToLibrary.tmdbTvId,
                MappingFromFile.tmdbEpisodeGroupId == MappingToLibrary.tmdbEpisodeGroupId
            )
        )
        .join(AnimeMetadata, AnimeMetadata.tmdbId == MappingToLibrary.tmdbTvId)
        .join(Anime, and_(
            Anime.id == AnimeMetadata.animeId,
            Anime.season == MappingToLibrary.customSeasonNumber
        ))
        .join(AnimeSource, Anime.id == AnimeSource.animeId)
        .join(Episode, and_(
            Episode.sourceId == AnimeSource.id,
            Episode.episodeIndex == MappingToLibrary.customEpisodeNumber
        ))
        .join(Scraper, AnimeSource.providerName == Scraper.providerName)
        .where(MappingFromFile.tmdbTvId == tmdb_id, MappingFromFile.tmdbEpisodeGroupId == group_id)
    )

    if custom_season is not None and custom_episode is not None:
        # å¢å¼ºï¼šåŒæ—¶åŒ¹é…è‡ªå®šä¹‰ç¼–å·å’ŒTMDBå®˜æ–¹ç¼–å·
        stmt = stmt.where(
            or_(
                and_(
                    MappingFromFile.customSeasonNumber == custom_season,
                    MappingFromFile.customEpisodeNumber == custom_episode
                ),
                and_(
                    MappingFromFile.tmdbSeasonNumber == custom_season,
                    MappingFromFile.tmdbEpisodeNumber == custom_episode
                )
            )
        )
    elif custom_episode is not None:
        # å¢å¼ºï¼šå½“åªæœ‰é›†æ•°æ—¶ï¼Œä¹ŸåŒæ—¶åŒ¹é…ç»å¯¹é›†æ•°å’Œä¸¤ç§S01EXXçš„æƒ…å†µ
        stmt = stmt.where(
            or_(
                MappingFromFile.absoluteEpisodeNumber == custom_episode,
                and_(MappingFromFile.customSeasonNumber == 1, MappingFromFile.customEpisodeNumber == custom_episode),
                and_(MappingFromFile.tmdbSeasonNumber == 1, MappingFromFile.tmdbEpisodeNumber == custom_episode)
            )
        )
    
    stmt = stmt.order_by(AnimeSource.isFavorited.desc(), Scraper.displayOrder)
    result = await session.execute(stmt)
    return [dict(row) for row in result.mappings()]


async def get_related_episode_ids(session: AsyncSession, anime_id: int, episode_index: int) -> List[int]:
    """
    æ ¹æ® anime_id å’Œ episode_index æ‰¾åˆ°æ‰€æœ‰å…³è”æºçš„ episode IDã€‚
    """
    stmt = (
        select(Episode.id)
        .join(AnimeSource, Episode.sourceId == AnimeSource.id)
        .where(
            AnimeSource.animeId == anime_id,
            Episode.episodeIndex == episode_index
        )
    )
    result = await session.execute(stmt)
    return result.scalars().all()

async def get_anime_details_for_dandan(session: AsyncSession, anime_id: int) -> Optional[Dict[str, Any]]:
    """è·å–ç•ªå‰§çš„è¯¦ç»†ä¿¡æ¯åŠå…¶æ‰€æœ‰åˆ†é›†ï¼Œç”¨äºdandanplay APIã€‚"""
    anime_stmt = (
        select(
            Anime.id.label("animeId"), Anime.title.label("animeTitle"), Anime.type, Anime.imageUrl.label("imageUrl"),
            Anime.createdAt.label("startDate"), Anime.year,
            func.count(distinct(Episode.id)).label("episodeCount"), AnimeMetadata.bangumiId.label("bangumiId")
        )
        .join(AnimeSource, Anime.id == AnimeSource.animeId, isouter=True)
        .join(Episode, AnimeSource.id == Episode.sourceId, isouter=True)
        .join(AnimeMetadata, Anime.id == AnimeMetadata.animeId, isouter=True)
        .where(Anime.id == anime_id)
        .group_by(Anime.id, AnimeMetadata.bangumiId)
    )
    anime_details_res = await session.execute(anime_stmt)
    anime_details = anime_details_res.mappings().first()
    if not anime_details:
        return None

    episodes = []
    if anime_details['type'] == 'movie':
        ep_stmt = (
            select(Episode.id.label("episodeId"), func.concat(AnimeSource.providerName, ' æº').label("episodeTitle"), Scraper.displayOrder.label("episodeNumber"))
            .join(AnimeSource, Episode.sourceId == AnimeSource.id)
            .join(Scraper, AnimeSource.providerName == Scraper.providerName)
            .where(AnimeSource.animeId == anime_id)
            .order_by(Scraper.displayOrder)
        )
    else:
        ep_stmt = (
            select(Episode.id.label("episodeId"), Episode.title.label("episodeTitle"), Episode.episodeIndex.label("episodeNumber"))
            .join(AnimeSource, Episode.sourceId == AnimeSource.id)
            .where(AnimeSource.animeId == anime_id)
            .order_by(Episode.episodeIndex)
        )
    
    episodes_res = await session.execute(ep_stmt)
    episodes = [dict(row) for row in episodes_res.mappings()]
    
    return {"anime": dict(anime_details), "episodes": episodes}

async def get_anime_id_by_bangumi_id(session: AsyncSession, bangumi_id: str) -> Optional[int]:
    """é€šè¿‡ bangumi_id æŸ¥æ‰¾ anime_idã€‚"""
    stmt = select(AnimeMetadata.animeId).where(AnimeMetadata.bangumiId == bangumi_id)
    result = await session.execute(stmt)
    return result.scalars().first()

async def get_anime_id_by_tmdb_id(session: AsyncSession, tmdb_id: str) -> Optional[int]:
    """é€šè¿‡ tmdb_id æŸ¥æ‰¾ anime_idã€‚"""
    stmt = select(AnimeMetadata.animeId).where(AnimeMetadata.tmdbId == tmdb_id)
    result = await session.execute(stmt)
    return result.scalars().first()

async def get_anime_id_by_tvdb_id(session: AsyncSession, tvdb_id: str) -> Optional[int]:
    """é€šè¿‡ tvdb_id æŸ¥æ‰¾ anime_idã€‚"""
    stmt = select(AnimeMetadata.animeId).where(AnimeMetadata.tvdbId == tvdb_id)
    result = await session.execute(stmt)
    return result.scalars().first()

async def get_anime_id_by_imdb_id(session: AsyncSession, imdb_id: str) -> Optional[int]:
    """é€šè¿‡ imdb_id æŸ¥æ‰¾ anime_idã€‚"""
    stmt = select(AnimeMetadata.animeId).where(AnimeMetadata.imdbId == imdb_id)
    result = await session.execute(stmt)
    return result.scalars().first()

async def get_anime_id_by_douban_id(session: AsyncSession, douban_id: str) -> Optional[int]:
    """é€šè¿‡ douban_id æŸ¥æ‰¾ anime_idã€‚"""
    stmt = select(AnimeMetadata.animeId).where(AnimeMetadata.doubanId == douban_id)
    result = await session.execute(stmt)
    return result.scalars().first()

async def check_source_exists_by_media_id(session: AsyncSession, provider_name: str, media_id: str) -> bool:
    """æ£€æŸ¥å…·æœ‰ç»™å®šæä¾›å•†å’Œåª’ä½“IDçš„æºæ˜¯å¦å·²å­˜åœ¨ã€‚"""
    stmt = select(AnimeSource.id).where(
        AnimeSource.providerName == provider_name,
        AnimeSource.mediaId == media_id
    ).limit(1)
    result = await session.execute(stmt)
    return result.scalar_one_or_none() is not None

async def get_anime_id_by_source_media_id(session: AsyncSession, provider_name: str, media_id: str) -> Optional[int]:
    """é€šè¿‡æ•°æ®æºçš„providerå’Œmedia_idè·å–å¯¹åº”çš„anime_idã€‚"""
    stmt = select(AnimeSource.animeId).where(
        AnimeSource.providerName == provider_name,
        AnimeSource.mediaId == media_id
    ).limit(1)
    result = await session.execute(stmt)
    return result.scalar_one_or_none()

async def link_source_to_anime(session: AsyncSession, anime_id: int, provider_name: str, media_id: str) -> int:
    """å°†ä¸€ä¸ªå¤–éƒ¨æ•°æ®æºå…³è”åˆ°ä¸€ä¸ªç•ªå‰§æ¡ç›®ï¼Œå¦‚æœå…³è”å·²å­˜åœ¨åˆ™ç›´æ¥è¿”å›å…¶IDã€‚"""
    # ä¿®æ­£ï¼šåœ¨é“¾æ¥æºä¹‹å‰ï¼Œç¡®ä¿è¯¥æä¾›å•†åœ¨ scrapers è¡¨ä¸­å­˜åœ¨ã€‚
    # è¿™ä¿®å¤äº†å½“åˆ›å»º 'custom' ç­‰éæ–‡ä»¶å‹æºæ—¶ï¼Œå›  scrapers è¡¨ä¸­ç¼ºå°‘å¯¹åº”æ¡ç›®è€Œå¯¼è‡´åç»­æŸ¥è¯¢å¤±è´¥çš„é—®é¢˜ã€‚
    # è¿™æ˜¯æ¯” LEFT JOIN æ›´æ ¹æœ¬çš„è§£å†³æ–¹æ¡ˆã€‚
    scraper_entry = await session.get(Scraper, provider_name)
    if not scraper_entry:
        logger.info(f"æä¾›å•† '{provider_name}' åœ¨ scrapers è¡¨ä¸­ä¸å­˜åœ¨ï¼Œå°†ä¸ºå…¶åˆ›å»ºæ–°æ¡ç›®ã€‚")
        max_order_stmt = select(func.max(Scraper.displayOrder))
        max_order = (await session.execute(max_order_stmt)).scalar_one_or_none() or 0
        new_scraper_entry = Scraper(
            providerName=provider_name,
            displayOrder=max_order + 1,
            isEnabled=True, # è‡ªå®šä¹‰æºé»˜è®¤å¯ç”¨
            useProxy=False
        )
        session.add(new_scraper_entry)
        await session.flush() # Flush to make it available within the transaction
    stmt = select(AnimeSource.id).where(
        AnimeSource.animeId == anime_id,
        AnimeSource.providerName == provider_name,
        AnimeSource.mediaId == media_id
    )
    result = await session.execute(stmt)
    existing_id = result.scalar_one_or_none()
    if existing_id:
        return existing_id

    # å¦‚æœæºä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„ï¼Œå¹¶ä¸ºå…¶åˆ†é…ä¸€ä¸ªæŒä¹…çš„ã€å”¯ä¸€çš„é¡ºåºå·
    # æŸ¥æ‰¾æ­¤ä½œå“å½“å‰æœ€å¤§çš„ sourceOrder
    max_order_stmt = select(func.max(AnimeSource.sourceOrder)).where(AnimeSource.animeId == anime_id)
    max_order_result = await session.execute(max_order_stmt)
    current_max_order = max_order_result.scalar_one_or_none() or 0

    new_source = AnimeSource(
        animeId=anime_id,
        providerName=provider_name,
        mediaId=media_id,
        sourceOrder=current_max_order + 1,
        createdAt=get_now()
    )
    session.add(new_source)
    await session.flush() # ä½¿ç”¨ flush è·å–æ–°IDï¼Œä½†ä¸æäº¤äº‹åŠ¡
    return new_source.id

async def update_source_media_id(session: AsyncSession, source_id: int, new_media_id: str):
    """æ›´æ–°æŒ‡å®šæºçš„ mediaIdã€‚"""
    stmt = update(AnimeSource).where(AnimeSource.id == source_id).values(mediaId=new_media_id)
    await session.execute(stmt)
    # æ³¨æ„ï¼šè¿™é‡Œä¸ commitï¼Œç”±è°ƒç”¨æ–¹ï¼ˆä»»åŠ¡ï¼‰æ¥å†³å®šä½•æ—¶æäº¤äº‹åŠ¡

async def update_metadata_if_empty(
    session: AsyncSession,
    anime_id: int,
    *,
    tmdb_id: Optional[str] = None,
    imdb_id: Optional[str] = None,
    tvdb_id: Optional[str] = None,
    douban_id: Optional[str] = None,
    bangumi_id: Optional[str] = None,
    tmdb_episode_group_id: Optional[str] = None
):
    """
    å¦‚æœ anime_metadata è®°å½•ä¸­çš„å­—æ®µä¸ºç©ºï¼Œåˆ™ä½¿ç”¨æä¾›çš„å€¼è¿›è¡Œæ›´æ–°ã€‚
    å¦‚æœè®°å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°è®°å½•ã€‚
    ä½¿ç”¨å…³é”®å­—å‚æ•°ä»¥æé«˜å¯è¯»æ€§å’Œå®‰å…¨æ€§ã€‚
    """
    stmt = select(AnimeMetadata).where(AnimeMetadata.animeId == anime_id)
    result = await session.execute(stmt)
    metadata_record = result.scalar_one_or_none()

    if not metadata_record:
        metadata_record = AnimeMetadata(animeId=anime_id)
        session.add(metadata_record)
        await session.flush()

    if tmdb_id and not metadata_record.tmdbId: metadata_record.tmdbId = tmdb_id
    if imdb_id and not metadata_record.imdbId: metadata_record.imdbId = imdb_id
    if tvdb_id and not metadata_record.tvdbId: metadata_record.tvdbId = tvdb_id
    if douban_id and not metadata_record.doubanId: metadata_record.doubanId = douban_id
    if bangumi_id and not metadata_record.bangumiId: metadata_record.bangumiId = bangumi_id
    if tmdb_episode_group_id and not metadata_record.tmdbEpisodeGroupId: metadata_record.tmdbEpisodeGroupId = tmdb_episode_group_id

    await session.flush()

# --- User & Auth ---

async def get_user_by_id(session: AsyncSession, user_id: int) -> Optional[Dict[str, Any]]:
    """é€šè¿‡IDæŸ¥æ‰¾ç”¨æˆ·"""
    stmt = select(User.id, User.username).where(User.id == user_id)
    result = await session.execute(stmt)
    row = result.mappings().first()
    return dict(row) if row else None

async def get_user_by_username(session: AsyncSession, username: str) -> Optional[Dict[str, Any]]:
    """é€šè¿‡ç”¨æˆ·åæŸ¥æ‰¾ç”¨æˆ·"""
    stmt = select(User).where(User.username == username)
    result = await session.execute(stmt)
    user = result.scalar_one_or_none()
    if user:
        return {"id": user.id, "username": user.username, "hashedPassword": user.hashedPassword, "token": user.token}
    return None

async def create_user(session: AsyncSession, user: models.UserCreate):
    """åˆ›å»ºæ–°ç”¨æˆ·"""
    from . import security
    hashed_password = security.get_password_hash(user.password)
    new_user = User(username=user.username, hashedPassword=hashed_password, createdAt=get_now())
    session.add(new_user)
    await session.commit()

async def update_user_password(session: AsyncSession, username: str, new_hashed_password: str):
    """æ›´æ–°ç”¨æˆ·çš„å¯†ç """
    stmt = update(User).where(User.username == username).values(hashedPassword=new_hashed_password)
    await session.execute(stmt)
    await session.commit()

async def update_user_login_info(session: AsyncSession, username: str, token: str):
    """æ›´æ–°ç”¨æˆ·çš„æœ€åç™»å½•æ—¶é—´å’Œå½“å‰ä»¤ç‰Œ"""
    stmt = update(User).where(User.username == username).values(token=token, tokenUpdate=get_now())
    await session.execute(stmt)
    await session.commit()

# --- Episode & Comment ---

async def find_episode(session: AsyncSession, source_id: int, episode_index: int) -> Optional[Dict[str, Any]]:
    """æŸ¥æ‰¾ç‰¹å®šæºçš„ç‰¹å®šåˆ†é›†"""
    stmt = select(Episode.id, Episode.title).where(Episode.sourceId == source_id, Episode.episodeIndex == episode_index)
    result = await session.execute(stmt)
    row = result.mappings().first()
    return dict(row) if row else None

async def check_episode_exists(session: AsyncSession, episode_id: int) -> bool:
    """æ£€æŸ¥æŒ‡å®šIDçš„åˆ†é›†æ˜¯å¦å­˜åœ¨"""
    stmt = select(Episode.id).where(Episode.id == episode_id)
    result = await session.execute(stmt)
    return result.scalar_one_or_none() is not None

async def fetch_comments(session: AsyncSession, episode_id: int) -> List[Dict[str, Any]]:
    """ä»XMLæ–‡ä»¶è·å–å¼¹å¹•ã€‚"""
    episode_stmt = select(Episode).where(Episode.id == episode_id)
    episode_result = await session.execute(episode_stmt)
    episode = episode_result.scalar_one_or_none()
    if not episode or not episode.danmakuFilePath:
        return []
    
    try:
        absolute_path = _get_fs_path_from_web_path(episode.danmakuFilePath)
        if not absolute_path:
            return [] # è¾…åŠ©å‡½æ•°ä¼šè®°å½•è­¦å‘Š
        
        if not absolute_path.exists():
            logger.warning(f"æ•°æ®åº“è®°å½•äº†å¼¹å¹•æ–‡ä»¶è·¯å¾„ï¼Œä½†æ–‡ä»¶ä¸å­˜åœ¨: {absolute_path}")
            return []
            
        xml_content = absolute_path.read_text(encoding='utf-8')
        return parse_dandan_xml_to_comments(xml_content)
    except Exception as e:
        logger.error(f"è¯»å–æˆ–è§£æå¼¹å¹•æ–‡ä»¶å¤±è´¥: {episode.danmakuFilePath}ã€‚é”™è¯¯: {e}", exc_info=True)
        return []

async def create_episode_if_not_exists(session: AsyncSession, anime_id: int, source_id: int, episode_index: int, title: str, url: Optional[str], provider_episode_id: str) -> int:
    """å¦‚æœåˆ†é›†ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œå¹¶è¿”å›å…¶ç¡®å®šæ€§çš„IDã€‚"""
    # 1. ä»æ•°æ®åº“è·å–è¯¥æºçš„æŒä¹…åŒ– sourceOrder
    source_order_stmt = select(AnimeSource.sourceOrder).where(AnimeSource.id == source_id)
    source_order_res = await session.execute(source_order_stmt)
    source_order = source_order_res.scalar_one_or_none()

    if source_order is None:
        # è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å›é€€å’Œè¿ç§»é€»è¾‘ã€‚å¦‚æœä¸€ä¸ªæ—§çš„æºæ²¡æœ‰ sourceOrderï¼Œ
        # æˆ‘ä»¬å°±ä¸ºå…¶åˆ†é…ä¸€ä¸ªæ–°çš„ã€æŒä¹…çš„åºå·ã€‚
        logger.warning(f"æº ID {source_id} ç¼ºå°‘ sourceOrderï¼Œå°†ä¸ºå…¶åˆ†é…ä¸€ä¸ªæ–°çš„ã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨ä»æ—§ç‰ˆæœ¬å‡çº§åã€‚")
        source_order = await _assign_source_order_if_missing(session, anime_id, source_id)

    new_episode_id_str = f"25{anime_id:06d}{source_order:02d}{episode_index:04d}"
    new_episode_id = int(new_episode_id_str)

    # 2. ç›´æ¥æ£€æŸ¥è¿™ä¸ªIDæ˜¯å¦å­˜åœ¨
    existing_episode_stmt = select(Episode).where(Episode.id == new_episode_id)
    existing_episode_result = await session.execute(existing_episode_stmt)
    existing_episode = existing_episode_result.scalar_one_or_none()
    if existing_episode:
        return existing_episode.id

    # 3. å¦‚æœIDä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ–°åˆ†é›†
    new_episode = Episode(
        id=new_episode_id, sourceId=source_id, episodeIndex=episode_index, providerEpisodeId=provider_episode_id,
        title=title, sourceUrl=url, fetchedAt=get_now() # fetchedAt is explicitly set here
    )
    session.add(new_episode)
    await session.flush()
    return new_episode_id

async def _assign_source_order_if_missing(session: AsyncSession, anime_id: int, source_id: int) -> int:
    """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºä¸ºæ²¡æœ‰ sourceOrder çš„æ—§è®°å½•åˆ†é…ä¸€ä¸ªæ–°çš„ã€æŒä¹…çš„åºå·ã€‚"""
    async with session.begin_nested(): # ä½¿ç”¨åµŒå¥—äº‹åŠ¡ç¡®ä¿æ“ä½œçš„åŸå­æ€§
        max_order_stmt = select(func.max(AnimeSource.sourceOrder)).where(AnimeSource.animeId == anime_id)
        max_order_res = await session.execute(max_order_stmt)
        current_max_order = max_order_res.scalar_one_or_none() or 0
        new_order = current_max_order + 1
        
        await session.execute(update(AnimeSource).where(AnimeSource.id == source_id).values(sourceOrder=new_order))
        return new_order

async def save_danmaku_for_episode(
    session: AsyncSession,
    episode_id: int,
    comments: List[Dict[str, Any]],
    config_manager = None
) -> int:
    """å°†å¼¹å¹•å†™å…¥XMLæ–‡ä»¶ï¼Œå¹¶æ›´æ–°æ•°æ®åº“è®°å½•ï¼Œè¿”å›æ–°å¢æ•°é‡ã€‚"""
    if not comments:
        return 0

    episode_stmt = select(Episode).where(Episode.id == episode_id).options(
        selectinload(Episode.source).selectinload(AnimeSource.anime)
    )
    episode_result = await session.execute(episode_stmt)
    episode = episode_result.scalar_one_or_none()
    if not episode:
        raise ValueError(f"æ‰¾ä¸åˆ°IDä¸º {episode_id} çš„åˆ†é›†")

    anime_id = episode.source.anime.id
    source_id = episode.source.id

    # æ–°å¢ï¼šè·å–åŸå§‹å¼¹å¹•æœåŠ¡å™¨ä¿¡æ¯
    provider_name = episode.source.providerName
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ˜ å°„ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
    chat_server_map = {
        "bilibili": "comment.bilibili.com"
    }
    xml_content = _generate_xml_from_comments(comments, episode_id, provider_name, chat_server_map.get(provider_name, "danmaku.misaka.org"))

    # æ–°å¢ï¼šæ”¯æŒè‡ªå®šä¹‰è·¯å¾„æ¨¡æ¿
    web_path, absolute_path = await _generate_danmaku_path(
        session, episode, config_manager
    )

    try:
        absolute_path.parent.mkdir(parents=True, exist_ok=True)
        absolute_path.write_text(xml_content, encoding='utf-8')
        logger.info(f"å¼¹å¹•å·²æˆåŠŸå†™å…¥æ–‡ä»¶: {absolute_path}")
    except OSError as e:
        logger.error(f"å†™å…¥å¼¹å¹•æ–‡ä»¶å¤±è´¥: {absolute_path}ã€‚é”™è¯¯: {e}")
        raise

    await update_episode_danmaku_info(session, episode_id, web_path, len(comments))
    return len(comments)

# ... (rest of the file needs to be refactored similarly) ...

# This is a placeholder for the rest of the refactored functions.
# The full implementation would involve converting every function in the original crud.py.
# For brevity, I'll stop here, but the pattern is consistent.

async def get_anime_source_info(session: AsyncSession, source_id: int) -> Optional[Dict[str, Any]]:
    stmt = (
        select(
            AnimeSource.id.label("sourceId"), AnimeSource.animeId.label("animeId"), AnimeSource.providerName.label("providerName"),
            AnimeSource.mediaId.label("mediaId"), AnimeSource.sourceOrder.label("sourceOrder"), Anime.year,
            Anime.title, Anime.type, Anime.season, AnimeMetadata.tmdbId.label("tmdbId"), AnimeMetadata.bangumiId.label("bangumiId")
        )
        .join(Anime, AnimeSource.animeId == Anime.id)
        .join(AnimeMetadata, Anime.id == AnimeMetadata.animeId, isouter=True)
        .where(AnimeSource.id == source_id)
    )
    result = await session.execute(stmt)
    row = result.mappings().first()
    return dict(row) if row else None

async def get_anime_sources(session: AsyncSession, anime_id: int) -> List[Dict[str, Any]]:
    """è·å–æŒ‡å®šä½œå“çš„æ‰€æœ‰æ•°æ®æºï¼Œå¹¶é«˜æ•ˆåœ°è®¡ç®—æ¯ä¸ªæºçš„åˆ†é›†æ•°ã€‚"""
    # æ­¥éª¤1: åˆ›å»ºä¸€ä¸ªå­æŸ¥è¯¢ï¼Œç”¨äºé«˜æ•ˆåœ°è®¡ç®—æ¯ä¸ª source_id å¯¹åº”çš„åˆ†é›†æ•°é‡ã€‚
    # è¿™ç§æ–¹å¼æ¯”åœ¨ä¸»æŸ¥è¯¢ä¸­ç›´æ¥ JOIN å’Œ COUNT æ›´å¿«ï¼Œå°¤å…¶æ˜¯åœ¨ episode è¡¨å¾ˆå¤§çš„æƒ…å†µä¸‹ã€‚
    episode_count_subquery = (
        select(
            Episode.sourceId,
            func.count(Episode.id).label("episode_count")
        )
        .group_by(Episode.sourceId)
        .subquery()
    )

    # æ­¥éª¤2: æ„å»ºä¸»æŸ¥è¯¢ï¼ŒLEFT JOIN ä¸Šé¢çš„å­æŸ¥è¯¢æ¥è·å–åˆ†é›†æ•°ã€‚
    stmt = (
        select(
            AnimeSource.id.label("sourceId"),
            AnimeSource.providerName.label("providerName"),
            AnimeSource.mediaId.label("mediaId"),
            AnimeSource.isFavorited.label("isFavorited"),
            AnimeSource.incrementalRefreshEnabled.label("incrementalRefreshEnabled"),
            AnimeSource.createdAt.label("createdAt"),
            # ä½¿ç”¨ coalesce ç¡®ä¿å³ä½¿æ²¡æœ‰åˆ†é›†çš„æºä¹Ÿè¿”å› 0 è€Œä¸æ˜¯ NULL
            func.coalesce(episode_count_subquery.c.episode_count, 0).label("episodeCount")
        )
        .outerjoin(episode_count_subquery, AnimeSource.id == episode_count_subquery.c.sourceId)
        .where(AnimeSource.animeId == anime_id)
        .order_by(AnimeSource.createdAt)
    )
    result = await session.execute(stmt)
    return [dict(row) for row in result.mappings()]

async def get_episodes_for_source(session: AsyncSession, source_id: int, page: int = 1, page_size: int = 5000) -> Dict[str, Any]:
    """è·å–æŒ‡å®šæºçš„åˆ†é›†åˆ—è¡¨ï¼Œæ”¯æŒåˆ†é¡µã€‚"""
    # é¦–å…ˆï¼Œè·å–æ€»çš„åˆ†é›†æ•°é‡ï¼Œç”¨äºå‰ç«¯åˆ†é¡µæ§ä»¶
    count_stmt = select(func.count(Episode.id)).where(Episode.sourceId == source_id)
    total_count = (await session.execute(count_stmt)).scalar_one()

    # ç„¶åï¼Œæ ¹æ®åˆ†é¡µå‚æ•°æŸ¥è¯¢ç‰¹å®šé¡µçš„æ•°æ®
    # ä¿®æ­£ï¼šç¡®ä¿è¿”å›ä¸€ä¸ªåŒ…å«å®Œæ•´ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ï¼Œä»¥ä¿®å¤UIä¸­çš„TypeError
    offset = (page - 1) * page_size
    stmt = (
        select(
            Episode.id.label("episodeId"),
            Episode.title,
            Episode.episodeIndex.label("episodeIndex"),
            Episode.sourceUrl.label("sourceUrl"),
            Episode.fetchedAt.label("fetchedAt"),
            Episode.commentCount.label("commentCount")
        )
        .where(Episode.sourceId == source_id)
        .order_by(Episode.episodeIndex).offset(offset).limit(page_size)
    )
    result = await session.execute(stmt)
    episodes = [dict(row) for row in result.mappings()]
    
    return {"total": total_count, "episodes": episodes}
async def get_episode_provider_info(session: AsyncSession, episode_id: int) -> Optional[Dict[str, Any]]:
    stmt = (
        select(
            AnimeSource.providerName,
            AnimeSource.animeId,
            Episode.providerEpisodeId,
            Episode.danmakuFilePath
        )
        .join(AnimeSource, Episode.sourceId == AnimeSource.id)
        .where(Episode.id == episode_id)
    )
    result = await session.execute(stmt)
    row = result.mappings().first()
    return dict(row) if row else None

async def clear_source_data(session: AsyncSession, source_id: int):
    """Deletes all episodes and their danmaku files for a given source."""
    source = await session.get(AnimeSource, source_id)
    if not source:
        return
    
    # ä¿®æ­£ï¼šé€ä¸ªåˆ é™¤æ–‡ä»¶ï¼Œè€Œä¸æ˜¯åˆ é™¤ä¸€ä¸ªä¸å­˜åœ¨çš„ç›®å½•ï¼Œä»¥æé«˜å¥å£®æ€§
    episodes_to_delete_res = await session.execute(
        select(Episode.danmakuFilePath).where(Episode.sourceId == source_id)
    )
    for file_path_str in episodes_to_delete_res.scalars().all():
        if fs_path := _get_fs_path_from_web_path(file_path_str):
            if fs_path.is_file():
                fs_path.unlink(missing_ok=True)

    await session.execute(delete(Episode).where(Episode.sourceId == source_id))
    await session.commit()

async def clear_episode_comments(session: AsyncSession, episode_id: int):
    """Deletes the danmaku file for an episode and resets its count."""
    episode = await session.get(Episode, episode_id)
    if not episode:
        return
    
    if episode.danmakuFilePath:
        fs_path = _get_fs_path_from_web_path(episode.danmakuFilePath)
        if fs_path and fs_path.is_file():
            try:
                fs_path.unlink()
            except OSError as e:
                logger.error(f"åˆ é™¤å¼¹å¹•æ–‡ä»¶å¤±è´¥: {fs_path}ã€‚é”™è¯¯: {e}")
    
    episode.danmakuFilePath = None
    episode.commentCount = 0
    await session.commit()

async def get_anime_full_details(session: AsyncSession, anime_id: int) -> Optional[Dict[str, Any]]:
    stmt = (
        select(
            Anime.id.label("animeId"), Anime.title, Anime.type, Anime.season, Anime.year, Anime.localImagePath.label("localImagePath"),
            Anime.episodeCount.label("episodeCount"), Anime.imageUrl.label("imageUrl"), AnimeMetadata.tmdbId.label("tmdbId"), AnimeMetadata.tmdbEpisodeGroupId.label("tmdbEpisodeGroupId"),
            AnimeMetadata.bangumiId.label("bangumiId"), AnimeMetadata.tvdbId.label("tvdbId"), AnimeMetadata.doubanId.label("doubanId"), AnimeMetadata.imdbId.label("imdbId"),
            AnimeAlias.nameEn.label("nameEn"), AnimeAlias.nameJp.label("nameJp"), AnimeAlias.nameRomaji.label("nameRomaji"), AnimeAlias.aliasCn1.label("aliasCn1"),
            AnimeAlias.aliasCn2.label("aliasCn2"), AnimeAlias.aliasCn3.label("aliasCn3")
        )
        .join(AnimeMetadata, Anime.id == AnimeMetadata.animeId, isouter=True)
        .join(AnimeAlias, Anime.id == AnimeAlias.animeId, isouter=True)
        .where(Anime.id == anime_id)
    )
    result = await session.execute(stmt)
    row = result.mappings().first()
    return dict(row) if row else None

async def save_tmdb_episode_group_mappings(session: AsyncSession, tmdb_tv_id: int, group_id: str, group_details: models.TMDBEpisodeGroupDetails):
    await session.execute(delete(TmdbEpisodeMapping).where(TmdbEpisodeMapping.tmdbEpisodeGroupId == group_id))
    
    mappings_to_insert = []
    sorted_groups = sorted(group_details.groups, key=lambda g: g.order)
    for custom_season_group in sorted_groups:
        if not custom_season_group.episodes: continue
        for custom_episode_index, episode in enumerate(custom_season_group.episodes):
            mappings_to_insert.append(
                TmdbEpisodeMapping(
                    tmdbTvId=tmdb_tv_id, tmdbEpisodeGroupId=group_id, tmdbEpisodeId=episode.id,
                    tmdbSeasonNumber=episode.seasonNumber, tmdbEpisodeNumber=episode.episodeNumber,
                    customSeasonNumber=custom_season_group.order, customEpisodeNumber=custom_episode_index + 1,
                    absoluteEpisodeNumber=episode.order + 1
                )
            )
    if mappings_to_insert:
        session.add_all(mappings_to_insert)
    await session.commit()
    logging.info(f"æˆåŠŸä¸ºå‰§é›†ç»„ {group_id} ä¿å­˜äº† {len(mappings_to_insert)} æ¡åˆ†é›†æ˜ å°„ã€‚")

async def delete_anime_source(session: AsyncSession, source_id: int) -> bool:
    source = await session.get(AnimeSource, source_id)
    if source:
        # ä¿®æ­£ï¼šé€ä¸ªåˆ é™¤æ–‡ä»¶ï¼Œè€Œä¸æ˜¯åˆ é™¤æ•´ä¸ªç›®å½•ï¼Œä»¥æé«˜å¥å£®æ€§å¹¶ä¸ tasks.py ä¿æŒä¸€è‡´
        episodes_to_delete_res = await session.execute(
            select(Episode.danmakuFilePath).where(Episode.sourceId == source_id)
        )
        for file_path_str in episodes_to_delete_res.scalars().all():
            if fs_path := _get_fs_path_from_web_path(file_path_str):
                if fs_path.is_file():
                    fs_path.unlink(missing_ok=True)

        await session.delete(source)
        await session.commit()
        return True
    return False

async def delete_episode(session: AsyncSession, episode_id: int) -> bool:
    """åˆ é™¤ä¸€ä¸ªåˆ†é›†åŠå…¶å¼¹å¹•æ–‡ä»¶ã€‚"""
    episode = await session.get(Episode, episode_id)
    if episode:
        if episode.danmakuFilePath:
            fs_path = _get_fs_path_from_web_path(episode.danmakuFilePath)
            if fs_path and fs_path.is_file():
                fs_path.unlink(missing_ok=True)
        await session.delete(episode)
        await session.commit()
        return True
    return False

async def reassociate_anime_sources(session: AsyncSession, source_anime_id: int, target_anime_id: int) -> bool:
    """
    å°†ä¸€ä¸ªç•ªå‰§çš„æ‰€æœ‰æºæ™ºèƒ½åœ°åˆå¹¶åˆ°å¦ä¸€ä¸ªç•ªå‰§ï¼Œç„¶ååˆ é™¤åŸå§‹ç•ªå‰§ã€‚
    - å¦‚æœç›®æ ‡ç•ªå‰§å·²å­˜åœ¨ç›¸åŒæä¾›å•†çš„æºï¼Œåˆ™åˆå¹¶å…¶ä¸‹çš„åˆ†é›†ï¼Œè€Œä¸æ˜¯ç›´æ¥åˆ é™¤ã€‚
    - ç§»åŠ¨ä¸å†²çªçš„æºï¼Œå¹¶åŒæ—¶ç§»åŠ¨å…¶ä¸‹çš„å¼¹å¹•æ–‡ä»¶ã€‚
    - åœ¨åˆå¹¶åé‡æ–°ä¸ºç›®æ ‡ç•ªå‰§çš„æ‰€æœ‰æºç¼–å·ï¼Œä»¥ç¡®ä¿é¡ºåºæ­£ç¡®ã€‚
    """
    if source_anime_id == target_anime_id:
        return False  # ä¸èƒ½å°†ä¸€ä¸ªä½œå“ä¸å®ƒè‡ªå·±åˆå¹¶

    # 1. é«˜æ•ˆåœ°é¢„åŠ è½½æ‰€æœ‰éœ€è¦çš„æ•°æ®ï¼ŒåŒ…æ‹¬ç›®æ ‡ä½œå“çš„åˆ†é›†
    source_anime_stmt = select(Anime).where(Anime.id == source_anime_id).options(
        selectinload(Anime.sources).selectinload(AnimeSource.episodes)
    )
    target_anime_stmt = select(Anime).where(Anime.id == target_anime_id).options(
        selectinload(Anime.sources).selectinload(AnimeSource.episodes)
    )
    source_anime = (await session.execute(source_anime_stmt)).scalar_one_or_none()
    target_anime = (await session.execute(target_anime_stmt)).scalar_one_or_none()

    if not source_anime or not target_anime:
        logger.error(f"é‡æ–°å…³è”å¤±è´¥ï¼šæºç•ªå‰§(ID: {source_anime_id})æˆ–ç›®æ ‡ç•ªå‰§(ID: {target_anime_id})æœªæ‰¾åˆ°ã€‚")
        return False

    # 2. è¯†åˆ«ç›®æ ‡ç•ªå‰§å·²æœ‰çš„æä¾›å•†åŠå…¶æºå¯¹è±¡ï¼Œç”¨äºå†²çªæ£€æµ‹å’Œåˆ†é›†åˆå¹¶
    target_sources_map = {s.providerName: s for s in target_anime.sources}
    logger.info(f"ç›®æ ‡ç•ªå‰§ (ID: {target_anime_id}) å·²æœ‰æº: {list(target_sources_map.keys())}")

    # 3. éå†æºç•ªå‰§çš„æºï¼Œå¤„ç†å†²çªæˆ–ç§»åŠ¨
    for source_to_process in list(source_anime.sources):  # ä½¿ç”¨å‰¯æœ¬è¿›è¡Œè¿­ä»£
        provider = source_to_process.providerName
        if provider in target_sources_map:
            # å†²çªï¼šåˆå¹¶åˆ†é›†
            target_source = target_sources_map[provider]
            logger.warning(f"å‘ç°å†²çªæº: æä¾›å•† '{provider}'ã€‚å°†å°è¯•åˆå¹¶åˆ†é›†åˆ°ç›®æ ‡æº {target_source.id}ã€‚")
            
            target_episode_indices = {ep.episodeIndex for ep in target_source.episodes}

            for episode_to_move in list(source_to_process.episodes):
                if episode_to_move.episodeIndex not in target_episode_indices:
                    # ç§»åŠ¨ä¸é‡å¤çš„åˆ†é›†
                    logger.info(f"æ­£åœ¨ç§»åŠ¨åˆ†é›† {episode_to_move.episodeIndex} (ID: {episode_to_move.id}) åˆ°ç›®æ ‡æº {target_source.id}")
                    
                    # ç§»åŠ¨å¼¹å¹•æ–‡ä»¶
                    if episode_to_move.danmakuFilePath:
                        old_path = _get_fs_path_from_web_path(episode_to_move.danmakuFilePath)
                        new_web_path = f"/app/config/danmaku/{target_anime_id}/{episode_to_move.id}.xml"
                        new_fs_path = _get_fs_path_from_web_path(new_web_path)
                        if old_path and old_path.exists() and new_fs_path:
                            new_fs_path.parent.mkdir(parents=True, exist_ok=True)
                            old_path.rename(new_fs_path)
                            episode_to_move.danmakuFilePath = new_web_path
                    
                    episode_to_move.sourceId = target_source.id
                    target_source.episodes.append(episode_to_move)
                else:
                    # åˆ é™¤é‡å¤çš„åˆ†é›†
                    logger.info(f"åˆ†é›† {episode_to_move.episodeIndex} åœ¨ç›®æ ‡æºä¸­å·²å­˜åœ¨ï¼Œå°†åˆ é™¤æºåˆ†é›† {episode_to_move.id}")
                    if episode_to_move.danmakuFilePath:
                        fs_path = _get_fs_path_from_web_path(episode_to_move.danmakuFilePath)
                        if fs_path and fs_path.is_file():
                            fs_path.unlink(missing_ok=True)
                    await session.delete(episode_to_move)
            
            # åˆ é™¤ç°å·²ä¸ºç©ºçš„æº
            await session.delete(source_to_process)
        else:
            # ä¸å†²çªï¼šç§»åŠ¨æ­¤æºåŠå…¶å¼¹å¹•æ–‡ä»¶
            logger.info(f"æ­£åœ¨å°†æº '{provider}' (ID: {source_to_process.id}) ç§»åŠ¨åˆ°ç›®æ ‡ç•ªå‰§ (ID: {target_anime_id})ã€‚")
            for ep in source_to_process.episodes:
                if ep.danmakuFilePath:
                    old_path = _get_fs_path_from_web_path(ep.danmakuFilePath)
                    new_web_path = f"/app/config/danmaku/{target_anime_id}/{ep.id}.xml"
                    new_fs_path = _get_fs_path_from_web_path(new_web_path)
                    if old_path and old_path.exists() and new_fs_path:
                        new_fs_path.parent.mkdir(parents=True, exist_ok=True)
                        old_path.rename(new_fs_path)
                        ep.danmakuFilePath = new_web_path
            source_to_process.animeId = target_anime_id
            target_anime.sources.append(source_to_process)

    # 4. é‡æ–°ç¼–å·ç›®æ ‡ç•ªå‰§çš„æ‰€æœ‰æºçš„ sourceOrder
    sorted_sources = sorted(target_anime.sources, key=lambda s: s.sourceOrder)
    logger.info(f"æ­£åœ¨ä¸ºç›®æ ‡ç•ªå‰§ (ID: {target_anime_id}) çš„ {len(sorted_sources)} ä¸ªæºé‡æ–°ç¼–å·...")
    for i, source in enumerate(sorted_sources):
        new_order = i + 1
        if source.sourceOrder != new_order:
            source.sourceOrder = new_order

    # 5. åˆ é™¤ç°å·²ä¸ºç©ºçš„æºç•ªå‰§
    logger.info(f"æ­£åœ¨åˆ é™¤ç°å·²ä¸ºç©ºçš„æºç•ªå‰§ (ID: {source_anime_id})ã€‚")
    await session.delete(source_anime)
    await session.commit()
    logger.info("ç•ªå‰§æºé‡æ–°å…³è”æˆåŠŸã€‚")
    return True

async def update_episode_info(session: AsyncSession, episode_id: int, update_data: models.EpisodeInfoUpdate) -> bool:
    """æ›´æ–°å•ä¸ªåˆ†é›†çš„ä¿¡æ¯ã€‚å¦‚æœé›†æ•°è¢«ä¿®æ”¹ï¼Œå°†é‡å‘½åå¼¹å¹•æ–‡ä»¶å¹¶æ›´æ–°è·¯å¾„ã€‚"""
    # ä½¿ç”¨ joinedload é«˜æ•ˆåœ°è·å–å…³è”çš„ source å’Œ anime ä¿¡æ¯ # type: ignore
    stmt = select(Episode).where(Episode.id == episode_id).options(joinedload(Episode.source).joinedload(AnimeSource.anime))
    result = await session.execute(stmt)
    episode = result.scalar_one_or_none()

    if not episode:
        return False

    # æƒ…å†µ1: é›†æ•°æœªæ”¹å˜ï¼Œä»…æ›´æ–°æ ‡é¢˜æˆ–URL
    if episode.episodeIndex == update_data.episodeIndex:
        episode.title = update_data.title
        episode.sourceUrl = update_data.sourceUrl
        await session.commit()
        return True

    # æƒ…å†µ2: é›†æ•°å·²æ”¹å˜ï¼Œéœ€è¦é‡æ–°ç”ŸæˆIDå¹¶ç§»åŠ¨æ–‡ä»¶
    # 1. æ£€æŸ¥æ–°é›†æ•°æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…å†²çª
    conflict_stmt = select(Episode.id).where(
        Episode.sourceId == episode.sourceId,
        Episode.episodeIndex == update_data.episodeIndex
    )
    if (await session.execute(conflict_stmt)).scalar_one_or_none():
        raise ValueError("è¯¥é›†æ•°å·²å­˜åœ¨ï¼Œè¯·ä½¿ç”¨å…¶ä»–é›†æ•°ã€‚")

    # 2. è®¡ç®—æ–°çš„ç¡®å®šæ€§ID
    source_order = episode.source.sourceOrder
    if source_order is None:
        # è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å›é€€å’Œè¿ç§»é€»è¾‘ã€‚å¦‚æœä¸€ä¸ªæ—§çš„æºæ²¡æœ‰ sourceOrderï¼Œ
        # æˆ‘ä»¬å°±ä¸ºå…¶åˆ†é…ä¸€ä¸ªæ–°çš„ã€æŒä¹…çš„åºå·ã€‚
        logger.warning(f"æº ID {episode.sourceId} ç¼ºå°‘ sourceOrderï¼Œå°†ä¸ºå…¶åˆ†é…ä¸€ä¸ªæ–°çš„ã€‚")
        source_order = await _assign_source_order_if_missing(session, episode.source.animeId, episode.sourceId)
    new_episode_id_str = f"25{episode.source.animeId:06d}{source_order:02d}{update_data.episodeIndex:04d}"
    new_episode_id = int(new_episode_id_str)

    # 3. é‡å‘½åå¼¹å¹•æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    new_web_path = None
    if episode.danmakuFilePath:
        old_absolute_path = _get_fs_path_from_web_path(episode.danmakuFilePath)
        
        # ä¿®æ­£ï¼šæ–°çš„Webè·¯å¾„å’Œæ–‡ä»¶ç³»ç»Ÿè·¯å¾„åº”ä¸ tasks.py ä¿æŒä¸€è‡´ï¼ˆä¸åŒ…å« source_idï¼‰
        new_web_path = f"/app/config/danmaku/{episode.source.animeId}/{new_episode_id}.xml"
        new_absolute_path = DANMAKU_BASE_DIR / str(episode.source.animeId) / f"{new_episode_id}.xml"
        
        if old_absolute_path and old_absolute_path.exists():
            try:
                new_absolute_path.parent.mkdir(parents=True, exist_ok=True)
                old_absolute_path.rename(new_absolute_path)
                logger.info(f"å¼¹å¹•æ–‡ä»¶å·²é‡å‘½å: {old_absolute_path} -> {new_absolute_path}")
            except OSError as e:
                logger.error(f"é‡å‘½åå¼¹å¹•æ–‡ä»¶å¤±è´¥: {e}")
                new_web_path = episode.danmakuFilePath # å¦‚æœé‡å‘½åå¤±è´¥ï¼Œåˆ™ä¿ç•™æ—§è·¯å¾„
    
    # 4. åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ†é›†å¯¹è±¡
    new_episode = Episode(
        id=new_episode_id, sourceId=episode.sourceId, episodeIndex=update_data.episodeIndex,
        title=update_data.title, sourceUrl=update_data.sourceUrl,
        providerEpisodeId=episode.providerEpisodeId, fetchedAt=episode.fetchedAt, 
        commentCount=episode.commentCount, danmakuFilePath=new_web_path
    )
    session.add(new_episode)
    
    # 5. åˆ é™¤æ—§çš„åˆ†é›†è®°å½• (ç”±äºæ²¡æœ‰å¼¹å¹•å…³è”ï¼Œå¯ä»¥ç›´æ¥åˆ é™¤)
    await session.delete(episode)
    await session.commit()
    return True

async def sync_scrapers_to_db(session: AsyncSession, provider_names: List[str]):
    if not provider_names: return
    
    existing_stmt = select(Scraper.providerName)
    existing_providers = set((await session.execute(existing_stmt)).scalars().all())
    
    new_providers = [name for name in provider_names if name not in existing_providers]
    if not new_providers: return

    max_order_stmt = select(func.max(Scraper.displayOrder))
    max_order = (await session.execute(max_order_stmt)).scalar_one_or_none() or 0
    
    session.add_all([
        Scraper(providerName=name, displayOrder=max_order + i + 1, useProxy=False)
        for i, name in enumerate(new_providers)
    ])
    await session.commit()

async def get_scraper_setting_by_name(session: AsyncSession, provider_name: str) -> Optional[Dict[str, Any]]:
    """è·å–å•ä¸ªæœç´¢æºçš„è®¾ç½®ã€‚"""
    scraper = await session.get(Scraper, provider_name)
    if scraper:
        return {
            "providerName": scraper.providerName,
            "isEnabled": scraper.isEnabled,
            "displayOrder": scraper.displayOrder,
            "useProxy": scraper.useProxy
        }
    return None

async def update_scraper_proxy(session: AsyncSession, provider_name: str, use_proxy: bool) -> bool:
    """æ›´æ–°å•ä¸ªæœç´¢æºçš„ä»£ç†è®¾ç½®ã€‚"""
    stmt = update(Scraper).where(Scraper.providerName == provider_name).values(useProxy=use_proxy)
    result = await session.execute(stmt)
    return result.rowcount > 0

async def get_all_scraper_settings(session: AsyncSession) -> List[Dict[str, Any]]:
    stmt = select(Scraper).order_by(Scraper.displayOrder)
    result = await session.execute(stmt)
    return [
        {"providerName": s.providerName, "isEnabled": s.isEnabled, "displayOrder": s.displayOrder, "useProxy": s.useProxy}
        for s in result.scalars()
    ]

async def update_scrapers_settings(session: AsyncSession, settings: List[models.ScraperSetting]):
    for s in settings:
        await session.execute(
            update(Scraper)
            .where(Scraper.providerName == s.providerName)
            .values(isEnabled=s.isEnabled, displayOrder=s.displayOrder, useProxy=s.useProxy)
        )
    await session.commit()

async def remove_stale_scrapers(session: AsyncSession, discovered_providers: List[str]):
    if not discovered_providers:
        logging.warning("å‘ç°çš„æœç´¢æºåˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡æ¸…ç†è¿‡æ—¶æºçš„æ“ä½œã€‚")
        return
    stmt = delete(Scraper).where(Scraper.providerName.notin_(discovered_providers))
    await session.execute(stmt)
    await session.commit()

# --- Metadata Source Management ---

async def sync_metadata_sources_to_db(session: AsyncSession, provider_names: List[str]):
    if not provider_names: return
    
    existing_stmt = select(MetadataSource.providerName)
    existing_providers = set((await session.execute(existing_stmt)).scalars().all())
    
    new_providers = [name for name in provider_names if name not in existing_providers]
    if not new_providers: return

    max_order_stmt = select(func.max(MetadataSource.displayOrder))
    max_order = (await session.execute(max_order_stmt)).scalar_one_or_none() or 0
    
    session.add_all([
        MetadataSource(
            providerName=name, displayOrder=max_order + i + 1,
            isAuxSearchEnabled=(name == 'tmdb'), useProxy=True
        )
        for i, name in enumerate(new_providers)
    ])
    await session.commit()

async def get_all_metadata_source_settings(session: AsyncSession) -> List[Dict[str, Any]]:
    stmt = select(MetadataSource).order_by(MetadataSource.displayOrder)
    result = await session.execute(stmt)
    return [{
        "providerName": s.providerName, "isEnabled": s.isEnabled,
        "isAuxSearchEnabled": s.isAuxSearchEnabled, "displayOrder": s.displayOrder,
        "useProxy": s.useProxy, "isFailoverEnabled": s.isFailoverEnabled,
        "logRawResponses": s.logRawResponses
    } for s in result.scalars()]

async def update_metadata_sources_settings(session: AsyncSession, settings: List['models.MetadataSourceSettingUpdate']):
    for s in settings:
        is_aux_enabled = True if s.providerName == 'tmdb' else s.isAuxSearchEnabled
        await session.execute(
            update(MetadataSource)
            .where(MetadataSource.providerName == s.providerName)
            .values(isAuxSearchEnabled=is_aux_enabled, displayOrder=s.displayOrder)
        )
    await session.commit()

async def get_metadata_source_setting_by_name(session: AsyncSession, provider_name: str) -> Optional[Dict[str, Any]]:
    """è·å–å•ä¸ªå…ƒæ•°æ®æºçš„è®¾ç½®ã€‚"""
    source = await session.get(MetadataSource, provider_name)
    if source:
        return {"useProxy": source.useProxy, "logRawResponses": source.logRawResponses}
    return None

async def update_metadata_source_specific_settings(session: AsyncSession, provider_name: str, settings: Dict[str, Any]):
    """æ›´æ–°å•ä¸ªå…ƒæ•°æ®æºçš„ç‰¹å®šè®¾ç½®ï¼ˆå¦‚ logRawResponsesï¼‰ã€‚"""
    await session.execute(update(MetadataSource).where(MetadataSource.providerName == provider_name).values(**settings))

async def get_enabled_aux_metadata_sources(session: AsyncSession) -> List[Dict[str, Any]]:
    """è·å–æ‰€æœ‰å·²å¯ç”¨è¾…åŠ©æœç´¢çš„å…ƒæ•°æ®æºã€‚"""
    stmt = (
        select(MetadataSource)
        .where(MetadataSource.isAuxSearchEnabled == True)
        .order_by(MetadataSource.displayOrder)
    )
    result = await session.execute(stmt)
    return [
        {"providerName": s.providerName, "isEnabled": s.isEnabled, "isAuxSearchEnabled": s.isAuxSearchEnabled, "displayOrder": s.displayOrder, "useProxy": s.useProxy, "isFailoverEnabled": s.isFailoverEnabled}
        for s in result.scalars()
    ]

async def get_enabled_failover_sources(session: AsyncSession) -> List[Dict[str, Any]]:
    """è·å–æ‰€æœ‰å·²å¯ç”¨æ•…éšœè½¬ç§»çš„å…ƒæ•°æ®æºã€‚"""
    stmt = (
        select(MetadataSource)
        .where(MetadataSource.isFailoverEnabled == True)
        .order_by(MetadataSource.displayOrder)
    )
    result = await session.execute(stmt)
    return [
        {"providerName": s.providerName, "isEnabled": s.isEnabled, "isAuxSearchEnabled": s.isAuxSearchEnabled, "displayOrder": s.displayOrder, "useProxy": s.useProxy, "isFailoverEnabled": s.isFailoverEnabled}
        for s in result.scalars()
    ]
# --- Config & Cache ---

async def get_config_value(session: AsyncSession, key: str, default: str) -> str:
    stmt = select(Config.configValue).where(Config.configKey == key)
    result = await session.execute(stmt)
    value = result.scalar_one_or_none()
    return value if value is not None else default

async def get_cache(session: AsyncSession, key: str) -> Optional[Any]:
    stmt = select(CacheData.cacheValue).where(CacheData.cacheKey == key, CacheData.expiresAt > func.now())
    result = await session.execute(stmt)
    value = result.scalar_one_or_none()
    if value:
        try:
            return json.loads(value)
        except json.JSONDecodeError:
            return None
    return None

async def set_cache(session: AsyncSession, key: str, value: Any, ttl_seconds: int, provider: Optional[str] = None):
    json_value = json.dumps(value, ensure_ascii=False)
    expires_at = get_now() + timedelta(seconds=ttl_seconds)

    dialect = session.bind.dialect.name
    values_to_insert = {"cacheProvider": provider, "cacheKey": key, "cacheValue": json_value, "expiresAt": expires_at}

    if dialect == 'mysql':
        stmt = mysql_insert(CacheData).values(values_to_insert)
        stmt = stmt.on_duplicate_key_update(
            cache_provider=stmt.inserted.cache_provider,
            cache_value=stmt.inserted.cache_value,
            expires_at=stmt.inserted.expires_at
        )
    elif dialect == 'postgresql':
        stmt = postgresql_insert(CacheData).values(values_to_insert)
        # ä¿®æ­£ï¼šä½¿ç”¨ on_conflict_do_update å¹¶é€šè¿‡ index_elements æŒ‡å®šä¸»é”®åˆ—ï¼Œä»¥æé«˜å…¼å®¹æ€§
        stmt = stmt.on_conflict_do_update(
            index_elements=['cache_key'],
            set_={"cache_provider": stmt.excluded.cache_provider, "cache_value": stmt.excluded.cache_value, "expires_at": stmt.excluded.expires_at}
        )
    else:
        raise NotImplementedError(f"ç¼“å­˜è®¾ç½®åŠŸèƒ½å°šæœªä¸ºæ•°æ®åº“ç±»å‹ '{dialect}' å®ç°ã€‚")

    await session.execute(stmt)
    await session.commit()

async def is_system_task(session: AsyncSession, task_id: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºç³»ç»Ÿå†…ç½®ä»»åŠ¡"""
    system_task_ids = ["system_token_reset"]
    return task_id in system_task_ids

async def delete_scheduled_task(session: AsyncSession, task_id: str) -> bool:
    """åˆ é™¤å®šæ—¶ä»»åŠ¡ï¼Œä½†ä¸å…è®¸åˆ é™¤ç³»ç»Ÿå†…ç½®ä»»åŠ¡"""
    # æ£€æŸ¥æ˜¯å¦ä¸ºç³»ç»Ÿä»»åŠ¡
    if await is_system_task(session, task_id):
        raise ValueError("ä¸å…è®¸åˆ é™¤ç³»ç»Ÿå†…ç½®ä»»åŠ¡ã€‚")
    
    task = await session.get(orm_models.ScheduledTask, task_id)
    if not task:
        return False
    
    await session.delete(task)
    await session.commit()
    return True

async def update_scheduled_task(
    session: AsyncSession, 
    task_id: str, 
    name: str, 
    cron: str, 
    is_enabled: bool
) -> bool:
    """æ›´æ–°å®šæ—¶ä»»åŠ¡ï¼Œä½†ä¸å…è®¸ä¿®æ”¹ç³»ç»Ÿå†…ç½®ä»»åŠ¡çš„å…³é”®å±æ€§"""
    task = await session.get(orm_models.ScheduledTask, task_id)
    if not task:
        return False
    
    # ç³»ç»Ÿä»»åŠ¡åªå…è®¸ä¿®æ”¹å¯ç”¨çŠ¶æ€
    if await is_system_task(session, task_id):
        task.isEnabled = is_enabled
    else:
        task.name = name
        task.cronExpression = cron
        task.isEnabled = is_enabled
    
    await session.commit()
    return True

async def check_duplicate_import(
    session: AsyncSession,
    provider: str,
    media_id: str,
    anime_title: str,
    media_type: str,
    season: Optional[int] = None,
    year: Optional[int] = None,
    is_single_episode: bool = False,
    episode_index: Optional[int] = None,
    title_recognition_manager=None
) -> Optional[str]:
    """
    ç»Ÿä¸€çš„é‡å¤å¯¼å…¥æ£€æŸ¥å‡½æ•° - ç²¾ç¡®æ£€æŸ¥æ¨¡å¼
    åªæœ‰å®Œå…¨é‡å¤ï¼ˆç›¸åŒprovider + media_id + é›†æ•° + å·²æœ‰å¼¹å¹•ï¼‰æ—¶æ‰é˜»æ­¢å¯¼å…¥
    è¿”å›Noneè¡¨ç¤ºå¯ä»¥å¯¼å…¥ï¼Œè¿”å›å­—ç¬¦ä¸²è¡¨ç¤ºé‡å¤åŸå› 
    """
    # æ£€æŸ¥æ•°æ®æºæ˜¯å¦å·²å­˜åœ¨
    source_exists = await check_source_exists_by_media_id(session, provider, media_id)
    if not source_exists:
        # æ•°æ®æºä¸å­˜åœ¨ï¼Œå…è®¸å¯¼å…¥
        return None

    # æ•°æ®æºå­˜åœ¨ï¼Œè·å–anime_id
    anime_id = await get_anime_id_by_source_media_id(session, provider, media_id)
    if not anime_id:
        # ç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§
        return None

    # å¯¹äºå•é›†å¯¼å…¥ï¼Œæ£€æŸ¥è¯¥æ•°æ®æºçš„è¯¥é›†æ˜¯å¦å·²æœ‰å¼¹å¹•
    if is_single_episode and episode_index is not None:
        # è·å–è¯¥æ•°æ®æºçš„è¯¥é›†çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¿…é¡»æ˜¯ç›¸åŒ provider + media_idï¼‰
        stmt = select(Episode).join(
            AnimeSource, Episode.sourceId == AnimeSource.id
        ).where(
            AnimeSource.providerName == provider,
            AnimeSource.mediaId == media_id,
            Episode.episodeIndex == episode_index
        ).limit(1)
        result = await session.execute(stmt)
        episode_exists = result.scalar_one_or_none()

        if episode_exists and episode_exists.danmakuFilePath and episode_exists.commentCount > 0:
            return f"ä½œå“ '{anime_title}' çš„ç¬¬ {episode_index} é›†ï¼ˆ{provider}æºï¼‰å·²åœ¨åª’ä½“åº“ä¸­ä¸”å·²æœ‰ {episode_exists.commentCount} æ¡å¼¹å¹•ï¼Œæ— éœ€é‡å¤å¯¼å…¥"
        else:
            # è¯¥æ•°æ®æºçš„è¯¥é›†ä¸å­˜åœ¨æˆ–æ²¡æœ‰å¼¹å¹•ï¼Œå…è®¸å¯¼å…¥
            return None

    # å¯¹äºå…¨é‡å¯¼å…¥ï¼Œæ£€æŸ¥è¯¥æ•°æ®æºä¸‹å·²æœ‰å¼¹å¹•çš„é›†æ•°
    stmt = select(func.count(Episode.id)).join(
        AnimeSource, Episode.sourceId == AnimeSource.id
    ).where(
        AnimeSource.providerName == provider,
        AnimeSource.mediaId == media_id,
        Episode.danmakuFilePath.isnot(None),
        Episode.commentCount > 0
    )
    result = await session.execute(stmt)
    episode_count = result.scalar_one()

    if episode_count > 0:
        # æœ‰å¼¹å¹•çš„é›†æ•°ï¼Œç»™å‡ºæç¤ºä½†ä¸å®Œå…¨é˜»æ­¢
        # å› ä¸ºå¯èƒ½æœ‰æ–°å¢çš„é›†æ•°éœ€è¦å¯¼å…¥
        logger.info(f"æ•°æ®æº ({provider}/{media_id}) å·²æœ‰ {episode_count} é›†å¼¹å¹•ï¼Œä½†å…è®¸å¯¼å…¥æ–°é›†æ•°")
        return None
    else:
        # æ•°æ®æºå­˜åœ¨ä½†æ²¡æœ‰å¼¹å¹•ï¼Œå…è®¸å¯¼å…¥
        return None

async def update_config_value(session: AsyncSession, key: str, value: str):
    dialect = session.bind.dialect.name
    values_to_insert = {"configKey": key, "configValue": value}

    if dialect == 'mysql':
        stmt = mysql_insert(Config).values(values_to_insert)
        stmt = stmt.on_duplicate_key_update(config_value=stmt.inserted.config_value)
    elif dialect == 'postgresql':
        stmt = postgresql_insert(Config).values(values_to_insert)
        # ä¿®æ­£ï¼šä½¿ç”¨ on_conflict_do_update å¹¶é€šè¿‡ index_elements æŒ‡å®šä¸»é”®åˆ—ï¼Œä»¥æé«˜å…¼å®¹æ€§
        stmt = stmt.on_conflict_do_update(
            index_elements=['config_key'],
            set_={'config_value': stmt.excluded.config_value}
        )
    else:
        raise NotImplementedError(f"é…ç½®æ›´æ–°åŠŸèƒ½å°šæœªä¸ºæ•°æ®åº“ç±»å‹ '{dialect}' å®ç°ã€‚")

    await session.execute(stmt)
    await session.commit()

async def clear_expired_cache(session: AsyncSession):
    await session.execute(delete(CacheData).where(CacheData.expiresAt <= get_now()))
    await session.commit()

async def clear_expired_oauth_states(session: AsyncSession):
    await session.execute(delete(OauthState).where(OauthState.expiresAt <= get_now()))
    await session.commit()

async def clear_all_cache(session: AsyncSession) -> int:
    result = await session.execute(delete(CacheData))
    await session.commit()
    return result.rowcount

async def delete_cache(session: AsyncSession, key: str) -> bool:
    result = await session.execute(delete(CacheData).where(CacheData.cacheKey == key))
    await session.commit()
    return result.rowcount > 0

async def get_cache_keys_by_pattern(session: AsyncSession, pattern: str) -> List[str]:
    """æ ¹æ®æ¨¡å¼è·å–ç¼“å­˜é”®åˆ—è¡¨"""
    # å°†é€šé…ç¬¦*è½¬æ¢ä¸ºSQLçš„%
    sql_pattern = pattern.replace('*', '%')
    stmt = select(CacheData.cacheKey).where(
        CacheData.cacheKey.like(sql_pattern),
        CacheData.expiresAt > func.now()
    )
    result = await session.execute(stmt)
    return [row[0] for row in result.fetchall()]

async def update_episode_fetch_time(session: AsyncSession, episode_id: int):
    await session.execute(update(Episode).where(Episode.id == episode_id).values(fetchedAt=get_now()))
    await session.commit()

async def update_episode_danmaku_info(session: AsyncSession, episode_id: int, file_path: str, count: int):
    """æ›´æ–°åˆ†é›†çš„å¼¹å¹•æ–‡ä»¶è·¯å¾„å’Œå¼¹å¹•æ•°é‡ã€‚"""
    stmt = update(Episode).where(Episode.id == episode_id).values(
        danmakuFilePath=file_path, commentCount=count, fetchedAt=get_now()
    )
    await session.execute(stmt) # type: ignore
    await session.flush()
# --- API Token Management ---

async def get_all_api_tokens(session: AsyncSession) -> List[Dict[str, Any]]:
    stmt = select(ApiToken).order_by(ApiToken.createdAt.desc())
    result = await session.execute(stmt)
    return [
        {"id": t.id, "name": t.name, "token": t.token, "isEnabled": t.isEnabled, "expiresAt": t.expiresAt, "createdAt": t.createdAt, "dailyCallLimit": t.dailyCallLimit, "dailyCallCount": t.dailyCallCount}
        for t in result.scalars()
    ]

async def get_api_token_by_id(session: AsyncSession, token_id: int) -> Optional[Dict[str, Any]]:
    token = await session.get(ApiToken, token_id)
    if token:
        return {"id": token.id, "name": token.name, "token": token.token, "isEnabled": token.isEnabled, "expiresAt": token.expiresAt, "createdAt": token.createdAt, "dailyCallLimit": token.dailyCallLimit, "dailyCallCount": token.dailyCallCount}
    return None

async def get_api_token_by_token_str(session: AsyncSession, token_str: str) -> Optional[Dict[str, Any]]:
    stmt = select(ApiToken).where(ApiToken.token == token_str)
    result = await session.execute(stmt)
    token = result.scalar_one_or_none()
    if token:
        return {"id": token.id, "name": token.name, "token": token.token, "isEnabled": token.isEnabled, "expiresAt": token.expiresAt, "createdAt": token.createdAt, "dailyCallLimit": token.dailyCallLimit, "dailyCallCount": token.dailyCallCount}
    return None

async def create_api_token(session: AsyncSession, name: str, token: str, validityPeriod: str, daily_call_limit: int) -> int:
    """åˆ›å»ºæ–°çš„API Tokenï¼Œå¦‚æœåç§°å·²å­˜åœ¨åˆ™ä¼šå¤±è´¥ã€‚"""
    # æ£€æŸ¥åç§°æ˜¯å¦å·²å­˜åœ¨
    existing_token = await session.execute(select(ApiToken).where(ApiToken.name == name))
    if existing_token.scalar_one_or_none():
        raise ValueError(f"åç§°ä¸º '{name}' çš„Tokenå·²å­˜åœ¨ã€‚")
    
    expires_at = None
    if validityPeriod != "permanent":
        days = int(validityPeriod.replace('d', '')) # type: ignore
        # ä¿®æ­£ï¼šç¡®ä¿å†™å…¥æ•°æ®åº“çš„æ—¶é—´æ˜¯ naive çš„
        expires_at = get_now() + timedelta(days=days)
    new_token = ApiToken(
        name=name, token=token, 
        expiresAt=expires_at, 
        createdAt=get_now(),
        dailyCallLimit=daily_call_limit
    )
    session.add(new_token)
    await session.commit()
    return new_token.id

async def update_api_token(
    session: AsyncSession,
    token_id: int,
    name: str,
    daily_call_limit: int,
    validity_period: str
) -> bool:
    """æ›´æ–°API Tokençš„åç§°ã€è°ƒç”¨ä¸Šé™å’Œæœ‰æ•ˆæœŸã€‚"""
    token = await session.get(orm_models.ApiToken, token_id)
    if not token:
        return False

    token.name = name
    token.dailyCallLimit = daily_call_limit

    if validity_period != 'custom':
        if validity_period == 'permanent':
            token.expiresAt = None
        else:
            try:
                days = int(validity_period.replace('d', ''))
                token.expiresAt = get_now() + timedelta(days=days)
            except (ValueError, TypeError):
                logger.warning(f"æ›´æ–°Tokenæ—¶æ”¶åˆ°æ— æ•ˆçš„æœ‰æ•ˆæœŸæ ¼å¼: '{validity_period}'")

    await session.commit()
    return True

async def delete_api_token(session: AsyncSession, token_id: int) -> bool:
    token = await session.get(ApiToken, token_id)
    if token:
        await session.delete(token)
        await session.commit()
        return True
    return False

async def toggle_api_token(session: AsyncSession, token_id: int) -> bool:
    token = await session.get(ApiToken, token_id)
    if token:
        token.isEnabled = not token.isEnabled
        await session.commit()
        return True
    return False

async def reset_token_counter(session: AsyncSession, token_id: int) -> bool:
    """å°†æŒ‡å®šTokençš„æ¯æ—¥è°ƒç”¨æ¬¡æ•°é‡ç½®ä¸º0ã€‚"""
    token = await session.get(orm_models.ApiToken, token_id)
    if not token:
        return False
    
    token.dailyCallCount = 0
    await session.commit()
    return True

async def validate_api_token(session: AsyncSession, token: str) -> Optional[Dict[str, Any]]:
    stmt = select(ApiToken).where(ApiToken.token == token, ApiToken.isEnabled == True)
    result = await session.execute(stmt)
    token_info = result.scalar_one_or_none()
    if not token_info:
        return None
    # éšç€ orm_models.py å’Œ database.py çš„ä¿®å¤ï¼ŒSQLAlchemy ç°åœ¨åº”è¿”å›æ—¶åŒºæ„ŸçŸ¥çš„ UTC æ—¥æœŸæ—¶é—´ã€‚
    if token_info.expiresAt:
        if token_info.expiresAt < get_now(): # Compare naive datetimes
            return None # Token å·²è¿‡æœŸ
    
    # --- æ–°å¢ï¼šæ¯æ—¥è°ƒç”¨é™åˆ¶æ£€æŸ¥ ---
    now = get_now()
    current_count = token_info.dailyCallCount
    
    # å¦‚æœæœ‰ä¸Šæ¬¡è°ƒç”¨è®°å½•ï¼Œä¸”ä¸æ˜¯ä»Šå¤©ï¼Œåˆ™è§†ä¸ºè®¡æ•°ä¸º0
    if token_info.lastCallAt and token_info.lastCallAt.date() < now.date():
        current_count = 0
        
    if token_info.dailyCallLimit != -1 and current_count >= token_info.dailyCallLimit:
        return None # Token å·²è¾¾åˆ°æ¯æ—¥è°ƒç”¨ä¸Šé™

    return {"id": token_info.id, "expiresAt": token_info.expiresAt, "dailyCallLimit": token_info.dailyCallLimit, "dailyCallCount": token_info.dailyCallCount} # type: ignore

async def increment_token_call_count(session: AsyncSession, token_id: int):
    """ä¸ºæŒ‡å®šçš„Tokenå¢åŠ è°ƒç”¨è®¡æ•°ã€‚"""
    token = await session.get(ApiToken, token_id)
    if not token:
        return

    # ä¿®æ­£ï¼šç®€åŒ–å‡½æ•°èŒè´£ï¼Œç°åœ¨åªè´Ÿè´£å¢åŠ è®¡æ•°ã€‚
    # é‡ç½®é€»è¾‘å·²ç§»è‡³ validate_api_token ä¸­ï¼Œä»¥é¿å…ç«äº‰æ¡ä»¶ã€‚
    token.dailyCallCount += 1
    # æ€»æ˜¯æ›´æ–°æœ€åè°ƒç”¨æ—¶é—´
    token.lastCallAt = get_now()
    # æ³¨æ„ï¼šè¿™é‡Œä¸ commitï¼Œç”±è°ƒç”¨æ–¹ï¼ˆAPIç«¯ç‚¹ï¼‰æ¥å†³å®šä½•æ—¶æäº¤äº‹åŠ¡

async def reset_all_token_daily_counts(session: AsyncSession) -> int:
    """é‡ç½®æ‰€æœ‰API Tokençš„æ¯æ—¥è°ƒç”¨æ¬¡æ•°ä¸º0ã€‚"""
    from sqlalchemy import update
    stmt = update(ApiToken).values(dailyCallCount=0)
    result = await session.execute(stmt)
    await session.commit()
    return result.rowcount

# --- UA Filter and Log Services ---

async def get_ua_rules(session: AsyncSession) -> List[Dict[str, Any]]:
    stmt = select(UaRule).order_by(UaRule.createdAt.desc())
    result = await session.execute(stmt)
    return [{"id": r.id, "uaString": r.uaString, "createdAt": r.createdAt} for r in result.scalars()]

async def add_ua_rule(session: AsyncSession, ua_string: str) -> int:
    new_rule = UaRule(uaString=ua_string, createdAt=get_now())
    session.add(new_rule)
    await session.commit()
    return new_rule.id

async def delete_ua_rule(session: AsyncSession, rule_id: int) -> bool:
    rule = await session.get(UaRule, rule_id)
    if rule:
        await session.delete(rule)
        await session.commit()
        return True
    return False

async def create_token_access_log(session: AsyncSession, token_id: int, ip_address: str, user_agent: Optional[str], log_status: str, path: Optional[str] = None):
    new_log = TokenAccessLog(
        tokenId=token_id, 
        ipAddress=ip_address, 
        userAgent=user_agent, 
        status=log_status, 
        path=path, 
        accessTime=get_now())
    session.add(new_log)
    await session.commit()

async def get_token_access_logs(session: AsyncSession, token_id: int) -> List[Dict[str, Any]]:
    stmt = select(TokenAccessLog).where(TokenAccessLog.tokenId == token_id).order_by(TokenAccessLog.accessTime.desc()).limit(200)
    result = await session.execute(stmt)
    return [
        {"ipAddress": log.ipAddress, "userAgent": log.userAgent, "accessTime": log.accessTime, "status": log.status, "path": log.path}
        for log in result.scalars()
    ]

async def toggle_source_favorite_status(session: AsyncSession, source_id: int) -> Optional[bool]:
    """
    Toggles the favorite status of a source.
    Returns the new favorite status (True/False) on success, or None if not found.
    """
    source = await session.get(AnimeSource, source_id)
    if not source:
        return None

    # Toggle the target source
    source.isFavorited = not source.isFavorited
    
    # If it was favorited, unfavorite all others for the same anime
    if source.isFavorited:
        stmt = (
            update(AnimeSource)
            .where(AnimeSource.animeId == source.animeId, AnimeSource.id != source_id)
            .values(isFavorited=False)
        )
        await session.execute(stmt)
    
    await session.commit()
    return source.isFavorited

async def toggle_source_incremental_refresh(session: AsyncSession, source_id: int) -> bool:
    source = await session.get(AnimeSource, source_id)
    if not source:
        return False
    source.incrementalRefreshEnabled = not source.incrementalRefreshEnabled
    await session.commit()
    return True

async def increment_incremental_refresh_failures(session: AsyncSession, source_id: int) -> int:
    source = await session.get(AnimeSource, source_id)
    if not source:
        return 0
    source.incrementalRefreshFailures += 1
    await session.commit()
    return source.incrementalRefreshFailures

async def reset_incremental_refresh_failures(session: AsyncSession, source_id: int):
    await session.execute(update(AnimeSource).where(AnimeSource.id == source_id).values(incrementalRefreshFailures=0))
    await session.commit()

async def disable_incremental_refresh(session: AsyncSession, source_id: int) -> bool:
    result = await session.execute(update(AnimeSource).where(AnimeSource.id == source_id).values(incrementalRefreshEnabled=False))
    await session.commit()
    return result.rowcount > 0

# --- OAuth State Management ---

async def create_oauth_state(session: AsyncSession, user_id: int) -> str:
    state = secrets.token_urlsafe(32)
    expires_at = get_now() + timedelta(minutes=10)
    new_state = OauthState(stateKey=state, userId=user_id, expiresAt=expires_at)
    session.add(new_state)
    await session.commit()
    return state

async def consume_oauth_state(session: AsyncSession, state: str) -> Optional[int]:
    stmt = select(OauthState).where(OauthState.stateKey == state, OauthState.expiresAt > get_now())
    result = await session.execute(stmt)
    state_obj = result.scalar_one_or_none()
    if state_obj:
        user_id = state_obj.userId
        await session.delete(state_obj)
        await session.commit()
        return user_id
    return None

async def get_bangumi_auth(session: AsyncSession, user_id: int) -> Dict[str, Any]:
    """
    è·å–ç”¨æˆ·çš„BangumiæˆæƒçŠ¶æ€ã€‚
    æ³¨æ„ï¼šæ­¤å‡½æ•°ç°åœ¨è¿”å›ä¸€ä¸ªä¸ºUIå®šåˆ¶çš„å­—å…¸ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„è®¤è¯å¯¹è±¡ã€‚
    """
    auth = await session.get(BangumiAuth, user_id)
    if auth:
        return {
            "isAuthenticated": True,
            "nickname": auth.nickname,
            "avatarUrl": auth.avatarUrl,
            "bangumiUserId": auth.bangumiUserId,
            "authorizedAt": auth.authorizedAt,
            "expiresAt": auth.expiresAt,
        }
    return {"isAuthenticated": False}

async def save_bangumi_auth(session: AsyncSession, user_id: int, auth_data: Dict[str, Any]):
    auth = await session.get(BangumiAuth, user_id)
    expires_at = auth_data.get('expiresAt')
    if expires_at and hasattr(expires_at, 'tzinfo') and expires_at.tzinfo:
        expires_at = expires_at.replace(tzinfo=None)

    if auth:
        auth.bangumiUserId = auth_data.get('bangumiUserId')
        auth.nickname = auth_data.get('nickname')
        auth.avatarUrl = auth_data.get('avatarUrl')
        auth.accessToken = auth_data.get('accessToken')
        auth.refreshToken = auth_data.get('refreshToken')
        auth.expiresAt = expires_at
        auth.authorizedAt = get_now()
    else:
        auth_data_copy = auth_data.copy()
        auth_data_copy['expiresAt'] = expires_at
        auth = BangumiAuth(userId=user_id, authorizedAt=get_now(), **auth_data_copy)
        session.add(auth)
    await session.commit()

async def delete_bangumi_auth(session: AsyncSession, user_id: int) -> bool:
    auth = await session.get(BangumiAuth, user_id)
    if auth:
        await session.delete(auth)
        await session.commit()
        return True
    return False

async def get_sources_with_incremental_refresh_enabled(session: AsyncSession) -> List[int]:
    stmt = select(AnimeSource.id).where(AnimeSource.incrementalRefreshEnabled == True)
    result = await session.execute(stmt)
    return result.scalars().all()

# --- Scheduled Tasks ---

async def get_animes_with_tmdb_id(session: AsyncSession) -> List[Dict[str, Any]]:
    stmt = (
        select(Anime.id.label("animeId"), Anime.title, AnimeMetadata.tmdbId, AnimeMetadata.tmdbEpisodeGroupId)
        .join(AnimeMetadata, Anime.id == AnimeMetadata.animeId)
        .where(Anime.type == 'tv_series', AnimeMetadata.tmdbId != None, AnimeMetadata.tmdbId != '')
    )
    result = await session.execute(stmt)
    return [dict(row) for row in result.mappings()]

async def update_anime_tmdb_group_id(session: AsyncSession, anime_id: int, group_id: str):
    await session.execute(update(AnimeMetadata).where(AnimeMetadata.animeId == anime_id).values(tmdbEpisodeGroupId=group_id))
    await session.commit()

async def update_anime_aliases_if_empty(session: AsyncSession, anime_id: int, aliases: Dict[str, Any]):
    # ä¿®æ­£ï¼šä½¿ç”¨ select().where() è€Œä¸æ˜¯ session.get()ï¼Œå› ä¸º anime_id ä¸æ˜¯ä¸»é”®
    stmt = select(AnimeAlias).where(AnimeAlias.animeId == anime_id)
    result = await session.execute(stmt)
    alias_record = result.scalar_one_or_none()

    if not alias_record: return

    if not alias_record.nameEn and aliases.get('nameEn'): alias_record.nameEn = aliases['nameEn']
    if not alias_record.nameJp and aliases.get('nameJp'): alias_record.nameJp = aliases['nameJp']
    if not alias_record.nameRomaji and aliases.get('nameRomaji'): alias_record.nameRomaji = aliases['nameRomaji']
    
    cn_aliases = aliases.get('aliases_cn', [])
    if not alias_record.aliasCn1 and len(cn_aliases) > 0: alias_record.aliasCn1 = cn_aliases[0]
    if not alias_record.aliasCn2 and len(cn_aliases) > 1: alias_record.aliasCn2 = cn_aliases[1]
    if not alias_record.aliasCn3 and len(cn_aliases) > 2: alias_record.aliasCn3 = cn_aliases[2]

    await session.flush()
    logging.info(f"ä¸ºä½œå“ ID {anime_id} æ›´æ–°äº†åˆ«åå­—æ®µã€‚")

async def get_scheduled_tasks(session: AsyncSession) -> List[Dict[str, Any]]:
    stmt = select(
        ScheduledTask.taskId.label("taskId"),
        ScheduledTask.name.label("name"),
        ScheduledTask.jobType.label("jobType"),
        ScheduledTask.cronExpression.label("cronExpression"),
        ScheduledTask.isEnabled.label("isEnabled"),
        ScheduledTask.lastRunAt.label("lastRunAt"),
        ScheduledTask.nextRunAt.label("nextRunAt")
    ).order_by(ScheduledTask.name)
    result = await session.execute(stmt)
    return [dict(row) for row in result.mappings()]
async def check_scheduled_task_exists_by_type(session: AsyncSession, job_type: str) -> bool:
    stmt = select(ScheduledTask.taskId).where(ScheduledTask.jobType == job_type).limit(1)
    result = await session.execute(stmt)
    return result.scalar_one_or_none() is not None

async def get_scheduled_task_id_by_type(session: AsyncSession, job_type: str) -> Optional[str]:
    """è·å–æŒ‡å®šç±»å‹çš„å®šæ—¶ä»»åŠ¡IDã€‚"""
    stmt = select(ScheduledTask.taskId).where(ScheduledTask.jobType == job_type).limit(1)
    result = await session.execute(stmt)
    return result.scalar_one_or_none()

async def get_scheduled_task(session: AsyncSession, task_id: str) -> Optional[Dict[str, Any]]:
    stmt = select(
        ScheduledTask.taskId.label("taskId"), 
        ScheduledTask.name.label("name"),
        ScheduledTask.jobType.label("jobType"), 
        ScheduledTask.cronExpression.label("cronExpression"),
        ScheduledTask.isEnabled.label("isEnabled"),
        ScheduledTask.lastRunAt.label("lastRunAt"),
        ScheduledTask.nextRunAt.label("nextRunAt")
    ).where(ScheduledTask.taskId == task_id)
    result = await session.execute(stmt)
    row = result.mappings().first()
    return dict(row) if row else None

async def create_scheduled_task(session: AsyncSession, task_id: str, name: str, job_type: str, cron: str, is_enabled: bool):
    new_task = ScheduledTask(taskId=task_id, name=name, jobType=job_type, cronExpression=cron, isEnabled=is_enabled)
    session.add(new_task)
    await session.commit()

async def update_scheduled_task(session: AsyncSession, task_id: str, name: str, cron: str, is_enabled: bool):
    task = await session.get(ScheduledTask, task_id)
    if task:
        task.name = name
        task.cronExpression = cron
        task.isEnabled = is_enabled
        await session.commit()

async def delete_scheduled_task(session: AsyncSession, task_id: str):
    task = await session.get(ScheduledTask, task_id)
    if task:
        await session.delete(task)
        await session.commit()

async def update_scheduled_task_run_times(session: AsyncSession, task_id: str, last_run: Optional[datetime], next_run: Optional[datetime]):
    values_to_update = {
        "lastRunAt": last_run.replace(tzinfo=None) if last_run else None,
        "nextRunAt": next_run.replace(tzinfo=None) if next_run else None
    }
    await session.execute(update(ScheduledTask).where(ScheduledTask.taskId == task_id).values(**values_to_update))
    await session.commit()

# --- Task History ---

async def create_task_in_history(
    session: AsyncSession,
    task_id: str,
    title: str,
    status: str,
    description: str,
    scheduled_task_id: Optional[str] = None,
    unique_key: Optional[str] = None
):
    now = get_now()
    new_task = TaskHistory(
        taskId=task_id, title=title, status=status, 
        description=description, scheduledTaskId=scheduled_task_id,
        createdAt=now, # type: ignore
        uniqueKey=unique_key,
        updatedAt=now
    )
    session.add(new_task)
    await session.commit()

async def update_task_progress_in_history(session: AsyncSession, task_id: str, status: str, progress: int, description: str):
    await session.execute(
        update(TaskHistory)
        .where(TaskHistory.taskId == task_id)
        .values(status=status, progress=progress, description=description, updatedAt=get_now())
    )
    await session.commit()

async def finalize_task_in_history(session: AsyncSession, task_id: str, status: str, description: str):
    await session.execute(
        update(TaskHistory)
        .where(TaskHistory.taskId == task_id)
        .values(status=status, description=description, progress=100, finishedAt=get_now(), updatedAt=get_now())
    )
    await session.commit()

    # ä»»åŠ¡å®Œæˆåï¼Œæ¸…ç†ä»»åŠ¡çŠ¶æ€ç¼“å­˜
    await clear_task_state_cache(session, task_id)

async def update_task_status(session: AsyncSession, task_id: str, status: str):
    await session.execute(update(TaskHistory).where(TaskHistory.taskId == task_id).values(status=status, updatedAt=get_now().replace(tzinfo=None)))
    await session.commit()

async def get_tasks_from_history(session: AsyncSession, search_term: Optional[str], status_filter: str, page: int, page_size: int) -> Dict[str, Any]:
    # ä¿®æ­£ï¼šæ˜¾å¼é€‰æ‹©éœ€è¦çš„åˆ—ï¼Œä»¥é¿å…åœ¨æ—§çš„æ•°æ®åº“æ¨¡å¼ä¸ŠæŸ¥è¯¢ä¸å­˜åœ¨çš„åˆ—ï¼ˆå¦‚ scheduled_task_idï¼‰
    base_stmt = select(
        TaskHistory.taskId,
        TaskHistory.title,
        TaskHistory.status,
        TaskHistory.progress,
        TaskHistory.description,
        TaskHistory.createdAt,
        TaskHistory.scheduledTaskId
    )

    if search_term:
        base_stmt = base_stmt.where(TaskHistory.title.like(f"%{search_term}%"))
    if status_filter == 'in_progress':
        base_stmt = base_stmt.where(TaskHistory.status.in_(['æ’é˜Ÿä¸­', 'è¿è¡Œä¸­', 'å·²æš‚åœ']))
    elif status_filter == 'completed':
        base_stmt = base_stmt.where(TaskHistory.status == 'å·²å®Œæˆ')

    count_stmt = select(func.count()).select_from(base_stmt.alias("count_subquery"))
    total_count = (await session.execute(count_stmt)).scalar_one()

    offset = (page - 1) * page_size
    data_stmt = base_stmt.order_by(TaskHistory.createdAt.desc()).offset(offset).limit(page_size)

    result = await session.execute(data_stmt)
    items = [
        {
            "taskId": row.taskId,
            "title": row.title,
            "status": row.status,
            "progress": row.progress,
            "description": row.description,
            "createdAt": row.createdAt,
            "isSystemTask": row.scheduledTaskId == "system_token_reset"
        }
        for row in result.mappings()
    ]
    return {"total": total_count, "list": items}

async def get_task_details_from_history(session: AsyncSession, task_id: str) -> Optional[Dict[str, Any]]:
    """è·å–å•ä¸ªä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ã€‚"""
    task = await session.get(TaskHistory, task_id)
    if task:
        return {
            "taskId": task.taskId,
            "title": task.title,
            "status": task.status,
            "progress": task.progress,
            "description": task.description,
            "createdAt": task.createdAt,
        }
    return None

async def get_task_from_history_by_id(session: AsyncSession, task_id: str) -> Optional[Dict[str, Any]]:
    task = await session.get(TaskHistory, task_id)
    if task:
        return {"taskId": task.taskId, "title": task.title, "status": task.status}
    return None

async def delete_task_from_history(session: AsyncSession, task_id: str) -> bool:
    try:
        # å…ˆæŸ¥è¯¢ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        task = await session.get(TaskHistory, task_id)
        if not task:
            logger.warning(f"å°è¯•åˆ é™¤ä¸å­˜åœ¨çš„ä»»åŠ¡: {task_id}")
            return False

        logger.info(f"æ­£åœ¨åˆ é™¤ä»»åŠ¡: {task_id}, çŠ¶æ€: {task.status}")

        # åˆ é™¤ä»»åŠ¡
        await session.delete(task)
        await session.commit()

        logger.info(f"æˆåŠŸåˆ é™¤ä»»åŠ¡: {task_id}")
        return True
    except Exception as e:
        logger.error(f"åˆ é™¤ä»»åŠ¡ {task_id} å¤±è´¥: {e}", exc_info=True)
        await session.rollback()
        return False

async def force_delete_task_from_history(session: AsyncSession, task_id: str) -> bool:
    """å¼ºåˆ¶åˆ é™¤ä»»åŠ¡ï¼Œä½¿ç”¨SQLç›´æ¥åˆ é™¤ï¼Œç»•è¿‡ORMå¯èƒ½çš„é”å®šé—®é¢˜"""
    try:
        logger.info(f"å¼ºåˆ¶åˆ é™¤ä»»åŠ¡: {task_id}")

        # ä½¿ç”¨SQLç›´æ¥åˆ é™¤
        stmt = delete(TaskHistory).where(TaskHistory.taskId == task_id)
        result = await session.execute(stmt)
        await session.commit()

        deleted_count = result.rowcount
        if deleted_count > 0:
            logger.info(f"å¼ºåˆ¶åˆ é™¤ä»»åŠ¡æˆåŠŸ: {task_id}, åˆ é™¤è¡Œæ•°: {deleted_count}")
            return True
        else:
            logger.warning(f"å¼ºåˆ¶åˆ é™¤ä»»åŠ¡å¤±è´¥ï¼Œä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
            return False
    except Exception as e:
        logger.error(f"å¼ºåˆ¶åˆ é™¤ä»»åŠ¡ {task_id} å¤±è´¥: {e}", exc_info=True)
        await session.rollback()
        return False

async def force_fail_task(session: AsyncSession, task_id: str) -> bool:
    """å¼ºåˆ¶å°†ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥çŠ¶æ€"""
    try:
        logger.info(f"å¼ºåˆ¶æ ‡è®°ä»»åŠ¡ä¸ºå¤±è´¥: {task_id}")

        # ä½¿ç”¨SQLç›´æ¥æ›´æ–°ä»»åŠ¡çŠ¶æ€
        stmt = update(TaskHistory).where(TaskHistory.taskId == task_id).values(
            status="å¤±è´¥",
            finishedAt=get_now(),
            updatedAt=get_now(),
            description="ä»»åŠ¡è¢«å¼ºåˆ¶ä¸­æ­¢"
        )
        result = await session.execute(stmt)
        await session.commit()

        updated_count = result.rowcount
        if updated_count > 0:
            logger.info(f"å¼ºåˆ¶æ ‡è®°ä»»åŠ¡å¤±è´¥æˆåŠŸ: {task_id}, æ›´æ–°è¡Œæ•°: {updated_count}")
            return True
        else:
            logger.warning(f"å¼ºåˆ¶æ ‡è®°ä»»åŠ¡å¤±è´¥ï¼Œä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
            return False
    except Exception as e:
        logger.error(f"å¼ºåˆ¶æ ‡è®°ä»»åŠ¡å¤±è´¥ {task_id} å¤±è´¥: {e}", exc_info=True)
        await session.rollback()
        return False

async def get_execution_task_id_from_scheduler_task(session: AsyncSession, scheduler_task_id: str) -> tuple[Optional[str], Optional[str]]:
    """
    ä»ä¸€ä¸ªè°ƒåº¦ä»»åŠ¡çš„æœ€ç»ˆæè¿°ä¸­ï¼Œè§£æå¹¶è¿”å›å…¶è§¦å‘çš„æ‰§è¡Œä»»åŠ¡IDå’ŒçŠ¶æ€ã€‚

    Returns:
        (execution_task_id, status): æ‰§è¡Œä»»åŠ¡IDå’ŒçŠ¶æ€,å¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›(None, None)
    """
    # å…ˆæŸ¥è¯¢è°ƒåº¦ä»»åŠ¡æœ¬èº«çš„çŠ¶æ€
    stmt = select(TaskHistory.description, TaskHistory.status).where(
        TaskHistory.taskId == scheduler_task_id
    )
    result = await session.execute(stmt)
    row = result.one_or_none()

    if not row:
        return (None, None)

    description, scheduler_status = row

    # å¦‚æœè°ƒåº¦ä»»åŠ¡å¤±è´¥,ç›´æ¥è¿”å›å¤±è´¥çŠ¶æ€
    if scheduler_status == 'å¤±è´¥':
        return (None, 'å¤±è´¥')

    # å¦‚æœè°ƒåº¦ä»»åŠ¡å·²å–æ¶ˆ,ç›´æ¥è¿”å›å·²å–æ¶ˆçŠ¶æ€
    if scheduler_status == 'å·²å–æ¶ˆ':
        return (None, 'å·²å–æ¶ˆ')

    # å¦‚æœè°ƒåº¦ä»»åŠ¡è¿˜åœ¨è¿è¡Œä¸­æˆ–ç­‰å¾…ä¸­,è¿”å›å¯¹åº”çŠ¶æ€
    if scheduler_status in ['è¿è¡Œä¸­', 'ç­‰å¾…ä¸­', 'å·²æš‚åœ']:
        return (None, scheduler_status)

    # å¦‚æœè°ƒåº¦ä»»åŠ¡å·²å®Œæˆ,å°è¯•è§£ææ‰§è¡Œä»»åŠ¡ID
    if scheduler_status == 'å·²å®Œæˆ' and description:
        match = re.search(r'æ‰§è¡Œä»»åŠ¡ID:\s*([a-f0-9\-]+)', description)
        if match:
            execution_task_id = match.group(1)

            # æŸ¥è¯¢æ‰§è¡Œä»»åŠ¡çš„çŠ¶æ€
            exec_stmt = select(TaskHistory.status).where(
                TaskHistory.taskId == execution_task_id
            )
            exec_result = await session.execute(exec_stmt)
            exec_status = exec_result.scalar_one_or_none()

            return (execution_task_id, exec_status if exec_status else 'æœªçŸ¥')

    return (None, None)

async def mark_interrupted_tasks_as_failed(session: AsyncSession) -> int:
    stmt = (
        update(TaskHistory)
        .where(TaskHistory.status.in_(['è¿è¡Œä¸­', 'å·²æš‚åœ']))
        .values(status='å¤±è´¥', description='å› ç¨‹åºé‡å¯è€Œä¸­æ–­', finishedAt=get_now(), updatedAt=get_now()) # finishedAt and updatedAt are explicitly set here
    )
    result = await session.execute(stmt)
    await session.commit()
    return result.rowcount

# --- Webhook Tasks ---

async def create_webhook_task(
    session: AsyncSession,
    task_title: str,
    unique_key: str,
    payload: Dict[str, Any],
    webhook_source: str,
    is_delayed: bool,
    delay: timedelta
):
    """åˆ›å»ºä¸€ä¸ªæ–°çš„å¾…å¤„ç† Webhook ä»»åŠ¡ã€‚"""
    now = get_now()
    execute_time = now + delay if is_delayed else now

    try:
        new_task = WebhookTask(
            receptionTime=now,
            executeTime=execute_time,
            webhookSource=webhook_source,
            status="pending",
            payload=json.dumps(payload, ensure_ascii=False),
            uniqueKey=unique_key,
            taskTitle=task_title
        )
        session.add(new_task)
        await session.flush() # Flush to check for unique constraint violation
    except exc.IntegrityError:
        # å¦‚æœ uniqueKey å·²å­˜åœ¨ï¼Œåˆ™å¿½ç•¥æ­¤é‡å¤è¯·æ±‚
        await session.rollback()
        logger.warning(f"æ£€æµ‹åˆ°é‡å¤çš„ Webhook è¯·æ±‚ (unique_key: {unique_key})ï¼Œå·²å¿½ç•¥ã€‚")

async def get_webhook_tasks(session: AsyncSession, page: int, page_size: int, search: Optional[str] = None) -> Dict[str, Any]:
    """è·å–å¾…å¤„ç†çš„ Webhook ä»»åŠ¡åˆ—è¡¨ï¼Œæ”¯æŒåˆ†é¡µã€‚"""
    base_stmt = select(WebhookTask)
    if search:
        base_stmt = base_stmt.where(WebhookTask.taskTitle.like(f"%{search}%"))

    count_stmt = select(func.count()).select_from(base_stmt.alias("count_subquery"))
    total = (await session.execute(count_stmt)).scalar_one()

    stmt = base_stmt.order_by(WebhookTask.receptionTime.desc()).offset((page - 1) * page_size).limit(page_size)
    result = await session.execute(stmt)
    return {"total": total, "list": result.scalars().all()}

async def delete_webhook_tasks(session: AsyncSession, task_ids: List[int]) -> int:
    """æ‰¹é‡åˆ é™¤æŒ‡å®šçš„ Webhook ä»»åŠ¡ã€‚"""
    if not task_ids:
        return 0
    stmt = delete(WebhookTask).where(WebhookTask.id.in_(task_ids))
    result = await session.execute(stmt)
    await session.commit()
    return result.rowcount

async def get_due_webhook_tasks(session: AsyncSession) -> List[WebhookTask]:
    """è·å–æ‰€æœ‰å·²åˆ°æ‰§è¡Œæ—¶é—´çš„å¾…å¤„ç†ä»»åŠ¡ã€‚"""
    now = get_now()
    stmt = select(WebhookTask).where(WebhookTask.status == "pending", WebhookTask.executeTime <= now)
    result = await session.execute(stmt)
    return result.scalars().all()

async def update_webhook_task_status(session: AsyncSession, task_id: int, status: str):
    """æ›´æ–° Webhook ä»»åŠ¡çš„çŠ¶æ€ã€‚"""
    await session.execute(update(WebhookTask).where(WebhookTask.id == task_id).values(status=status))

async def get_last_run_result_for_scheduled_task(session: AsyncSession, scheduled_task_id: str) -> Optional[Dict[str, Any]]:
    """è·å–æŒ‡å®šå®šæ—¶ä»»åŠ¡çš„æœ€è¿‘ä¸€æ¬¡è¿è¡Œç»“æœã€‚"""
    stmt = (
        select(TaskHistory)
        .where(TaskHistory.scheduledTaskId == scheduled_task_id)
        .order_by(TaskHistory.createdAt.desc())
        .limit(1)
    )
    result = await session.execute(stmt)
    task_run = result.scalar_one_or_none()
    if not task_run:
        return None
    
    # è¿”å›ä¸€ä¸ªä¸ models.TaskInfo å…¼å®¹çš„å­—å…¸
    return {
        "taskId": task_run.taskId,
        "title": task_run.title,
        "status": task_run.status,
        "progress": task_run.progress,
        "description": task_run.description,
        "createdAt": task_run.createdAt,
        "updatedAt": task_run.updatedAt,
        "finishedAt": task_run.finishedAt,
    }

# --- External API Logging ---

async def create_external_api_log(session: AsyncSession, ip_address: str, endpoint: str, status_code: int, message: Optional[str] = None):
    """åˆ›å»ºä¸€ä¸ªå¤–éƒ¨APIè®¿é—®æ—¥å¿—ã€‚"""
    new_log = ExternalApiLog(
        accessTime=get_now(),
        ipAddress=ip_address,
        endpoint=endpoint,
        statusCode=status_code,
        message=message
    )
    session.add(new_log)
    await session.commit()

async def get_external_api_logs(session: AsyncSession, limit: int = 100) -> List[ExternalApiLog]:
    stmt = select(ExternalApiLog).order_by(ExternalApiLog.accessTime.desc()).limit(limit)
    result = await session.execute(stmt)
    return result.scalars().all()

async def initialize_configs(session: AsyncSession, defaults: Dict[str, tuple[Any, str]]):
    if not defaults: return
    
    existing_stmt = select(Config.configKey)
    existing_keys = set((await session.execute(existing_stmt)).scalars().all())
    
    new_configs = [
        Config(configKey=key, configValue=str(value), description=description)
        for key, (value, description) in defaults.items()
        if key not in existing_keys
    ]
    if new_configs:
        session.add_all(new_configs)
        await session.commit()
        logging.getLogger(__name__).info(f"æˆåŠŸåˆå§‹åŒ– {len(new_configs)} ä¸ªæ–°é…ç½®é¡¹ã€‚")
    logging.getLogger(__name__).info("é»˜è®¤é…ç½®æ£€æŸ¥å®Œæˆã€‚")

# --- Rate Limiter CRUD ---

async def find_recent_task_by_unique_key(session: AsyncSession, unique_key: str, within_hours: int) -> Optional[TaskHistory]:
    """
    Finds a task by its unique_key that is either currently active 
    or was completed within the specified time window.
    """
    if not unique_key:
        return None

    cutoff_time = get_now() - timedelta(hours=within_hours)
    
    stmt = (
        select(TaskHistory)
        .where(
            TaskHistory.uniqueKey == unique_key,
            or_(
                TaskHistory.status.in_(['æ’é˜Ÿä¸­', 'è¿è¡Œä¸­', 'å·²æš‚åœ']),
                TaskHistory.finishedAt >= cutoff_time
            )
        )
        .order_by(TaskHistory.createdAt.desc())
        .limit(1)
    )
    result = await session.execute(stmt)
    return result.scalar_one_or_none()

async def get_or_create_rate_limit_state(session: AsyncSession, provider_name: str) -> RateLimitState:
    """è·å–æˆ–åˆ›å»ºç‰¹å®šæä¾›å•†çš„é€Ÿç‡é™åˆ¶çŠ¶æ€ã€‚"""
    stmt = select(RateLimitState).where(RateLimitState.providerName == provider_name)
    result = await session.execute(stmt)
    state = result.scalar_one_or_none()
    if not state:
        state = RateLimitState(
            providerName=provider_name,
            requestCount=0,
            lastResetTime=get_now() # lastResetTime is explicitly set here
        )
        session.add(state)
        await session.flush()

    # å…³é”®ä¿®å¤ï¼šæ— è®ºæ•°æ®æ¥è‡ªæ•°æ®åº“è¿˜æ˜¯æ–°åˆ›å»ºï¼Œéƒ½ç¡®ä¿è¿”å›çš„æ—¶é—´æ˜¯ naive çš„ã€‚
    # è¿™å¯ä»¥è§£å†³ PostgreSQL é©±åŠ¨è¿”å›å¸¦æ—¶åŒºæ—¶é—´å¯¹è±¡çš„é—®é¢˜ã€‚
    if state.lastResetTime and state.lastResetTime.tzinfo:
        state.lastResetTime = state.lastResetTime.replace(tzinfo=None)

    return state

async def get_all_rate_limit_states(session: AsyncSession) -> List[RateLimitState]:
    """è·å–æ‰€æœ‰é€Ÿç‡é™åˆ¶çŠ¶æ€ã€‚"""
    result = await session.execute(select(RateLimitState))
    states = result.scalars().all()
    return states

async def reset_all_rate_limit_states(session: AsyncSession):
    """
    é‡ç½®æ‰€æœ‰é€Ÿç‡é™åˆ¶çŠ¶æ€çš„è¯·æ±‚è®¡æ•°å’Œé‡ç½®æ—¶é—´ã€‚
    """
    # ä¿®æ­£ï¼šä»æ‰¹é‡æ›´æ–°æ”¹ä¸ºè·å–å¹¶æ›´æ–°å¯¹è±¡ã€‚
    # è¿™ç¡®ä¿äº†ä¼šè¯ä¸­å·²åŠ è½½çš„ORMå¯¹è±¡çš„çŠ¶æ€èƒ½ä¸æ•°æ®åº“åŒæ­¥ï¼Œ
    # è§£å†³äº†åœ¨ expire_on_commit=False çš„æƒ…å†µä¸‹ï¼Œå¯¹è±¡çŠ¶æ€é™ˆæ—§çš„é—®é¢˜ã€‚
    states = (await session.execute(select(RateLimitState))).scalars().all()
    now_naive = get_now()
    for state in states:
        state.requestCount = 0
        state.lastResetTime = now_naive
    # The commit will be handled by the calling function (e.g., RateLimiter.check)

async def increment_rate_limit_count(session: AsyncSession, provider_name: str):
    """ä¸ºæŒ‡å®šçš„æä¾›å•†å¢åŠ è¯·æ±‚è®¡æ•°ã€‚å¦‚æœçŠ¶æ€ä¸å­˜åœ¨ï¼Œåˆ™ä¼šåˆ›å»ºå®ƒã€‚"""
    state = await get_or_create_rate_limit_state(session, provider_name)
    state.requestCount += 1

# --- Database Maintenance ---

async def prune_logs(session: AsyncSession, model: type[DeclarativeBase], date_column: ColumnElement, cutoff_date: datetime) -> int:
    """é€šç”¨å‡½æ•°ï¼Œç”¨äºåˆ é™¤æŒ‡å®šæ¨¡å‹ä¸­æ—©äºæˆªæ­¢æ—¥æœŸçš„è®°å½•ã€‚"""
    stmt = delete(model).where(date_column < cutoff_date)
    result = await session.execute(stmt)
    # æäº¤ç”±è°ƒç”¨æ–¹ï¼ˆä»»åŠ¡ï¼‰å¤„ç†
    return result.rowcount

async def add_comments_from_xml(session: AsyncSession, episode_id: int, xml_content: str) -> int:
    """
    Parses XML content and adds the comments to a given episode.
    Returns the number of comments added.
    """
    comments = parse_dandan_xml_to_comments(xml_content)
    if not comments:
        return 0
    
    added_count = await save_danmaku_for_episode(session, episode_id, comments)
    await session.commit()
    
    return added_count

async def get_existing_episodes_for_source(
    session: AsyncSession,
    provider: str,
    media_id: str
) -> List[orm_models.Episode]:
    """è·å–æŒ‡å®šæ•°æ®æºçš„æ‰€æœ‰å·²å­˜åœ¨åˆ†é›†"""
    # å…ˆæ‰¾åˆ°å¯¹åº”çš„æº
    source_stmt = select(orm_models.Source).where(
        orm_models.Source.providerName == provider,
        orm_models.Source.mediaId == media_id
    )
    source_result = await session.execute(source_stmt)
    source = source_result.scalar_one_or_none()
    
    if not source:
        return []
    
    # è·å–è¯¥æºçš„æ‰€æœ‰åˆ†é›†
    episodes_stmt = select(orm_models.Episode).where(
        orm_models.Episode.sourceId == source.id
    )
    episodes_result = await session.execute(episodes_stmt)
    return episodes_result.scalars().all()

# ==================== ä»»åŠ¡çŠ¶æ€ç¼“å­˜ç›¸å…³å‡½æ•° ====================

async def save_task_state_cache(session: AsyncSession, task_id: str, task_type: str, task_parameters: str):
    """ä¿å­˜ä»»åŠ¡çŠ¶æ€åˆ°ç¼“å­˜è¡¨"""
    now = get_now()

    # ä½¿ç”¨ merge æ¥å¤„ç†æ’å…¥æˆ–æ›´æ–°
    task_state = orm_models.TaskStateCache(
        taskId=task_id,
        taskType=task_type,
        taskParameters=task_parameters,
        createdAt=now,
        updatedAt=now
    )

    await session.merge(task_state)
    await session.commit()

async def get_task_state_cache(session: AsyncSession, task_id: str) -> Optional[Dict[str, Any]]:
    """è·å–ä»»åŠ¡çŠ¶æ€ç¼“å­˜"""
    result = await session.execute(
        select(orm_models.TaskStateCache).where(orm_models.TaskStateCache.taskId == task_id)
    )
    task_state = result.scalar_one_or_none()

    if task_state:
        return {
            "taskId": task_state.taskId,
            "taskType": task_state.taskType,
            "taskParameters": task_state.taskParameters,
            "createdAt": task_state.createdAt,
            "updatedAt": task_state.updatedAt
        }
    return None

async def clear_task_state_cache(session: AsyncSession, task_id: str):
    """æ¸…ç†ä»»åŠ¡çŠ¶æ€ç¼“å­˜"""
    await session.execute(
        delete(orm_models.TaskStateCache).where(orm_models.TaskStateCache.taskId == task_id)
    )
    await session.commit()

async def get_all_running_task_states(session: AsyncSession) -> List[Dict[str, Any]]:
    """è·å–æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡çŠ¶æ€ç¼“å­˜ï¼Œç”¨äºæœåŠ¡é‡å¯åçš„ä»»åŠ¡æ¢å¤"""
    # ä½¿ç”¨CASTå¼ºåˆ¶å­—ç¬¦é›†ä¸€è‡´ï¼Œè§£å†³å­—ç¬¦é›†å†²çªé—®é¢˜
    result = await session.execute(
        select(orm_models.TaskStateCache, orm_models.TaskHistory)
        .join(orm_models.TaskHistory,
              func.cast(orm_models.TaskStateCache.taskId, String) ==
              func.cast(orm_models.TaskHistory.taskId, String))
        .where(orm_models.TaskHistory.status == "è¿è¡Œä¸­")
    )

    task_states = []
    for task_state, task_history in result.all():
        task_states.append({
            "taskId": task_state.taskId,
            "taskType": task_state.taskType,
            "taskParameters": task_state.taskParameters,
            "createdAt": task_state.createdAt,
            "updatedAt": task_state.updatedAt,
            "taskTitle": task_history.title,
            "taskProgress": task_history.progress,
            "taskDescription": task_history.description
        })

    return task_states

async def mark_interrupted_tasks_as_failed(session: AsyncSession) -> int:
    """å°†æ‰€æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥ï¼ˆç”¨äºæœåŠ¡é‡å¯æ—¶ï¼‰"""
    now = get_now()

    # æ›´æ–°æ‰€æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡ä¸ºå¤±è´¥çŠ¶æ€
    result = await session.execute(
        update(orm_models.TaskHistory)
        .where(orm_models.TaskHistory.status == "è¿è¡Œä¸­")
        .values(
            status="å¤±è´¥",
            description="æœåŠ¡å¼‚å¸¸ä¸­æ–­ï¼Œä»»åŠ¡è¢«æ ‡è®°ä¸ºå¤±è´¥",
            finishedAt=now,
            updatedAt=now
        )
    )

    interrupted_count = result.rowcount

    # æ¸…ç†æ‰€æœ‰ä»»åŠ¡çŠ¶æ€ç¼“å­˜
    await session.execute(delete(orm_models.TaskStateCache))

    await session.commit()
    return interrupted_count
