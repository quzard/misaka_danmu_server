"""
å¤–éƒ¨æ§åˆ¶API - å¯¼å…¥ç›¸å…³è·¯ç”±
åŒ…å«: /import/auto, /import/direct, /import/edited, /import/xml, /import/url, /episodes
"""

import logging
import uuid
import hashlib

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, Request, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from thefuzz import fuzz

from src.db import crud, models, orm_models, get_db_session, ConfigManager
from src import tasks
from src.utils import common as utils
from src.core import get_now
from src.services import ScraperManager, TaskManager, MetadataSourceManager, unified_search, convert_to_chinese_title
from src.utils import (
    SearchTimer, SEARCH_TYPE_CONTROL_SEARCH, SubStepTiming,
    ai_type_and_season_mapping_and_correction
)
from src.rate_limiter import RateLimiter
from src.ai import AIMatcherManager

from .models import (
    AutoImportSearchType, AutoImportMediaType,
    ControlTaskResponse, ControlSearchResponse, ControlSearchResultItem,
    ControlAutoImportRequest, ControlDirectImportRequest,
    ControlEditedImportRequest, ControlXmlImportRequest, ControlUrlImportRequest
)
from .dependencies import (
    verify_api_key, get_scraper_manager, get_metadata_manager,
    get_task_manager, get_config_manager, get_rate_limiter,
    get_ai_matcher_manager, get_title_recognition_manager,
    _normalize_for_filtering, _is_movie_by_title
)

logger = logging.getLogger(__name__)

router = APIRouter()


@router.post("/import/auto", status_code=status.HTTP_202_ACCEPTED, summary="å…¨è‡ªåŠ¨æœç´¢å¹¶å¯¼å…¥", response_model=ControlTaskResponse)
async def auto_import(
    request: Request,
    searchType: AutoImportSearchType = Query(..., description="æœç´¢ç±»å‹ã€‚å¯é€‰å€¼: 'keyword', 'tmdb', 'tvdb', 'douban', 'imdb', 'bangumi'ã€‚"),
    searchTerm: str = Query(..., description="æœç´¢å†…å®¹ã€‚æ ¹æ® searchType çš„ä¸åŒï¼Œè¿™é‡Œåº”å¡«å…¥å…³é”®è¯æˆ–å¯¹åº”çš„å¹³å°IDã€‚"),
    season: int | None = Query(None, description="å­£åº¦å·ã€‚å¦‚æœæœªæä¾›ï¼Œå°†è‡ªåŠ¨æ¨æ–­æˆ–é»˜è®¤ä¸º1ã€‚"),
    episode: str | None = Query(None, description="é›†æ•°ã€‚æ”¯æŒå•é›†(å¦‚'1')æˆ–å¤šé›†(å¦‚'1,3,5,7,9,11-13')æ ¼å¼ã€‚å¦‚æœæä¾›ï¼Œå°†åªå¯¼å…¥æŒ‡å®šé›†æ•°ï¼ˆæ­¤æ—¶å¿…é¡»æä¾›å­£åº¦ï¼‰ã€‚"),
    mediaType: AutoImportMediaType | None = Query(None, description="åª’ä½“ç±»å‹ã€‚å½“ searchType ä¸º 'keyword' æ—¶å¿…å¡«ã€‚å¦‚æœç•™ç©ºï¼Œå°†æ ¹æ®æœ‰æ—  'season' å‚æ•°è‡ªåŠ¨æ¨æ–­ã€‚"),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter),
    config_manager: ConfigManager = Depends(get_config_manager),
    ai_matcher_manager: AIMatcherManager = Depends(get_ai_matcher_manager),
    title_recognition_manager = Depends(get_title_recognition_manager),
    api_key: str = Depends(verify_api_key)
):
    """
    ### åŠŸèƒ½
    è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„"å…¨è‡ªåŠ¨æœç´¢å¹¶å¯¼å…¥"æ¥å£ï¼Œå®ƒèƒ½æ ¹æ®ä¸åŒçš„IDç±»å‹ï¼ˆå¦‚TMDB IDã€Bangumi IDç­‰ï¼‰æˆ–å…³é”®è¯è¿›è¡Œæœç´¢ï¼Œå¹¶æ ¹æ®ä¸€ç³»åˆ—æ™ºèƒ½è§„åˆ™è‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„æ•°æ®æºè¿›è¡Œå¼¹å¹•å¯¼å…¥ã€‚

    ### å·¥ä½œæµç¨‹
    1.  **å…ƒæ•°æ®è·å–**: å¦‚æœä½¿ç”¨IDæœç´¢ï¼ˆå¦‚`tmdb`, `bangumi`ï¼‰ï¼Œæ¥å£ä¼šé¦–å…ˆä»å¯¹åº”çš„å…ƒæ•°æ®ç½‘ç«™è·å–ä½œå“çš„å®˜æ–¹æ ‡é¢˜å’Œåˆ«åã€‚
    2.  **åª’ä½“åº“æ£€æŸ¥**: æ£€æŸ¥æ­¤ä½œå“æ˜¯å¦å·²å­˜åœ¨äºæ‚¨çš„å¼¹å¹•åº“ä¸­ã€‚
        -   å¦‚æœå­˜åœ¨ä¸”æœ‰ç²¾ç¡®æ ‡è®°çš„æºï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨è¯¥æºã€‚
        -   å¦‚æœå­˜åœ¨ä½†æ— ç²¾ç¡®æ ‡è®°ï¼Œåˆ™ä½¿ç”¨å·²æœ‰å…³è”æºä¸­ä¼˜å…ˆçº§æœ€é«˜çš„é‚£ä¸ªã€‚
    3.  **å…¨ç½‘æœç´¢**: å¦‚æœåª’ä½“åº“ä¸­ä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨è·å–åˆ°çš„æ ‡é¢˜å’Œåˆ«ååœ¨æ‰€æœ‰å·²å¯ç”¨çš„å¼¹å¹•æºä¸­è¿›è¡Œæœç´¢ã€‚
    4.  **æ™ºèƒ½é€‰æ‹©**: ä»æœç´¢ç»“æœä¸­ï¼Œæ ¹æ®æ‚¨åœ¨"æœç´¢æº"é¡µé¢è®¾ç½®çš„ä¼˜å…ˆçº§ï¼Œé€‰æ‹©æœ€ä½³åŒ¹é…é¡¹ã€‚
    5.  **ä»»åŠ¡æäº¤**: ä¸ºæœ€ç»ˆé€‰æ‹©çš„æºåˆ›å»ºä¸€ä¸ªåå°å¯¼å…¥ä»»åŠ¡ã€‚

    ### å‚æ•°ä½¿ç”¨è¯´æ˜
    -   `searchType`:
        -   `keyword`: æŒ‰å…³é”®è¯æœç´¢ã€‚æ­¤æ—¶ `mediaType` å­—æ®µ**å¿…å¡«**ã€‚
        -   `tmdb`, `tvdb`, `douban`, `imdb`, `bangumi`: æŒ‰å¯¹åº”å¹³å°çš„IDè¿›è¡Œç²¾ç¡®æœç´¢ã€‚
    -   `season` & `episode`:
        -   **ç”µè§†å‰§/ç•ªå‰§**:
            -   æä¾› `season`ï¼Œä¸æä¾› `episode`: å¯¼å…¥ `season` æŒ‡å®šçš„æ•´å­£ã€‚
            -   æä¾› `season` å’Œ `episode`: å¯¼å…¥æŒ‡å®šçš„é›†æ•°ã€‚æ”¯æŒå•é›†(å¦‚'1')æˆ–å¤šé›†(å¦‚'1,3,5,7,9,11-13')æ ¼å¼ã€‚
        -   **ç”µå½±**:
            -   `season` å’Œ `episode` å‚æ•°ä¼šè¢«å¿½ç•¥ã€‚
    -   `mediaType`:
        -   å½“ `searchType` ä¸º `keyword` æ—¶ï¼Œæ­¤å­—æ®µä¸º**å¿…å¡«é¡¹**ã€‚
        -   å½“ `searchType` ä¸ºå…¶ä»–IDç±»å‹æ—¶ï¼Œæ­¤å­—æ®µä¸ºå¯é€‰é¡¹ã€‚å¦‚æœç•™ç©ºï¼Œç³»ç»Ÿå°†æ ¹æ® `season` å‚æ•°æ˜¯å¦å­˜åœ¨æ¥è‡ªåŠ¨æ¨æ–­åª’ä½“ç±»å‹ï¼ˆæœ‰ `season` åˆ™ä¸ºç”µè§†å‰§ï¼Œå¦åˆ™ä¸ºç”µå½±ï¼‰ã€‚
    """
    if episode is not None and season is None:
        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail="å½“æä¾› 'episode' å‚æ•°æ—¶ï¼Œ'season' å‚æ•°ä¹Ÿå¿…é¡»æä¾›ã€‚")

    payload = ControlAutoImportRequest(
        searchType=searchType,
        searchTerm=searchTerm,
        season=season,
        episode=episode,
        mediaType=mediaType
    )

    # ä¿®æ­£ï¼šä¸å†å¼ºåˆ¶å°†éå…³é”®è¯æœç´¢çš„ mediaType è®¾ä¸º Noneã€‚
    # å…è®¸ç”¨æˆ·åœ¨è°ƒç”¨ TMDB ç­‰IDæœç´¢æ—¶ï¼Œé¢„å…ˆæŒ‡å®šåª’ä½“ç±»å‹ï¼Œä»¥é¿å…é”™è¯¯çš„ç±»å‹æ¨æ–­ã€‚
    if payload.searchType == AutoImportSearchType.KEYWORD and not payload.mediaType:
        raise HTTPException(status_code=400, detail="ä½¿ç”¨ keyword æœç´¢æ—¶ï¼ŒmediaType å­—æ®µæ˜¯å¿…éœ€çš„ã€‚")

    # æ–°å¢ï¼šå¦‚æœä¸æ˜¯å…³é”®è¯æœç´¢ï¼Œåˆ™æ£€æŸ¥æ‰€é€‰çš„å…ƒæ•°æ®æºæ˜¯å¦å·²å¯ç”¨
    if payload.searchType != AutoImportSearchType.KEYWORD:
        provider_name = payload.searchType.value
        provider_setting = metadata_manager.source_settings.get(provider_name)

        if not provider_setting or not provider_setting.get('isEnabled'):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"å…ƒä¿¡æ¯æœç´¢æº '{provider_name}' æœªå¯ç”¨ã€‚è¯·åœ¨'è®¾ç½®-å…ƒä¿¡æ¯æœç´¢æº'é¡µé¢ä¸­å¯ç”¨å®ƒã€‚"
            )

    if not await manager.acquire_search_lock(api_key):
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail="å·²æœ‰æœç´¢æˆ–è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚"
        )

    # ä¿®æ­£ï¼šå°† seasonã€episode å’Œ mediaType çº³å…¥ unique_keyï¼Œä»¥å…è®¸åŒä¸€ä½œå“ä¸åŒå­£/é›†çš„å¯¼å…¥
    unique_key_parts = [payload.searchType.value, payload.searchTerm]
    if payload.season is not None:
        unique_key_parts.append(f"s{payload.season}")
    if payload.episode is not None:
        # å¯¹äºå¤šé›†æ ¼å¼ï¼Œä¿ç•™åŸå§‹å­—ç¬¦ä¸²ä½œä¸º unique_key çš„ä¸€éƒ¨åˆ†
        unique_key_parts.append(f"e{payload.episode}")
    # å§‹ç»ˆåŒ…å« mediaType ä»¥åŒºåˆ†åŒåä½†ä¸åŒç±»å‹çš„ä½œå“ï¼Œé¿å…é‡å¤ä»»åŠ¡æ£€æµ‹é—®é¢˜
    if payload.mediaType is not None:
        unique_key_parts.append(payload.mediaType.value)
    unique_key = f"auto-import-{'-'.join(unique_key_parts)}"

    # æ–°å¢ï¼šæ£€æŸ¥æœ€è¿‘æ˜¯å¦æœ‰é‡å¤ä»»åŠ¡
    config_manager_local = get_config_manager(request)
    threshold_hours_str = await config_manager_local.get("externalApiDuplicateTaskThresholdHours", "3")
    try:
        threshold_hours = int(threshold_hours_str)
    except (ValueError, TypeError):
        threshold_hours = 3

    if threshold_hours > 0:
        session_factory = request.app.state.db_session_factory
        async with session_factory() as session:
            recent_task = await crud.find_recent_task_by_unique_key(session, unique_key, threshold_hours)
            if recent_task:
                time_since_creation = get_now() - recent_task.createdAt
                hours_ago = time_since_creation.total_seconds() / 3600
                # å…³é”®ä¿®å¤ï¼šæŠ›å‡ºå¼‚å¸¸å‰é‡Šæ”¾æœç´¢é”
                await manager.release_search_lock(api_key)
                raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=f"ä¸€ä¸ªç›¸ä¼¼çš„ä»»åŠ¡åœ¨ {hours_ago:.1f} å°æ—¶å‰å·²è¢«æäº¤ (çŠ¶æ€: {recent_task.status})ã€‚è¯·åœ¨ {threshold_hours} å°æ—¶åé‡è¯•ã€‚")

            # å…³é”®ä¿®å¤ï¼šå¤–éƒ¨APIä¹Ÿåº”è¯¥æ£€æŸ¥åº“å†…æ˜¯å¦å·²å­˜åœ¨ç›¸åŒä½œå“
            # ä½¿ç”¨ä¸WebUIç›¸åŒçš„æ£€æŸ¥é€»è¾‘ï¼Œé€šè¿‡æ ‡é¢˜+å­£åº¦+é›†æ•°è¿›è¡Œæ£€æŸ¥
            title_recognition_manager_local = get_title_recognition_manager(request)

            # æ£€æŸ¥ä½œå“æ˜¯å¦å·²å­˜åœ¨äºåº“å†…
            existing_anime = await crud.find_anime_by_title_season_year(
                session, searchTerm, season, None, title_recognition_manager_local, None  # sourceå‚æ•°æš‚æ—¶ä¸ºNoneï¼Œå› ä¸ºè¿™é‡Œæ˜¯æŸ¥æ‰¾ç°æœ‰æ¡ç›®
            )

            if existing_anime and episode is not None:
                # å¯¹äºå•é›†/å¤šé›†å¯¼å…¥ï¼Œæ£€æŸ¥å…·ä½“é›†æ•°æ˜¯å¦å·²å­˜åœ¨ï¼ˆéœ€è¦è€ƒè™‘è¯†åˆ«è¯è½¬æ¢ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œä¸å†æ‹’ç»è¯·æ±‚ï¼Œè€Œæ˜¯åœ¨ä»»åŠ¡æ‰§è¡Œæ—¶è·³è¿‡å·²å­˜åœ¨çš„é›†æ•°
                pass
            elif existing_anime and episode is None:
                # å¯¹äºæ•´å­£å¯¼å…¥ï¼Œå¦‚æœä½œå“å·²å­˜åœ¨åˆ™æ‹’ç»
                # å…³é”®ä¿®å¤ï¼šæŠ›å‡ºå¼‚å¸¸å‰é‡Šæ”¾æœç´¢é”
                await manager.release_search_lock(api_key)
                raise HTTPException(
                    status_code=status.HTTP_409_CONFLICT,
                    detail=f"ä½œå“ '{searchTerm}' å·²åœ¨åª’ä½“åº“ä¸­ï¼Œæ— éœ€é‡å¤å¯¼å…¥æ•´å­£"
                )

    # ä¿®æ­£ï¼šä¸ºä»»åŠ¡æ ‡é¢˜æ·»åŠ å­£/é›†ä¿¡æ¯ï¼Œä»¥ç¡®ä¿å…¶å”¯ä¸€æ€§ï¼Œé˜²æ­¢å› ä»»åŠ¡åé‡å¤è€Œæäº¤å¤±è´¥ã€‚
    title_parts = [f"å¤–éƒ¨APIè‡ªåŠ¨å¯¼å…¥: {payload.searchTerm} (ç±»å‹: {payload.searchType})"]
    if payload.season is not None:
        title_parts.append(f"S{payload.season:02d}")
    if payload.episode is not None:
        # å¯¹äºå¤šé›†æ ¼å¼ï¼Œç›´æ¥æ˜¾ç¤ºåŸå§‹å­—ç¬¦ä¸²
        title_parts.append(f"E{payload.episode}")
    task_title = " ".join(title_parts)

    try:
        task_coro = lambda session, cb: tasks.auto_search_and_import_task(
            payload, cb, session, config_manager, manager, metadata_manager, task_manager,
            ai_matcher_manager=ai_matcher_manager,
            rate_limiter=rate_limiter,
            title_recognition_manager=title_recognition_manager,
            api_key=api_key
        )
        task_id, _ = await task_manager.submit_task(task_coro, task_title, unique_key=unique_key)
        # æ³¨æ„: æœç´¢é”ç”±ä»»åŠ¡å†…éƒ¨çš„ finally å—è´Ÿè´£é‡Šæ”¾,ç¡®ä¿ä»»åŠ¡å®Œæˆåæ‰é‡Šæ”¾
        return {"message": "è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except HTTPException as e:
        # æ•è·å·²çŸ¥çš„å†²çªé”™è¯¯å¹¶é‡æ–°æŠ›å‡º
        # å¦‚æœä»»åŠ¡æäº¤å¤±è´¥,éœ€è¦é‡Šæ”¾é”
        await manager.release_search_lock(api_key)
        raise e
    except Exception as e:
        # æ•è·ä»»ä½•åœ¨ä»»åŠ¡æäº¤é˜¶æ®µå‘ç”Ÿçš„å¼‚å¸¸,å¹¶ç¡®ä¿é‡Šæ”¾é”
        logger.error(f"æäº¤è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        await manager.release_search_lock(api_key)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æäº¤ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")




@router.get("/search", response_model=ControlSearchResponse, summary="æœç´¢åª’ä½“")
async def search_media(
    keyword: str,
    season: int | None = Query(None, description="è¦æœç´¢çš„å­£åº¦ (å¯é€‰)"),
    episode: int | None = Query(None, description="è¦æœç´¢çš„é›†æ•° (å¯é€‰)"),
    session: AsyncSession = Depends(get_db_session),
    manager: ScraperManager = Depends(get_scraper_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    config_manager: ConfigManager = Depends(get_config_manager),
    ai_matcher_manager: AIMatcherManager = Depends(get_ai_matcher_manager),
    api_key: str = Depends(verify_api_key)
):
    """
    ### åŠŸèƒ½
    æ ¹æ®å…³é”®è¯ä»æ‰€æœ‰å¯ç”¨çš„å¼¹å¹•æºæœç´¢åª’ä½“ã€‚è¿™æ˜¯æ‰§è¡Œå¯¼å…¥æ“ä½œçš„ç¬¬ä¸€æ­¥ã€‚

    ### å·¥ä½œæµç¨‹
    1.  æ¥æ”¶å…³é”®è¯ï¼Œä»¥åŠå¯é€‰çš„å­£åº¦å’Œé›†æ•°ã€‚
    2.  å¹¶å‘åœ°åœ¨æ‰€æœ‰å·²å¯ç”¨çš„å¼¹å¹•æºä¸Šè¿›è¡Œæœç´¢ã€‚
    3.  è¿”å›ä¸€ä¸ªåŒ…å«`searchId`å’Œç»“æœåˆ—è¡¨çš„å“åº”ã€‚`searchId`æ˜¯æœ¬æ¬¡æœç´¢çš„å”¯ä¸€æ ‡è¯†ï¼Œç”¨äºåç»­çš„å¯¼å…¥æ“ä½œã€‚
    4.  æœç´¢ç»“æœä¼šåœ¨æœåŠ¡å™¨ä¸Šç¼“å­˜10åˆ†é’Ÿã€‚

    ### å‚æ•°ä½¿ç”¨è¯´æ˜
    -   `keyword`: å¿…éœ€çš„æœç´¢å…³é”®è¯ã€‚
    -   `season`: (å¯é€‰) å¦‚æœæä¾›ï¼Œæœç´¢å°†æ›´å€¾å‘äºæˆ–åªè¿”å›ç”µè§†å‰§ç±»å‹çš„ç»“æœã€‚
    -   `episode`: (å¯é€‰) å¿…é¡»ä¸`season`ä¸€åŒæä¾›ï¼Œç”¨äºæ›´ç²¾ç¡®çš„å•é›†åŒ¹é…ã€‚
    """
    if episode is not None and season is None:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="æŒ‡å®šé›†æ•°æ—¶å¿…é¡»åŒæ—¶æä¾›å­£åº¦ä¿¡æ¯ã€‚"
        )

    if not await manager.acquire_search_lock(api_key):
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail="å·²æœ‰æœç´¢æˆ–è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚"
        )

    # åˆå§‹åŒ–è®¡æ—¶å™¨å¹¶å¼€å§‹è®¡æ—¶
    timer = SearchTimer(SEARCH_TYPE_CONTROL_SEARCH, keyword, logger)
    timer.start()

    try:
        timer.step_start("å…³é”®è¯è§£æ")
        # --- Start of new logic, copied and adapted from ui_api.py ---
        parsed_keyword = utils.parse_search_keyword(keyword)
        search_title = parsed_keyword["title"]
        original_title = search_title  # ä¿å­˜åŸå§‹æ ‡é¢˜ç”¨äºæ—¥å¿—
        # Prioritize explicit query params over parsed ones
        final_season = season if season is not None else parsed_keyword.get("season")
        final_episode = episode if episode is not None else parsed_keyword.get("episode")

        episode_info = {"season": final_season, "episode": final_episode} if final_season is not None or final_episode is not None else None

        # Create a dummy user for metadata calls, as this API is not user-specific
        user = models.User(id=0, username="control_api")

        logger.info(f"Control API æ­£åœ¨æœç´¢: '{keyword}' (è§£æä¸º: title='{search_title}', season={final_season}, episode={final_episode})")
        timer.step_end()

        # ğŸš€ åç§°è½¬æ¢åŠŸèƒ½ - æ£€æµ‹éä¸­æ–‡æ ‡é¢˜å¹¶å°è¯•è½¬æ¢ä¸ºä¸­æ–‡ï¼ˆåœ¨æ‰€æœ‰å¤„ç†ä¹‹å‰æ‰§è¡Œï¼‰
        timer.step_start("åç§°è½¬æ¢")
        converted_title, conversion_applied = await convert_to_chinese_title(
            search_title,
            config_manager,
            metadata_manager,
            ai_matcher_manager,
            user
        )
        if conversion_applied:
            logger.info(f"âœ“ Control API åç§°è½¬æ¢: '{original_title}' â†’ '{converted_title}'")
            search_title = converted_title
        else:
            logger.info(f"â—‹ Control API åç§°è½¬æ¢æœªç”Ÿæ•ˆ: '{original_title}'")
        timer.step_end()

        if not manager.has_enabled_scrapers:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="æ²¡æœ‰å¯ç”¨çš„å¼¹å¹•æœç´¢æºï¼Œè¯·åœ¨'æœç´¢æº'é¡µé¢ä¸­å¯ç”¨è‡³å°‘ä¸€ä¸ªã€‚"
            )

        # å¤–éƒ¨æ§åˆ¶æœç´¢ AIæ˜ å°„é…ç½®æ£€æŸ¥
        external_search_season_mapping_enabled = await config_manager.get("externalSearchEnableTmdbSeasonMapping", "false")
        if external_search_season_mapping_enabled.lower() != "true":
            logger.info("â—‹ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ ç»Ÿä¸€AIæ˜ å°„: åŠŸèƒ½æœªå¯ç”¨")

        timer.step_start("å¼¹å¹•æºæœç´¢")
        # ä½¿ç”¨ç»Ÿä¸€çš„æœç´¢å‡½æ•°ï¼ˆä¸è¿›è¡Œæ’åºï¼Œåé¢è‡ªå·±å¤„ç†ï¼‰
        results = await unified_search(
            search_term=search_title,
            session=session,
            scraper_manager=manager,
            metadata_manager=metadata_manager,
            use_alias_expansion=True,
            use_alias_filtering=True,
            use_title_filtering=True,
            use_source_priority_sorting=False,  # ä¸æ’åºï¼Œåé¢è‡ªå·±å¤„ç†
            progress_callback=None
        )
        # æ”¶é›†å•æºæœç´¢è€—æ—¶ä¿¡æ¯
        source_timing_sub_steps = [
            SubStepTiming(name=name, duration_ms=dur, result_count=cnt)
            for name, dur, cnt in manager.last_search_timing
        ]
        timer.step_end(details=f"{len(results)}ä¸ªç»“æœ", sub_steps=source_timing_sub_steps)

        logger.info(f"æœç´¢å®Œæˆï¼Œå…± {len(results)} ä¸ªç»“æœ")

        for item in results:
            if item.type == 'tv_series' and _is_movie_by_title(item.title):
                item.type = 'movie'

            # ä¿®æ­£ï¼šå¦‚æœç”¨æˆ·æŒ‡å®šäº†é›†æ•°ï¼Œåˆ™è®¾ç½® currentEpisodeIndex
            # è¿™æ ·åç»­çš„å¯¼å…¥é€»è¾‘ä¼šè‡ªåŠ¨è¯†åˆ«ä¸ºå•é›†å¯¼å…¥
            if final_episode is not None:
                item.currentEpisodeIndex = final_episode

        if final_season:
            original_count = len(results)
            filtered_by_type = [item for item in results if item.type == 'tv_series']
            results = [item for item in filtered_by_type if item.season == final_season]
            logger.info(f"æ ¹æ®æŒ‡å®šçš„å­£åº¦ ({final_season}) è¿›è¡Œè¿‡æ»¤ï¼Œä» {original_count} ä¸ªç»“æœä¸­ä¿ç•™äº† {len(results)} ä¸ªã€‚")

        source_settings = await crud.get_all_scraper_settings(session)
        source_order_map = {s['providerName']: s['displayOrder'] for s in source_settings}

        def sort_key(item: models.ProviderSearchInfo):
            return (source_order_map.get(item.provider, 999), -fuzz.token_set_ratio(keyword, item.title))

        sorted_results = sorted(results, key=sort_key)

        # ä½¿ç”¨ç»Ÿä¸€çš„AIç±»å‹å’Œå­£åº¦æ˜ å°„ä¿®æ­£å‡½æ•°
        if external_search_season_mapping_enabled.lower() == "true":
            try:
                timer.step_start("AIæ˜ å°„ä¿®æ­£")
                # è·å–AIåŒ¹é…å™¨ï¼ˆä½¿ç”¨ä¾èµ–æ³¨å…¥çš„å®ä¾‹ï¼‰
                ai_matcher = await ai_matcher_manager.get_matcher()
                if ai_matcher:
                    logger.info(f"â—‹ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ å¼€å§‹ç»Ÿä¸€AIæ˜ å°„ä¿®æ­£: '{search_title}' ({len(sorted_results)} ä¸ªç»“æœ)")

                    # ä½¿ç”¨æ–°çš„ç»Ÿä¸€å‡½æ•°è¿›è¡Œç±»å‹å’Œå­£åº¦ä¿®æ­£
                    mapping_result = await ai_type_and_season_mapping_and_correction(
                        search_title=search_title,
                        search_results=sorted_results,
                        metadata_manager=metadata_manager,
                        ai_matcher=ai_matcher,
                        logger=logger,
                        similarity_threshold=60.0
                    )

                    # åº”ç”¨ä¿®æ­£ç»“æœ
                    if mapping_result['total_corrections'] > 0:
                        logger.info(f"âœ“ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ ç»Ÿä¸€AIæ˜ å°„æˆåŠŸ: æ€»è®¡ä¿®æ­£äº† {mapping_result['total_corrections']} ä¸ªç»“æœ")
                        logger.info(f"  - ç±»å‹ä¿®æ­£: {len(mapping_result['type_corrections'])} ä¸ª")
                        logger.info(f"  - å­£åº¦ä¿®æ­£: {len(mapping_result['season_corrections'])} ä¸ª")

                        # æ›´æ–°æœç´¢ç»“æœï¼ˆå·²ç»ç›´æ¥ä¿®æ”¹äº†sorted_resultsï¼‰
                        sorted_results = mapping_result['corrected_results']
                        timer.step_end(details=f"ä¿®æ­£{mapping_result['total_corrections']}ä¸ª")
                    else:
                        logger.info(f"â—‹ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ ç»Ÿä¸€AIæ˜ å°„: æœªæ‰¾åˆ°éœ€è¦ä¿®æ­£çš„ä¿¡æ¯")
                        timer.step_end(details="æ— ä¿®æ­£")
                else:
                    logger.warning("â—‹ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ AIæ˜ å°„: AIåŒ¹é…å™¨æœªå¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥")
                    timer.step_end(details="åŒ¹é…å™¨æœªå¯ç”¨")

            except Exception as e:
                logger.warning(f"å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ ç»Ÿä¸€AIæ˜ å°„ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                timer.step_end(details=f"å¤±è´¥: {e}")
        else:
            logger.info("â—‹ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ ç»Ÿä¸€AIæ˜ å°„: åŠŸèƒ½æœªå¯ç”¨")

        timer.step_start("ç»“æœç¼“å­˜")
        search_id = str(uuid.uuid4())
        indexed_results = [ControlSearchResultItem(**r.model_dump(), resultIndex=i) for i, r in enumerate(sorted_results)]
        await crud.set_cache(session, f"control_search_{search_id}", [r.model_dump() for r in sorted_results], 600)
        timer.step_end()

        timer.finish()  # æ‰“å°è®¡æ—¶æŠ¥å‘Š
        return ControlSearchResponse(searchId=search_id, results=indexed_results)
    finally:
        await manager.release_search_lock(api_key)



@router.post("/import/direct", status_code=status.HTTP_202_ACCEPTED, summary="ç›´æ¥å¯¼å…¥æœç´¢ç»“æœ", response_model=ControlTaskResponse)
async def direct_import(
    payload: ControlDirectImportRequest,
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    config_manager: ConfigManager = Depends(get_config_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter),
    title_recognition_manager = Depends(get_title_recognition_manager)
):
    """
    ### åŠŸèƒ½
    åœ¨æ‰§è¡Œ`/search`åï¼Œä½¿ç”¨è¿”å›çš„`searchId`å’Œæ‚¨é€‰æ‹©çš„ç»“æœç´¢å¼•ï¼ˆ`resultIndex`ï¼‰æ¥ç›´æ¥å¯¼å…¥å¼¹å¹•ã€‚

    ### å·¥ä½œæµç¨‹
    è¿™æ˜¯ä¸€ä¸ªç®€å•ã€ç›´æ¥çš„å¯¼å…¥æ–¹å¼ã€‚å®ƒä¼šä¸ºé€‰å®šçš„åª’ä½“åˆ›å»ºä¸€ä¸ªåå°å¯¼å…¥ä»»åŠ¡ã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨è¯·æ±‚ä¸­é™„åŠ å…ƒæ•°æ®IDï¼ˆå¦‚`tmdbId`ï¼‰æ¥è¦†ç›–æˆ–è¡¥å……ä½œå“ä¿¡æ¯ã€‚
    """
    cache_key = f"control_search_{payload.searchId}"
    cached_results_raw = await crud.get_cache(session, cache_key)

    if cached_results_raw is None:
        raise HTTPException(status_code=404, detail="æœç´¢ä¼šè¯å·²è¿‡æœŸæˆ–æ— æ•ˆï¼Œè¯·é‡æ–°æœç´¢ã€‚")

    try:
        cached_results = [models.ProviderSearchInfo.model_validate(r) for r in cached_results_raw]
    except Exception:
        raise HTTPException(status_code=500, detail="æ— æ³•è§£æç¼“å­˜çš„æœç´¢ç»“æœã€‚")

    if not (0 <= payload.resultIndex < len(cached_results)):
        raise HTTPException(status_code=400, detail="æä¾›çš„ result_index æ— æ•ˆã€‚")

    item_to_import = cached_results[payload.resultIndex]

    # å…³é”®ä¿®å¤ï¼šæ¢å¤å¹¶å®Œå–„åœ¨ä»»åŠ¡æäº¤å‰çš„é‡å¤æ£€æŸ¥ã€‚
    # è¿™ç¡®ä¿äº†ç›´æ¥å¯¼å…¥çš„è¡Œä¸ºä¸UIå¯¼å…¥å®Œå…¨ä¸€è‡´ã€‚
    duplicate_reason = await crud.check_duplicate_import(
        session=session,
        provider=item_to_import.provider,
        media_id=item_to_import.mediaId,
        anime_title=item_to_import.title,
        media_type=item_to_import.type,
        season=item_to_import.season,
        year=item_to_import.year,
        is_single_episode=item_to_import.currentEpisodeIndex is not None,
        episode_index=item_to_import.currentEpisodeIndex,
        title_recognition_manager=title_recognition_manager
    )
    if duplicate_reason:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=duplicate_reason
        )

    # ä¿®æ­£ï¼šä¸ºä»»åŠ¡æ ‡é¢˜æ·»åŠ å­£/é›†ä¿¡æ¯ï¼Œä»¥ç¡®ä¿å…¶å”¯ä¸€æ€§ï¼Œé˜²æ­¢å› ä»»åŠ¡åé‡å¤è€Œæäº¤å¤±è´¥ã€‚
    title_parts = [f"å¤–éƒ¨APIå¯¼å…¥: {item_to_import.title} ({item_to_import.provider})"]
    if item_to_import.currentEpisodeIndex is not None and item_to_import.season is not None:
        title_parts.append(f"S{item_to_import.season:02d}E{item_to_import.currentEpisodeIndex:02d}")
    task_title = " ".join(title_parts)

    # ä¿®æ­£ï¼šä¸ºå•é›†å¯¼å…¥ä»»åŠ¡ç”Ÿæˆæ›´å…·ä½“çš„å”¯ä¸€é”®ï¼Œä»¥å…è®¸å¯¹åŒä¸€ä½œå“çš„ä¸åŒå•é›†è¿›è¡Œæ’é˜Ÿã€‚
    unique_key = f"import-{item_to_import.provider}-{item_to_import.mediaId}"
    if item_to_import.currentEpisodeIndex is not None:
        unique_key += f"-ep{item_to_import.currentEpisodeIndex}"

    try:
        task_coro = lambda session, cb: tasks.generic_import_task(
            provider=item_to_import.provider,
            mediaId=item_to_import.mediaId,
            animeTitle=item_to_import.title,
            mediaType=item_to_import.type,
            season=item_to_import.season,
            year=item_to_import.year,
            currentEpisodeIndex=item_to_import.currentEpisodeIndex,
            imageUrl=item_to_import.imageUrl,
            doubanId=payload.doubanId,
            config_manager=config_manager,
            metadata_manager=metadata_manager, tmdbId=payload.tmdbId, imdbId=payload.imdbId,
            tvdbId=payload.tvdbId, bangumiId=payload.bangumiId,
            progress_callback=cb, session=session, manager=manager, task_manager=task_manager,
            rate_limiter=rate_limiter, title_recognition_manager=title_recognition_manager
        )
        task_id, _ = await task_manager.submit_task(task_coro, task_title, unique_key=unique_key)
        return {"message": "å¯¼å…¥ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"æäº¤ç›´æ¥å¯¼å…¥ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æäº¤ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")


from typing import List

@router.get("/episodes", response_model=List[models.ProviderEpisodeInfo], summary="è·å–æœç´¢ç»“æœçš„åˆ†é›†åˆ—è¡¨")
async def get_episodes(
    searchId: str = Query(..., description="æ¥è‡ª/searchæ¥å£çš„searchId"),
    result_index: int = Query(..., ge=0, description="è¦è·å–åˆ†é›†çš„ç»“æœçš„ç´¢å¼•"),
    session: AsyncSession = Depends(get_db_session),
    manager: ScraperManager = Depends(get_scraper_manager),
):
    """
    ### åŠŸèƒ½
    åœ¨æ‰§è¡Œ`/search`åï¼Œè·å–æŒ‡å®šæœç´¢ç»“æœçš„å®Œæ•´åˆ†é›†åˆ—è¡¨ã€‚

    ### å·¥ä½œæµç¨‹
    æ­¤æ¥å£ä¸»è¦ç”¨äº"ç¼–è¾‘åå¯¼å…¥"çš„åœºæ™¯ã€‚æ‚¨å¯ä»¥å…ˆè·å–åŸå§‹çš„åˆ†é›†åˆ—è¡¨ï¼Œåœ¨æ‚¨çš„å®¢æˆ·ç«¯è¿›è¡Œä¿®æ”¹ï¼ˆä¾‹å¦‚ï¼Œåˆ é™¤é¢„å‘Šã€è°ƒæ•´é¡ºåºï¼‰ï¼Œç„¶åå†é€šè¿‡`/import/edited`æ¥å£æäº¤ä¿®æ”¹åçš„åˆ—è¡¨è¿›è¡Œå¯¼å…¥ã€‚
    """
    cache_key = f"control_search_{searchId}"
    cached_results_raw = await crud.get_cache(session, cache_key)

    if cached_results_raw is None:
        raise HTTPException(status_code=404, detail="æœç´¢ä¼šè¯å·²è¿‡æœŸæˆ–æ— æ•ˆï¼Œè¯·é‡æ–°æœç´¢ã€‚")

    try:
        cached_results = [models.ProviderSearchInfo.model_validate(r) for r in cached_results_raw]
    except Exception:
        raise HTTPException(status_code=500, detail="æ— æ³•è§£æç¼“å­˜çš„æœç´¢ç»“æœã€‚")

    if not (0 <= result_index < len(cached_results)):
        raise HTTPException(status_code=400, detail="æä¾›çš„ result_index æ— æ•ˆã€‚")

    item_to_fetch = cached_results[result_index]

    scraper = manager.get_scraper(item_to_fetch.provider)
    try:
        return await scraper.get_episodes(item_to_fetch.mediaId, db_media_type=item_to_fetch.type)
    except httpx.RequestError as e:
        logger.error(f"è·å–åˆ†é›†åˆ—è¡¨æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯ (provider={item_to_fetch.provider}, media_id={item_to_fetch.mediaId}): {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=f"ä» {item_to_fetch.provider} è·å–åˆ†é›†åˆ—è¡¨æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")



@router.post("/import/edited", status_code=status.HTTP_202_ACCEPTED, summary="å¯¼å…¥ç¼–è¾‘åçš„åˆ†é›†åˆ—è¡¨", response_model=ControlTaskResponse)
async def edited_import(
    payload: ControlEditedImportRequest,
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter),
    config_manager: ConfigManager = Depends(get_config_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    title_recognition_manager = Depends(get_title_recognition_manager)
):
    """
    ### åŠŸèƒ½
    å¯¼å…¥ä¸€ä¸ªç»è¿‡ç”¨æˆ·ç¼–è¾‘å’Œè°ƒæ•´çš„åˆ†é›†åˆ—è¡¨ã€‚

    ### å·¥ä½œæµç¨‹
    è¿™æ˜¯æœ€çµæ´»çš„å¯¼å…¥æ–¹å¼ã€‚å®ƒå…è®¸æ‚¨å®Œå…¨æ§åˆ¶è¦å¯¼å…¥çš„åˆ†é›†ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€é¡ºåºç­‰ã€‚æ‚¨å¯ä»¥åœ¨è¯·æ±‚ä¸­è¦†ç›–ä½œå“æ ‡é¢˜å’Œé™„åŠ å…ƒæ•°æ®IDã€‚
    """
    cache_key = f"control_search_{payload.searchId}"
    cached_results_raw = await crud.get_cache(session, cache_key)

    if cached_results_raw is None:
        raise HTTPException(status_code=404, detail="æœç´¢ä¼šè¯å·²è¿‡æœŸæˆ–æ— æ•ˆï¼Œè¯·é‡æ–°æœç´¢ã€‚")

    try:
        cached_results = [models.ProviderSearchInfo.model_validate(r) for r in cached_results_raw]
    except Exception:
        raise HTTPException(status_code=500, detail="æ— æ³•è§£æç¼“å­˜çš„æœç´¢ç»“æœã€‚")

    if not (0 <= payload.resultIndex < len(cached_results)):
        raise HTTPException(status_code=400, detail="æä¾›çš„ result_index æ— æ•ˆã€‚")

    item_to_import = cached_results[payload.resultIndex]

    # å…³é”®ä¿®å¤ï¼šæ¢å¤å¹¶å®Œå–„åœ¨ä»»åŠ¡æäº¤å‰çš„é‡å¤æ£€æŸ¥ã€‚
    # å¯¹äºç¼–è¾‘åå¯¼å…¥ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥æ¯ä¸ªå•é›†æ˜¯å¦å·²å­˜åœ¨ï¼ˆå¿…é¡»æ˜¯ç›¸åŒæ•°æ®æºï¼‰
    # æ£€æŸ¥æ•°æ®æºæ˜¯å¦å·²å­˜åœ¨
    source_exists = await crud.check_source_exists_by_media_id(session, item_to_import.provider, item_to_import.mediaId)

    if source_exists:
        # æ•°æ®æºå·²å­˜åœ¨ï¼Œæ£€æŸ¥æ¯ä¸ªè¦å¯¼å…¥çš„å•é›†æ˜¯å¦å·²æœ‰å¼¹å¹•ï¼ˆå¿…é¡»æ˜¯ç›¸åŒ provider + media_idï¼‰
        existing_episodes = []
        for episode in payload.episodes:
            # ä½¿ç”¨ç²¾ç¡®æ£€æŸ¥ï¼šprovider + media_id + episode_index
            stmt = select(orm_models.Episode.id).join(
                orm_models.AnimeSource, orm_models.Episode.sourceId == orm_models.AnimeSource.id
            ).where(
                orm_models.AnimeSource.providerName == item_to_import.provider,
                orm_models.AnimeSource.mediaId == item_to_import.mediaId,
                orm_models.Episode.episodeIndex == episode.episodeIndex,
                orm_models.Episode.danmakuFilePath.isnot(None),
                orm_models.Episode.commentCount > 0
            ).limit(1)
            result = await session.execute(stmt)
            if result.scalar_one_or_none() is not None:
                existing_episodes.append(episode.episodeIndex)

        # å¦‚æœæ‰€æœ‰é›†éƒ½å·²å­˜åœ¨ï¼Œåˆ™é˜»æ­¢å¯¼å…¥
        if len(existing_episodes) == len(payload.episodes):
            episode_list = ", ".join(map(str, existing_episodes))
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT,
                detail=f"æ‰€æœ‰è¦å¯¼å…¥çš„åˆ†é›† ({episode_list}) éƒ½å·²åœ¨è¯¥æ•°æ®æºï¼ˆ{item_to_import.provider}ï¼‰ä¸­å­˜åœ¨å¼¹å¹•"
            )
        # å¦‚æœéƒ¨åˆ†é›†å·²å­˜åœ¨ï¼Œç»™å‡ºè­¦å‘Šä½†å…è®¸å¯¼å…¥
        elif existing_episodes:
            episode_list = ", ".join(map(str, existing_episodes))
            logger.warning(f"å¤–éƒ¨APIç¼–è¾‘å¯¼å…¥: åˆ†é›† {episode_list} å·²åœ¨è¯¥æ•°æ®æºï¼ˆ{item_to_import.provider}ï¼‰ä¸­å­˜åœ¨ï¼Œå°†è·³è¿‡è¿™äº›åˆ†é›†")


    # æ„å»ºç¼–è¾‘å¯¼å…¥è¯·æ±‚
    edited_request = models.EditedImportRequest(
        provider=item_to_import.provider,
        mediaId=item_to_import.mediaId,
        animeTitle=payload.title or item_to_import.title,
        mediaType=item_to_import.type,
        season=item_to_import.season,
        year=item_to_import.year,
        episodes=payload.episodes,
        tmdbId=payload.tmdbId,
        imdbId=payload.imdbId,
        tvdbId=payload.tvdbId,
        doubanId=payload.doubanId,
        bangumiId=payload.bangumiId,
        tmdbEpisodeGroupId=payload.tmdbEpisodeGroupId,
        imageUrl=item_to_import.imageUrl
    )

    # ä¿®æ­£ï¼šä¸ºä»»åŠ¡æ ‡é¢˜æ·»åŠ å­£/é›†ä¿¡æ¯ï¼Œä»¥ç¡®ä¿å…¶å”¯ä¸€æ€§ï¼Œé˜²æ­¢å› ä»»åŠ¡åé‡å¤è€Œæäº¤å¤±è´¥ã€‚
    title_parts = [f"å¤–éƒ¨APIç¼–è¾‘åå¯¼å…¥: {edited_request.animeTitle} ({edited_request.provider})"]
    if edited_request.season is not None:
        title_parts.append(f"S{edited_request.season:02d}")
    if payload.episodes:
        episode_indices = sorted([ep.episodeIndex for ep in payload.episodes])
        if len(episode_indices) == 1:
            title_parts.append(f"E{episode_indices[0]:02d}")
        else:
            title_parts.append(f"({len(episode_indices)}é›†)")
    task_title = " ".join(title_parts)

    # ä¿®æ­£ï¼šä½¿ unique_key æ›´å…·ä½“ï¼Œä»¥å…è®¸å¯¹åŒä¸€åª’ä½“çš„ä¸åŒåˆ†é›†åˆ—è¡¨è¿›è¡Œæ’é˜Ÿå¯¼å…¥ã€‚
    episode_indices_str = ",".join(sorted([str(ep.episodeIndex) for ep in payload.episodes]))
    episodes_hash = hashlib.md5(episode_indices_str.encode('utf-8')).hexdigest()[:8]
    unique_key = f"import-{edited_request.provider}-{edited_request.mediaId}-{episodes_hash}"

    try:
        task_coro = lambda session, cb: tasks.edited_import_task(
            request_data=edited_request, progress_callback=cb, session=session,
            config_manager=config_manager, manager=manager, rate_limiter=rate_limiter,
            metadata_manager=metadata_manager, title_recognition_manager=title_recognition_manager
        )
        task_id, _ = await task_manager.submit_task(task_coro, task_title, unique_key=unique_key)
        return {"message": "ç¼–è¾‘åå¯¼å…¥ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"æäº¤ç¼–è¾‘åå¯¼å…¥ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æäº¤ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")



@router.post("/import/xml", status_code=status.HTTP_202_ACCEPTED, summary="ä»XML/æ–‡æœ¬å¯¼å…¥å¼¹å¹•", response_model=ControlTaskResponse)
async def xml_import(
    payload: ControlXmlImportRequest,
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter)
):
    """
    ### åŠŸèƒ½
    ä¸ºä¸€ä¸ªå·²å­˜åœ¨çš„æ•°æ®æºï¼Œå¯¼å…¥æŒ‡å®šé›†æ•°çš„å¼¹å¹•ï¼ˆé€šè¿‡XMLæˆ–çº¯æ–‡æœ¬å†…å®¹ï¼‰ã€‚
    ### å·¥ä½œæµç¨‹
    1.  æ‚¨éœ€è¦æä¾›ä¸€ä¸ªå·²å­˜åœ¨äºç³»ç»Ÿä¸­çš„ `sourceId`ã€‚
    2.  æä¾›è¦å¯¼å…¥çš„ `episodeIndex` (é›†æ•°) å’Œå¼¹å¹• `content`ã€‚
    3.  ç³»ç»Ÿä¼šä¸ºè¯¥æ•°æ®æºåˆ›å»ºä¸€ä¸ªåå°ä»»åŠ¡ï¼Œå°†å†…å®¹è§£æå¹¶å¯¼å…¥åˆ°æŒ‡å®šåˆ†é›†ã€‚

    æ­¤æ¥å£éå¸¸é€‚åˆç”¨äºå¯¹å·²æœ‰çš„æ•°æ®æºè¿›è¡Œå•é›†è¡¥å…¨æˆ–æ›´æ–°ã€‚
    """
    source_info = await crud.get_anime_source_info(session, payload.sourceId)
    if not source_info:
        raise HTTPException(status_code=404, detail=f"æ•°æ®æº ID: {payload.sourceId} æœªæ‰¾åˆ°ã€‚")

    # This type of import should only be for 'custom' provider
    if source_info["providerName"] != 'custom':
        raise HTTPException(status_code=400, detail=f"XML/æ–‡æœ¬å¯¼å…¥ä»…æ”¯æŒ 'custom' ç±»å‹çš„æºï¼Œä½†ç›®æ ‡æºç±»å‹ä¸º '{source_info['providerName']}'ã€‚")

    anime_id = source_info["animeId"]
    anime_title = source_info["title"]

    task_title = f"å¤–éƒ¨API XMLå¯¼å…¥: {anime_title} - ç¬¬ {payload.episodeIndex} é›†"
    unique_key = f"manual-import-{payload.sourceId}-{payload.episodeIndex}"

    try:
        task_coro = lambda s, cb: tasks.manual_import_task(
            sourceId=payload.sourceId,
            animeId=anime_id,
            title=payload.title,
            episodeIndex=payload.episodeIndex,
            content=payload.content,
            providerName='custom',
            progress_callback=cb,
            session=s,
            manager=manager,
            rate_limiter=rate_limiter
        )
        task_id, _ = await task_manager.submit_task(task_coro, task_title, unique_key=unique_key)
        return {"message": "XMLå¯¼å…¥ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"æäº¤XMLå¯¼å…¥ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æäº¤ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")


@router.post("/import/url", status_code=status.HTTP_202_ACCEPTED, summary="ä»URLå¯¼å…¥", response_model=ControlTaskResponse)
async def url_import(
    payload: ControlUrlImportRequest,
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter)
):
    """
    ### åŠŸèƒ½
    ä¸ºä¸€ä¸ªå·²å­˜åœ¨çš„æ•°æ®æºï¼Œå¯¼å…¥æŒ‡å®šé›†æ•°çš„å¼¹å¹•ã€‚
    ### å·¥ä½œæµç¨‹
    1.  æ‚¨éœ€è¦æä¾›ä¸€ä¸ªå·²å­˜åœ¨äºç³»ç»Ÿä¸­çš„ `sourceId`ã€‚
    2.  æä¾›è¦å¯¼å…¥çš„ `episodeIndex` (é›†æ•°) å’ŒåŒ…å«å¼¹å¹•çš„è§†é¢‘é¡µé¢ `url`ã€‚
    3.  ç³»ç»Ÿä¼šä¸ºè¯¥æ•°æ®æºåˆ›å»ºä¸€ä¸ªåå°ä»»åŠ¡ï¼Œç²¾ç¡®åœ°è·å–å¹¶å¯¼å…¥æŒ‡å®šé›†æ•°çš„å¼¹å¹•ã€‚

    æ­¤æ¥å£éå¸¸é€‚åˆç”¨äºå¯¹å·²æœ‰çš„æ•°æ®æºè¿›è¡Œå•é›†è¡¥å…¨æˆ–æ›´æ–°ã€‚
    """
    source_info = await crud.get_anime_source_info(session, payload.sourceId)
    if not source_info:
        raise HTTPException(status_code=404, detail=f"æ•°æ®æº ID: {payload.sourceId} æœªæ‰¾åˆ°ã€‚")

    provider_name = source_info["providerName"]
    anime_id = source_info["animeId"]
    anime_title = source_info["title"]

    scraper = manager.get_scraper(provider_name)
    if not hasattr(scraper, 'get_info_from_url'):
        raise HTTPException(status_code=400, detail=f"æ•°æ®æº '{provider_name}' ä¸æ”¯æŒä»URLå¯¼å…¥ã€‚")

    task_title = f"å¤–éƒ¨API URLå¯¼å…¥: {anime_title} - ç¬¬ {payload.episodeIndex} é›† ({provider_name})"
    unique_key = f"manual-import-{payload.sourceId}-{payload.episodeIndex}"

    try:
        task_coro = lambda s, cb: tasks.manual_import_task(
            sourceId=payload.sourceId,
            animeId=anime_id,
            title=payload.title,
            episodeIndex=payload.episodeIndex,
            content=payload.url,
            providerName=provider_name,
            progress_callback=cb,
            session=s,
            manager=manager,
            rate_limiter=rate_limiter
        )
        task_id, _ = await task_manager.submit_task(task_coro, task_title, unique_key=unique_key)
        return {"message": "URLå¯¼å…¥ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"æäº¤URLå¯¼å…¥ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æäº¤ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")