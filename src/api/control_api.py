import logging
import secrets
import uuid
import re
import hashlib
import ipaddress
import json
import asyncio
from enum import Enum
from typing import List, Optional, Dict, Any, Callable, Union

import httpx
from fastapi import APIRouter, Depends, HTTPException, Query, Request, status, Path
from fastapi.responses import StreamingResponse
from fastapi.security import APIKeyQuery
from thefuzz import fuzz
from pydantic import BaseModel, Field, model_validator
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import exc, select

from .. import crud, models, tasks, utils, orm_models
from ..rate_limiter import RateLimiter, RateLimitExceededError
from ..config_manager import ConfigManager
from ..ai.ai_matcher_manager import AIMatcherManager
from ..database import get_db_session
from ..metadata_manager import MetadataSourceManager
from ..scheduler import SchedulerManager
from ..scraper_manager import ScraperManager
from ..task_manager import TaskManager, TaskSuccess, TaskStatus
from ..search_utils import unified_search
from ..season_mapper import ai_type_and_season_mapping_and_correction
from ..ai.ai_matcher import AIMatcher
from ..season_mapper import title_contains_season_name

from ..timezone import get_now
from ..search_timer import SearchTimer, SEARCH_TYPE_CONTROL_SEARCH
from ..name_converter import convert_to_chinese_title
logger = logging.getLogger(__name__)

# --- Helper Functions ---

def _normalize_for_filtering(title: str) -> str:
    """Removes brackets and standardizes a title for fuzzy matching."""
    if not title:
        return ""
    # Remove content in brackets
    title = re.sub(r'[\[ã€(ï¼ˆ].*?[\]ã€‘)ï¼‰]', '', title)
    # Normalize to lowercase, remove spaces, and standardize colons
    return title.lower().replace(" ", "").replace("ï¼š", ":").strip()

def _is_movie_by_title(title: str) -> bool:
    """Checks if a title likely represents a movie based on keywords."""
    if not title:
        return False
    return any(kw in title.lower() for kw in ["å‰§åœºç‰ˆ", "åŠ‡å ´ç‰ˆ", "movie", "æ˜ ç”»"])

# --- ä¾èµ–é¡¹ ---

def get_scraper_manager(request: Request) -> ScraperManager:
    """ä¾èµ–é¡¹ï¼šä»åº”ç”¨çŠ¶æ€è·å– Scraper ç®¡ç†å™¨"""
    return request.app.state.scraper_manager

def get_metadata_manager(request: Request) -> MetadataSourceManager:
    """ä¾èµ–é¡¹ï¼šä»åº”ç”¨çŠ¶æ€è·å–å…ƒæ•°æ®æºç®¡ç†å™¨"""
    return request.app.state.metadata_manager

def get_task_manager(request: Request) -> TaskManager:
    """ä¾èµ–é¡¹ï¼šä»åº”ç”¨çŠ¶æ€è·å–ä»»åŠ¡ç®¡ç†å™¨"""
    return request.app.state.task_manager

def get_scheduler_manager(request: Request) -> SchedulerManager:
    """ä¾èµ–é¡¹ï¼šä»åº”ç”¨çŠ¶æ€è·å–å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    return request.app.state.scheduler_manager

def get_config_manager(request: Request) -> ConfigManager:
    """ä¾èµ–é¡¹ï¼šä»åº”ç”¨çŠ¶æ€è·å–é…ç½®ç®¡ç†å™¨"""
    return request.app.state.config_manager

def get_rate_limiter(request: Request) -> RateLimiter:
    """ä¾èµ–é¡¹ï¼šä»åº”ç”¨çŠ¶æ€è·å–é€Ÿç‡é™åˆ¶å™¨"""
    return request.app.state.rate_limiter

def get_ai_matcher_manager(request: Request) -> AIMatcherManager:
    """ä¾èµ–é¡¹ï¼šä»åº”ç”¨çŠ¶æ€è·å–AIåŒ¹é…å™¨ç®¡ç†å™¨"""
    return request.app.state.ai_matcher_manager

def get_title_recognition_manager(request: Request):
    """ä¾èµ–é¡¹ï¼šä»åº”ç”¨çŠ¶æ€è·å–æ ‡é¢˜è¯†åˆ«ç®¡ç†å™¨"""
    return request.app.state.title_recognition_manager



# æ–°å¢ï¼šå®šä¹‰API Keyçš„å®‰å…¨æ–¹æ¡ˆï¼Œè¿™å°†è‡ªåŠ¨åœ¨Swagger UIä¸­ç”Ÿæˆâ€œAuthorizeâ€æŒ‰é’®
api_key_scheme = APIKeyQuery(name="api_key", auto_error=False, description="ç”¨äºæ‰€æœ‰å¤–éƒ¨æ§åˆ¶APIçš„è®¿é—®å¯†é’¥ã€‚")

async def verify_api_key(
    request: Request,
    api_key: str = Depends(api_key_scheme),
    session: AsyncSession = Depends(get_db_session),
) -> str:
    """ä¾èµ–é¡¹ï¼šéªŒè¯APIå¯†é’¥å¹¶è®°å½•è¯·æ±‚ã€‚å¦‚æœéªŒè¯æˆåŠŸï¼Œè¿”å› API Keyã€‚"""
    # --- æ–°å¢ï¼šè§£æçœŸå®å®¢æˆ·ç«¯IPï¼Œæ”¯æŒCIDR ---
    config_manager: ConfigManager = request.app.state.config_manager
    trusted_proxies_str = await config_manager.get("trustedProxies", "")
    trusted_networks = []
    if trusted_proxies_str:
        for proxy_entry in trusted_proxies_str.split(','):
            try:
                trusted_networks.append(ipaddress.ip_network(proxy_entry.strip()))
            except ValueError:
                logger.warning(f"æ— æ•ˆçš„å—ä¿¡ä»»ä»£ç†IPæˆ–CIDR: '{proxy_entry.strip()}'ï¼Œå·²å¿½ç•¥ã€‚")
    
    client_ip_str = request.client.host if request.client else "127.0.0.1"
    is_trusted = False
    if trusted_networks:
        try:
            client_addr = ipaddress.ip_address(client_ip_str)
            is_trusted = any(client_addr in network for network in trusted_networks)
        except ValueError:
            logger.warning(f"æ— æ³•å°†å®¢æˆ·ç«¯IP '{client_ip_str}' è§£æä¸ºæœ‰æ•ˆçš„IPåœ°å€ã€‚")

    if is_trusted:
        x_forwarded_for = request.headers.get("x-forwarded-for")
        if x_forwarded_for:
            client_ip_str = x_forwarded_for.split(',')[0].strip()
        else:
            client_ip_str = request.headers.get("x-real-ip", client_ip_str)
    # --- IPè§£æç»“æŸ ---

    endpoint = request.url.path

    if not api_key:
        await crud.create_external_api_log(
            session, client_ip_str, endpoint, status.HTTP_401_UNAUTHORIZED, "API Keyç¼ºå¤±"
        )
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Not authenticated: API Key is missing.",
        )

    stored_key = await config_manager.get("externalApiKey", "")

    if not stored_key or not secrets.compare_digest(api_key, stored_key):
        await crud.create_external_api_log(
            session, client_ip_str, endpoint, status.HTTP_401_UNAUTHORIZED, "æ— æ•ˆçš„APIå¯†é’¥"
        )
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, detail="æ— æ•ˆçš„APIå¯†é’¥"
        )
    # è®°å½•æˆåŠŸçš„API KeyéªŒè¯
    await crud.create_external_api_log(
        session, client_ip_str, endpoint, status.HTTP_200_OK, "API KeyéªŒè¯é€šè¿‡"
    )
    return api_key

router = APIRouter(dependencies=[Depends(verify_api_key)])

# --- Pydantic æ¨¡å‹ ---

class AutoImportSearchType(str, Enum):
    KEYWORD = "keyword"
    TMDB = "tmdb"
    TVDB = "tvdb"
    DOUBAN = "douban"
    IMDB = "imdb"
    BANGUMI = "bangumi"

class AutoImportMediaType(str, Enum):
    TV_SERIES = "tv_series"
    MOVIE = "movie"

class ControlActionResponse(BaseModel):
    """é€šç”¨æ“ä½œæˆåŠŸå“åº”æ¨¡å‹"""
    message: str

class ControlTaskResponse(BaseModel):
    """ä»»åŠ¡æäº¤æˆåŠŸå“åº”æ¨¡å‹"""
    message: str
    taskId: str

class ExecutionTaskResponse(BaseModel):
    """ç”¨äºè¿”å›æ‰§è¡Œä»»åŠ¡IDçš„å“åº”æ¨¡å‹"""
    schedulerTaskId: str
    executionTaskId: Optional[str] = None
    status: Optional[str] = Field(None, description="æ‰§è¡Œä»»åŠ¡çŠ¶æ€: è¿è¡Œä¸­/å·²å®Œæˆ/å¤±è´¥/å·²å–æ¶ˆ/ç­‰å¾…ä¸­/å·²æš‚åœ")

class ControlSearchResultItem(models.ProviderSearchInfo):
    resultIndex: int = Field(..., alias="result_index", description="ç»“æœåœ¨åˆ—è¡¨ä¸­çš„é¡ºåºç´¢å¼•ï¼Œä»0å¼€å§‹")

    class Config:
        populate_by_name = True

class ControlSearchResponse(BaseModel):
    searchId: str = Field(..., description="æœ¬æ¬¡æœç´¢æ“ä½œçš„å”¯ä¸€IDï¼Œç”¨äºåç»­æ“ä½œ")
    results: List[ControlSearchResultItem] = Field(..., description="æœç´¢ç»“æœåˆ—è¡¨")

class ControlDirectImportRequest(BaseModel):
    searchId: str = Field(..., description="æ¥è‡ªæœç´¢å“åº”çš„searchId")
    resultIndex: int = Field(..., alias="result_index", ge=0, description="è¦å¯¼å…¥çš„ç»“æœçš„ç´¢å¼• (ä»0å¼€å§‹)")
    # ä¿®æ­£ï¼šå°†å¯é€‰çš„å…ƒæ•°æ®IDç§»åˆ°æ¨¡å‹æœ«å°¾ï¼Œä»¥æ”¹å–„æ–‡æ¡£æ˜¾ç¤ºé¡ºåº
    tmdbId: Optional[str] = None
    tvdbId: Optional[str] = None
    bangumiId: Optional[str] = None
    imdbId: Optional[str] = None
    doubanId: Optional[str] = None

    class Config:
        populate_by_name = True

class ControlAnimeCreateRequest(BaseModel):
    """ç”¨äºå¤–éƒ¨APIè‡ªå®šä¹‰åˆ›å»ºå½±è§†æ¡ç›®çš„è¯·æ±‚æ¨¡å‹"""
    title: str = Field(..., description="ä½œå“ä¸»æ ‡é¢˜")
    type: AutoImportMediaType = Field(..., description="åª’ä½“ç±»å‹")
    season: Optional[int] = Field(None, description="å­£åº¦å· (tv_series ç±»å‹å¿…éœ€)")
    year: Optional[int] = Field(None, description="å¹´ä»½")
    nameEn: Optional[str] = Field(None, description="è‹±æ–‡æ ‡é¢˜")
    nameJp: Optional[str] = Field(None, description="æ—¥æ–‡æ ‡é¢˜")
    nameRomaji: Optional[str] = Field(None, description="ç½—é©¬éŸ³æ ‡é¢˜")
    aliasCn1: Optional[str] = Field(None, description="ä¸­æ–‡åˆ«å1")
    aliasCn2: Optional[str] = Field(None, description="ä¸­æ–‡åˆ«å2")
    aliasCn3: Optional[str] = Field(None, description="ä¸­æ–‡åˆ«å3")
    # ä¿®æ­£ï¼šå°†å¯é€‰çš„å…ƒæ•°æ®IDç§»åˆ°æ¨¡å‹æœ«å°¾ï¼Œä»¥æ”¹å–„æ–‡æ¡£æ˜¾ç¤ºé¡ºåº
    tmdbId: Optional[str] = None
    tvdbId: Optional[str] = None
    bangumiId: Optional[str] = None
    imdbId: Optional[str] = None
    doubanId: Optional[str] = None

    @model_validator(mode='after')
    def check_season_for_tv_series(self):
        if self.type == 'tv_series' and self.season is None:
            raise ValueError('å¯¹äºç”µè§†èŠ‚ç›® (tv_series)ï¼Œå­£åº¦ (season) æ˜¯å¿…éœ€çš„ã€‚')
        return self

class ControlEditedImportRequest(BaseModel):
    searchId: str = Field(..., description="æ¥è‡ªæœç´¢å“åº”çš„searchId")
    resultIndex: int = Field(..., alias="result_index", ge=0, description="è¦ç¼–è¾‘çš„ç»“æœçš„ç´¢å¼• (ä»0å¼€å§‹)")
    title: Optional[str] = Field(None, description="è¦†ç›–åŸå§‹æ ‡é¢˜")
    episodes: List[models.ProviderEpisodeInfo] = Field(..., description="ç¼–è¾‘åçš„åˆ†é›†åˆ—è¡¨")
    # ä¿®æ­£ï¼šå°†å¯é€‰çš„å…ƒæ•°æ®IDç§»åˆ°æ¨¡å‹æœ«å°¾ï¼Œä»¥æ”¹å–„æ–‡æ¡£æ˜¾ç¤ºé¡ºåº
    tmdbId: Optional[str] = None
    tvdbId: Optional[str] = None
    bangumiId: Optional[str] = None
    imdbId: Optional[str] = None
    doubanId: Optional[str] = None
    tmdbEpisodeGroupId: Optional[str] = Field(None, description="å¼ºåˆ¶æŒ‡å®šTMDBå‰§é›†ç»„ID")

    class Config:
        populate_by_name = True

class ControlUrlImportRequest(BaseModel):
    """ç”¨äºå¤–éƒ¨APIé€šè¿‡URLå¯¼å…¥åˆ°æŒ‡å®šæºçš„è¯·æ±‚æ¨¡å‹"""
    sourceId: int = Field(..., description="è¦å¯¼å…¥åˆ°çš„ç›®æ ‡æ•°æ®æºID")
    episodeIndex: int = Field(..., alias="episode_index", description="è¦å¯¼å…¥çš„ç‰¹å®šé›†æ•°", gt=0)
    url: str = Field(..., description="åŒ…å«å¼¹å¹•çš„è§†é¢‘é¡µé¢çš„URL")
    title: Optional[str] = Field(None, description="ï¼ˆå¯é€‰ï¼‰å¼ºåˆ¶æŒ‡å®šåˆ†é›†æ ‡é¢˜")

    class Config:
        populate_by_name = True

class ControlXmlImportRequest(BaseModel):
    """ç”¨äºå¤–éƒ¨APIé€šè¿‡XML/æ–‡æœ¬å¯¼å…¥åˆ°æŒ‡å®šæºçš„è¯·æ±‚æ¨¡å‹"""
    sourceId: int = Field(..., description="è¦å¯¼å…¥åˆ°çš„ç›®æ ‡æ•°æ®æºID")
    episodeIndex: int = Field(..., alias="episode_index", description="è¦å¯¼å…¥çš„ç‰¹å®šé›†æ•°", gt=0)
    content: str = Field(..., description="XMLæˆ–çº¯æ–‡æœ¬æ ¼å¼çš„å¼¹å¹•å†…å®¹")
    title: Optional[str] = Field(None, description="ï¼ˆå¯é€‰ï¼‰å¼ºåˆ¶æŒ‡å®šåˆ†é›†æ ‡é¢˜")

    class Config:
        populate_by_name = True

class DanmakuOutputSettings(BaseModel):
    limitPerSource: int = Field(..., alias="limit_per_source")
    mergeOutputEnabled: bool = Field(..., alias="merge_output_enabled")

    class Config:
        populate_by_name = True

class ControlAnimeDetailsResponse(BaseModel):
    """ç”¨äºå¤–éƒ¨APIçš„ç•ªå‰§è¯¦æƒ…å“åº”æ¨¡å‹ï¼Œä¸åŒ…å«å†—ä½™çš„anime_idã€‚"""
    title: str
    type: str
    season: int
    year: Optional[int] = None
    episodeCount: Optional[int] = None
    localImagePath: Optional[str] = None
    imageUrl: Optional[str] = None
    tmdbId: Optional[str] = None
    tmdbEpisodeGroupId: Optional[str] = None
    bangumiId: Optional[str] = None
    tvdbId: Optional[str] = None
    doubanId: Optional[str] = None
    imdbId: Optional[str] = None
    nameEn: Optional[str] = None
    nameJp: Optional[str] = None
    nameRomaji: Optional[str] = None
    aliasCn1: Optional[str] = None
    aliasCn2: Optional[str] = None
    aliasCn3: Optional[str] = None

class ControlAutoImportRequest(BaseModel):
    searchType: AutoImportSearchType
    searchTerm: str
    season: Optional[int] = None
    episode: Optional[str] = None  # æ”¯æŒå•é›†(å¦‚"1")æˆ–å¤šé›†(å¦‚"1,3,5,7,9,11-13")æ ¼å¼
    mediaType: Optional[AutoImportMediaType] = None
    preassignedAnimeId: Optional[int] = None  # é¢„åˆ†é…çš„anime_idï¼ˆç”¨äºåŒ¹é…åå¤‡ï¼‰

class ControlMetadataSearchResponse(BaseModel):
    """ç”¨äºå¤–éƒ¨APIçš„å…ƒæ•°æ®æœç´¢å“åº”æ¨¡å‹"""
    results: List[models.MetadataDetailsResponse]

# --- API è·¯ç”± ---

@router.post("/import/auto", status_code=status.HTTP_202_ACCEPTED, summary="å…¨è‡ªåŠ¨æœç´¢å¹¶å¯¼å…¥", response_model=ControlTaskResponse)
async def auto_import(
    request: Request,
    searchType: AutoImportSearchType = Query(..., description="æœç´¢ç±»å‹ã€‚å¯é€‰å€¼: 'keyword', 'tmdb', 'tvdb', 'douban', 'imdb', 'bangumi'ã€‚"),
    searchTerm: str = Query(..., description="æœç´¢å†…å®¹ã€‚æ ¹æ® searchType çš„ä¸åŒï¼Œè¿™é‡Œåº”å¡«å…¥å…³é”®è¯æˆ–å¯¹åº”çš„å¹³å°IDã€‚"),
    season: Optional[int] = Query(None, description="å­£åº¦å·ã€‚å¦‚æœæœªæä¾›ï¼Œå°†è‡ªåŠ¨æ¨æ–­æˆ–é»˜è®¤ä¸º1ã€‚"),
    episode: Optional[str] = Query(None, description="é›†æ•°ã€‚æ”¯æŒå•é›†(å¦‚'1')æˆ–å¤šé›†(å¦‚'1,3,5,7,9,11-13')æ ¼å¼ã€‚å¦‚æœæä¾›ï¼Œå°†åªå¯¼å…¥æŒ‡å®šé›†æ•°ï¼ˆæ­¤æ—¶å¿…é¡»æä¾›å­£åº¦ï¼‰ã€‚"),
    mediaType: Optional[AutoImportMediaType] = Query(None, description="åª’ä½“ç±»å‹ã€‚å½“ searchType ä¸º 'keyword' æ—¶å¿…å¡«ã€‚å¦‚æœç•™ç©ºï¼Œå°†æ ¹æ®æœ‰æ—  'season' å‚æ•°è‡ªåŠ¨æ¨æ–­ã€‚"),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter),
    config_manager: ConfigManager = Depends(get_config_manager),
    ai_matcher_manager: AIMatcherManager = Depends(get_ai_matcher_manager),
    title_recognition_manager = Depends(get_title_recognition_manager),
    api_key: str = Depends(verify_api_key)
):
    """
    ### åŠŸèƒ½
    è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„â€œå…¨è‡ªåŠ¨æœç´¢å¹¶å¯¼å…¥â€æ¥å£ï¼Œå®ƒèƒ½æ ¹æ®ä¸åŒçš„IDç±»å‹ï¼ˆå¦‚TMDB IDã€Bangumi IDç­‰ï¼‰æˆ–å…³é”®è¯è¿›è¡Œæœç´¢ï¼Œå¹¶æ ¹æ®ä¸€ç³»åˆ—æ™ºèƒ½è§„åˆ™è‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„æ•°æ®æºè¿›è¡Œå¼¹å¹•å¯¼å…¥ã€‚

    ### å·¥ä½œæµç¨‹
    1.  **å…ƒæ•°æ®è·å–**: å¦‚æœä½¿ç”¨IDæœç´¢ï¼ˆå¦‚`tmdb`, `bangumi`ï¼‰ï¼Œæ¥å£ä¼šé¦–å…ˆä»å¯¹åº”çš„å…ƒæ•°æ®ç½‘ç«™è·å–ä½œå“çš„å®˜æ–¹æ ‡é¢˜å’Œåˆ«åã€‚
    2.  **åª’ä½“åº“æ£€æŸ¥**: æ£€æŸ¥æ­¤ä½œå“æ˜¯å¦å·²å­˜åœ¨äºæ‚¨çš„å¼¹å¹•åº“ä¸­ã€‚
        -   å¦‚æœå­˜åœ¨ä¸”æœ‰ç²¾ç¡®æ ‡è®°çš„æºï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨è¯¥æºã€‚
        -   å¦‚æœå­˜åœ¨ä½†æ— ç²¾ç¡®æ ‡è®°ï¼Œåˆ™ä½¿ç”¨å·²æœ‰å…³è”æºä¸­ä¼˜å…ˆçº§æœ€é«˜çš„é‚£ä¸ªã€‚
    3.  **å…¨ç½‘æœç´¢**: å¦‚æœåª’ä½“åº“ä¸­ä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨è·å–åˆ°çš„æ ‡é¢˜å’Œåˆ«ååœ¨æ‰€æœ‰å·²å¯ç”¨çš„å¼¹å¹•æºä¸­è¿›è¡Œæœç´¢ã€‚
    4.  **æ™ºèƒ½é€‰æ‹©**: ä»æœç´¢ç»“æœä¸­ï¼Œæ ¹æ®æ‚¨åœ¨â€œæœç´¢æºâ€é¡µé¢è®¾ç½®çš„ä¼˜å…ˆçº§ï¼Œé€‰æ‹©æœ€ä½³åŒ¹é…é¡¹ã€‚
    5.  **ä»»åŠ¡æäº¤**: ä¸ºæœ€ç»ˆé€‰æ‹©çš„æºåˆ›å»ºä¸€ä¸ªåå°å¯¼å…¥ä»»åŠ¡ã€‚

    ### å‚æ•°ä½¿ç”¨è¯´æ˜
    -   `searchType`:
        -   `keyword`: æŒ‰å…³é”®è¯æœç´¢ã€‚æ­¤æ—¶ `mediaType` å­—æ®µ**å¿…å¡«**ã€‚
        -   `tmdb`, `tvdb`, `douban`, `imdb`, `bangumi`: æŒ‰å¯¹åº”å¹³å°çš„IDè¿›è¡Œç²¾ç¡®æœç´¢ã€‚
    -   `season` & `episode`:
        -   **ç”µè§†å‰§/ç•ªå‰§**:
            -   æä¾› `season`ï¼Œä¸æä¾› `episode`: å¯¼å…¥ `season` æŒ‡å®šçš„æ•´å­£ã€‚
            -   æä¾› `season` å’Œ `episode`: å¯¼å…¥æŒ‡å®šçš„é›†æ•°ã€‚æ”¯æŒå•é›†(å¦‚'1')æˆ–å¤šé›†(å¦‚'1,3,5,7,9,11-13')æ ¼å¼ã€‚
        -   **ç”µå½±**:
            -   `season` å’Œ `episode` å‚æ•°ä¼šè¢«å¿½ç•¥ã€‚
    -   `mediaType`:
        -   å½“ `searchType` ä¸º `keyword` æ—¶ï¼Œæ­¤å­—æ®µä¸º**å¿…å¡«é¡¹**ã€‚
        -   å½“ `searchType` ä¸ºå…¶ä»–IDç±»å‹æ—¶ï¼Œæ­¤å­—æ®µä¸ºå¯é€‰é¡¹ã€‚å¦‚æœç•™ç©ºï¼Œç³»ç»Ÿå°†æ ¹æ® `season` å‚æ•°æ˜¯å¦å­˜åœ¨æ¥è‡ªåŠ¨æ¨æ–­åª’ä½“ç±»å‹ï¼ˆæœ‰ `season` åˆ™ä¸ºç”µè§†å‰§ï¼Œå¦åˆ™ä¸ºç”µå½±ï¼‰ã€‚
    """
    if episode is not None and season is None:
        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail="å½“æä¾› 'episode' å‚æ•°æ—¶ï¼Œ'season' å‚æ•°ä¹Ÿå¿…é¡»æä¾›ã€‚")

    payload = ControlAutoImportRequest(
        searchType=searchType,
        searchTerm=searchTerm,
        season=season,
        episode=episode,
        mediaType=mediaType
    )

    # ä¿®æ­£ï¼šä¸å†å¼ºåˆ¶å°†éå…³é”®è¯æœç´¢çš„ mediaType è®¾ä¸º Noneã€‚
    # å…è®¸ç”¨æˆ·åœ¨è°ƒç”¨ TMDB ç­‰IDæœç´¢æ—¶ï¼Œé¢„å…ˆæŒ‡å®šåª’ä½“ç±»å‹ï¼Œä»¥é¿å…é”™è¯¯çš„ç±»å‹æ¨æ–­ã€‚
    if payload.searchType == AutoImportSearchType.KEYWORD and not payload.mediaType:
        raise HTTPException(status_code=400, detail="ä½¿ç”¨ keyword æœç´¢æ—¶ï¼ŒmediaType å­—æ®µæ˜¯å¿…éœ€çš„ã€‚")

    # æ–°å¢ï¼šå¦‚æœä¸æ˜¯å…³é”®è¯æœç´¢ï¼Œåˆ™æ£€æŸ¥æ‰€é€‰çš„å…ƒæ•°æ®æºæ˜¯å¦å·²å¯ç”¨
    if payload.searchType != AutoImportSearchType.KEYWORD:
        provider_name = payload.searchType.value
        provider_setting = metadata_manager.source_settings.get(provider_name)
        
        if not provider_setting or not provider_setting.get('isEnabled'):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"å…ƒä¿¡æ¯æœç´¢æº '{provider_name}' æœªå¯ç”¨ã€‚è¯·åœ¨â€œè®¾ç½®-å…ƒä¿¡æ¯æœç´¢æºâ€é¡µé¢ä¸­å¯ç”¨å®ƒã€‚"
            )

    if not await manager.acquire_search_lock(api_key):
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail="å·²æœ‰æœç´¢æˆ–è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚"
        )

    # ä¿®æ­£ï¼šå°† seasonã€episode å’Œ mediaType çº³å…¥ unique_keyï¼Œä»¥å…è®¸åŒä¸€ä½œå“ä¸åŒå­£/é›†çš„å¯¼å…¥
    unique_key_parts = [payload.searchType.value, payload.searchTerm]
    if payload.season is not None:
        unique_key_parts.append(f"s{payload.season}")
    if payload.episode is not None:
        # å¯¹äºå¤šé›†æ ¼å¼ï¼Œä¿ç•™åŸå§‹å­—ç¬¦ä¸²ä½œä¸º unique_key çš„ä¸€éƒ¨åˆ†
        unique_key_parts.append(f"e{payload.episode}")
    # å§‹ç»ˆåŒ…å« mediaType ä»¥åŒºåˆ†åŒåä½†ä¸åŒç±»å‹çš„ä½œå“ï¼Œé¿å…é‡å¤ä»»åŠ¡æ£€æµ‹é—®é¢˜
    if payload.mediaType is not None:
        unique_key_parts.append(payload.mediaType.value)
    unique_key = f"auto-import-{'-'.join(unique_key_parts)}"

    # æ–°å¢ï¼šæ£€æŸ¥æœ€è¿‘æ˜¯å¦æœ‰é‡å¤ä»»åŠ¡
    config_manager = get_config_manager(request)
    threshold_hours_str = await config_manager.get("externalApiDuplicateTaskThresholdHours", "3")
    try:
        threshold_hours = int(threshold_hours_str)
    except (ValueError, TypeError):
        threshold_hours = 3

    if threshold_hours > 0:
        session_factory = request.app.state.db_session_factory
        async with session_factory() as session:
            recent_task = await crud.find_recent_task_by_unique_key(session, unique_key, threshold_hours)
            if recent_task:
                time_since_creation = get_now() - recent_task.createdAt
                hours_ago = time_since_creation.total_seconds() / 3600
                # å…³é”®ä¿®å¤ï¼šæŠ›å‡ºå¼‚å¸¸å‰é‡Šæ”¾æœç´¢é”
                await manager.release_search_lock(api_key)
                raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=f"ä¸€ä¸ªç›¸ä¼¼çš„ä»»åŠ¡åœ¨ {hours_ago:.1f} å°æ—¶å‰å·²è¢«æäº¤ (çŠ¶æ€: {recent_task.status})ã€‚è¯·åœ¨ {threshold_hours} å°æ—¶åé‡è¯•ã€‚")

            # å…³é”®ä¿®å¤ï¼šå¤–éƒ¨APIä¹Ÿåº”è¯¥æ£€æŸ¥åº“å†…æ˜¯å¦å·²å­˜åœ¨ç›¸åŒä½œå“
            # ä½¿ç”¨ä¸WebUIç›¸åŒçš„æ£€æŸ¥é€»è¾‘ï¼Œé€šè¿‡æ ‡é¢˜+å­£åº¦+é›†æ•°è¿›è¡Œæ£€æŸ¥
            title_recognition_manager = get_title_recognition_manager(request)

            # æ£€æŸ¥ä½œå“æ˜¯å¦å·²å­˜åœ¨äºåº“å†…
            existing_anime = await crud.find_anime_by_title_season_year(
                session, searchTerm, season, None, title_recognition_manager, None  # sourceå‚æ•°æš‚æ—¶ä¸ºNoneï¼Œå› ä¸ºè¿™é‡Œæ˜¯æŸ¥æ‰¾ç°æœ‰æ¡ç›®
            )

            if existing_anime and episode is not None:
                # å¯¹äºå•é›†/å¤šé›†å¯¼å…¥ï¼Œæ£€æŸ¥å…·ä½“é›†æ•°æ˜¯å¦å·²å­˜åœ¨ï¼ˆéœ€è¦è€ƒè™‘è¯†åˆ«è¯è½¬æ¢ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œä¸å†æ‹’ç»è¯·æ±‚ï¼Œè€Œæ˜¯åœ¨ä»»åŠ¡æ‰§è¡Œæ—¶è·³è¿‡å·²å­˜åœ¨çš„é›†æ•°
                pass
            elif existing_anime and episode is None:
                # å¯¹äºæ•´å­£å¯¼å…¥ï¼Œå¦‚æœä½œå“å·²å­˜åœ¨åˆ™æ‹’ç»
                # å…³é”®ä¿®å¤ï¼šæŠ›å‡ºå¼‚å¸¸å‰é‡Šæ”¾æœç´¢é”
                await manager.release_search_lock(api_key)
                raise HTTPException(
                    status_code=status.HTTP_409_CONFLICT,
                    detail=f"ä½œå“ '{searchTerm}' å·²åœ¨åª’ä½“åº“ä¸­ï¼Œæ— éœ€é‡å¤å¯¼å…¥æ•´å­£"
                )

    # ä¿®æ­£ï¼šä¸ºä»»åŠ¡æ ‡é¢˜æ·»åŠ å­£/é›†ä¿¡æ¯ï¼Œä»¥ç¡®ä¿å…¶å”¯ä¸€æ€§ï¼Œé˜²æ­¢å› ä»»åŠ¡åé‡å¤è€Œæäº¤å¤±è´¥ã€‚
    title_parts = [f"å¤–éƒ¨APIè‡ªåŠ¨å¯¼å…¥: {payload.searchTerm} (ç±»å‹: {payload.searchType})"]
    if payload.season is not None:
        title_parts.append(f"S{payload.season:02d}")
    if payload.episode is not None:
        # å¯¹äºå¤šé›†æ ¼å¼ï¼Œç›´æ¥æ˜¾ç¤ºåŸå§‹å­—ç¬¦ä¸²
        title_parts.append(f"E{payload.episode}")
    task_title = " ".join(title_parts)

    try:
        task_coro = lambda session, cb: tasks.auto_search_and_import_task(
            payload, cb, session, config_manager, manager, metadata_manager, task_manager,
            ai_matcher_manager=ai_matcher_manager,
            rate_limiter=rate_limiter,
            title_recognition_manager=title_recognition_manager,
            api_key=api_key
        )
        task_id, _ = await task_manager.submit_task(task_coro, task_title, unique_key=unique_key)
        # æ³¨æ„: æœç´¢é”ç”±ä»»åŠ¡å†…éƒ¨çš„ finally å—è´Ÿè´£é‡Šæ”¾,ç¡®ä¿ä»»åŠ¡å®Œæˆåæ‰é‡Šæ”¾
        return {"message": "è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except HTTPException as e:
        # æ•è·å·²çŸ¥çš„å†²çªé”™è¯¯å¹¶é‡æ–°æŠ›å‡º
        # å¦‚æœä»»åŠ¡æäº¤å¤±è´¥,éœ€è¦é‡Šæ”¾é”
        await manager.release_search_lock(api_key)
        raise e
    except Exception as e:
        # æ•è·ä»»ä½•åœ¨ä»»åŠ¡æäº¤é˜¶æ®µå‘ç”Ÿçš„å¼‚å¸¸,å¹¶ç¡®ä¿é‡Šæ”¾é”
        logger.error(f"æäº¤è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        await manager.release_search_lock(api_key)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æäº¤ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")

@router.get("/search", response_model=ControlSearchResponse, summary="æœç´¢åª’ä½“")
async def search_media(
    keyword: str,
    season: Optional[int] = Query(None, description="è¦æœç´¢çš„å­£åº¦ (å¯é€‰)"),
    episode: Optional[int] = Query(None, description="è¦æœç´¢çš„é›†æ•° (å¯é€‰)"),
    session: AsyncSession = Depends(get_db_session),
    manager: ScraperManager = Depends(get_scraper_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    config_manager: ConfigManager = Depends(get_config_manager),
    ai_matcher_manager: AIMatcherManager = Depends(get_ai_matcher_manager),
    api_key: str = Depends(verify_api_key)
):
    """
    ### åŠŸèƒ½
    æ ¹æ®å…³é”®è¯ä»æ‰€æœ‰å¯ç”¨çš„å¼¹å¹•æºæœç´¢åª’ä½“ã€‚è¿™æ˜¯æ‰§è¡Œå¯¼å…¥æ“ä½œçš„ç¬¬ä¸€æ­¥ã€‚

    ### å·¥ä½œæµç¨‹
    1.  æ¥æ”¶å…³é”®è¯ï¼Œä»¥åŠå¯é€‰çš„å­£åº¦å’Œé›†æ•°ã€‚
    2.  å¹¶å‘åœ°åœ¨æ‰€æœ‰å·²å¯ç”¨çš„å¼¹å¹•æºä¸Šè¿›è¡Œæœç´¢ã€‚
    3.  è¿”å›ä¸€ä¸ªåŒ…å«`searchId`å’Œç»“æœåˆ—è¡¨çš„å“åº”ã€‚`searchId`æ˜¯æœ¬æ¬¡æœç´¢çš„å”¯ä¸€æ ‡è¯†ï¼Œç”¨äºåç»­çš„å¯¼å…¥æ“ä½œã€‚
    4.  æœç´¢ç»“æœä¼šåœ¨æœåŠ¡å™¨ä¸Šç¼“å­˜10åˆ†é’Ÿã€‚

    ### å‚æ•°ä½¿ç”¨è¯´æ˜
    -   `keyword`: å¿…éœ€çš„æœç´¢å…³é”®è¯ã€‚
    -   `season`: (å¯é€‰) å¦‚æœæä¾›ï¼Œæœç´¢å°†æ›´å€¾å‘äºæˆ–åªè¿”å›ç”µè§†å‰§ç±»å‹çš„ç»“æœã€‚
    -   `episode`: (å¯é€‰) å¿…é¡»ä¸`season`ä¸€åŒæä¾›ï¼Œç”¨äºæ›´ç²¾ç¡®çš„å•é›†åŒ¹é…ã€‚
    """
    if episode is not None and season is None:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="æŒ‡å®šé›†æ•°æ—¶å¿…é¡»åŒæ—¶æä¾›å­£åº¦ä¿¡æ¯ã€‚"
        )

    if not await manager.acquire_search_lock(api_key):
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail="å·²æœ‰æœç´¢æˆ–è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚"
        )

    # åˆå§‹åŒ–è®¡æ—¶å™¨å¹¶å¼€å§‹è®¡æ—¶
    timer = SearchTimer(SEARCH_TYPE_CONTROL_SEARCH, keyword, logger)
    timer.start()

    try:
        timer.step_start("å…³é”®è¯è§£æ")
        # --- Start of new logic, copied and adapted from ui_api.py ---
        parsed_keyword = utils.parse_search_keyword(keyword)
        search_title = parsed_keyword["title"]
        original_title = search_title  # ä¿å­˜åŸå§‹æ ‡é¢˜ç”¨äºæ—¥å¿—
        # Prioritize explicit query params over parsed ones
        final_season = season if season is not None else parsed_keyword.get("season")
        final_episode = episode if episode is not None else parsed_keyword.get("episode")

        episode_info = {"season": final_season, "episode": final_episode} if final_season is not None or final_episode is not None else None

        # Create a dummy user for metadata calls, as this API is not user-specific
        user = models.User(id=0, username="control_api")

        logger.info(f"Control API æ­£åœ¨æœç´¢: '{keyword}' (è§£æä¸º: title='{search_title}', season={final_season}, episode={final_episode})")
        timer.step_end()

        # ğŸš€ åç§°è½¬æ¢åŠŸèƒ½ - æ£€æµ‹éä¸­æ–‡æ ‡é¢˜å¹¶å°è¯•è½¬æ¢ä¸ºä¸­æ–‡ï¼ˆåœ¨æ‰€æœ‰å¤„ç†ä¹‹å‰æ‰§è¡Œï¼‰
        timer.step_start("åç§°è½¬æ¢")
        converted_title, conversion_applied = await convert_to_chinese_title(
            search_title,
            config_manager,
            metadata_manager,
            ai_matcher_manager,
            user
        )
        if conversion_applied:
            logger.info(f"âœ“ Control API åç§°è½¬æ¢: '{original_title}' â†’ '{converted_title}'")
            search_title = converted_title
        else:
            logger.info(f"â—‹ Control API åç§°è½¬æ¢æœªç”Ÿæ•ˆ: '{original_title}'")
        timer.step_end()

        if not manager.has_enabled_scrapers:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="æ²¡æœ‰å¯ç”¨çš„å¼¹å¹•æœç´¢æºï¼Œè¯·åœ¨â€œæœç´¢æºâ€é¡µé¢ä¸­å¯ç”¨è‡³å°‘ä¸€ä¸ªã€‚"
            )

        # å¤–éƒ¨æ§åˆ¶æœç´¢ AIæ˜ å°„é…ç½®æ£€æŸ¥
        external_search_season_mapping_enabled = await config_manager.get("externalSearchEnableTmdbSeasonMapping", "false")
        if external_search_season_mapping_enabled.lower() != "true":
            logger.info("â—‹ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ ç»Ÿä¸€AIæ˜ å°„: åŠŸèƒ½æœªå¯ç”¨")

        timer.step_start("å¼¹å¹•æºæœç´¢")
        # ä½¿ç”¨ç»Ÿä¸€çš„æœç´¢å‡½æ•°ï¼ˆä¸è¿›è¡Œæ’åºï¼Œåé¢è‡ªå·±å¤„ç†ï¼‰
        results = await unified_search(
            search_term=search_title,
            session=session,
            scraper_manager=manager,
            metadata_manager=metadata_manager,
            use_alias_expansion=True,
            use_alias_filtering=True,
            use_title_filtering=True,
            use_source_priority_sorting=False,  # ä¸æ’åºï¼Œåé¢è‡ªå·±å¤„ç†
            progress_callback=None
        )
        # æ”¶é›†å•æºæœç´¢è€—æ—¶ä¿¡æ¯
        from ..search_timer import SubStepTiming
        source_timing_sub_steps = [
            SubStepTiming(name=name, duration_ms=dur, result_count=cnt)
            for name, dur, cnt in manager.last_search_timing
        ]
        timer.step_end(details=f"{len(results)}ä¸ªç»“æœ", sub_steps=source_timing_sub_steps)

        logger.info(f"æœç´¢å®Œæˆï¼Œå…± {len(results)} ä¸ªç»“æœ")

        for item in results:
            if item.type == 'tv_series' and _is_movie_by_title(item.title):
                item.type = 'movie'

            # ä¿®æ­£ï¼šå¦‚æœç”¨æˆ·æŒ‡å®šäº†é›†æ•°ï¼Œåˆ™è®¾ç½® currentEpisodeIndex
            # è¿™æ ·åç»­çš„å¯¼å…¥é€»è¾‘ä¼šè‡ªåŠ¨è¯†åˆ«ä¸ºå•é›†å¯¼å…¥
            if final_episode is not None:
                item.currentEpisodeIndex = final_episode

        if final_season:
            original_count = len(results)
            filtered_by_type = [item for item in results if item.type == 'tv_series']
            results = [item for item in filtered_by_type if item.season == final_season]
            logger.info(f"æ ¹æ®æŒ‡å®šçš„å­£åº¦ ({final_season}) è¿›è¡Œè¿‡æ»¤ï¼Œä» {original_count} ä¸ªç»“æœä¸­ä¿ç•™äº† {len(results)} ä¸ªã€‚")

        source_settings = await crud.get_all_scraper_settings(session)
        source_order_map = {s['providerName']: s['displayOrder'] for s in source_settings}

        def sort_key(item: models.ProviderSearchInfo):
            return (source_order_map.get(item.provider, 999), -fuzz.token_set_ratio(keyword, item.title))

        sorted_results = sorted(results, key=sort_key)

        # ä½¿ç”¨ç»Ÿä¸€çš„AIç±»å‹å’Œå­£åº¦æ˜ å°„ä¿®æ­£å‡½æ•°
        if external_search_season_mapping_enabled.lower() == "true":
            try:
                timer.step_start("AIæ˜ å°„ä¿®æ­£")
                # è·å–AIåŒ¹é…å™¨ï¼ˆä½¿ç”¨ä¾èµ–æ³¨å…¥çš„å®ä¾‹ï¼‰
                ai_matcher = await ai_matcher_manager.get_matcher()
                if ai_matcher:
                    logger.info(f"â—‹ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ å¼€å§‹ç»Ÿä¸€AIæ˜ å°„ä¿®æ­£: '{search_title}' ({len(sorted_results)} ä¸ªç»“æœ)")

                    # ä½¿ç”¨æ–°çš„ç»Ÿä¸€å‡½æ•°è¿›è¡Œç±»å‹å’Œå­£åº¦ä¿®æ­£
                    mapping_result = await ai_type_and_season_mapping_and_correction(
                        search_title=search_title,
                        search_results=sorted_results,
                        metadata_manager=metadata_manager,
                        ai_matcher=ai_matcher,
                        logger=logger,
                        similarity_threshold=60.0
                    )

                    # åº”ç”¨ä¿®æ­£ç»“æœ
                    if mapping_result['total_corrections'] > 0:
                        logger.info(f"âœ“ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ ç»Ÿä¸€AIæ˜ å°„æˆåŠŸ: æ€»è®¡ä¿®æ­£äº† {mapping_result['total_corrections']} ä¸ªç»“æœ")
                        logger.info(f"  - ç±»å‹ä¿®æ­£: {len(mapping_result['type_corrections'])} ä¸ª")
                        logger.info(f"  - å­£åº¦ä¿®æ­£: {len(mapping_result['season_corrections'])} ä¸ª")

                        # æ›´æ–°æœç´¢ç»“æœï¼ˆå·²ç»ç›´æ¥ä¿®æ”¹äº†sorted_resultsï¼‰
                        sorted_results = mapping_result['corrected_results']
                        timer.step_end(details=f"ä¿®æ­£{mapping_result['total_corrections']}ä¸ª")
                    else:
                        logger.info(f"â—‹ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ ç»Ÿä¸€AIæ˜ å°„: æœªæ‰¾åˆ°éœ€è¦ä¿®æ­£çš„ä¿¡æ¯")
                        timer.step_end(details="æ— ä¿®æ­£")
                else:
                    logger.warning("â—‹ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ AIæ˜ å°„: AIåŒ¹é…å™¨æœªå¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥")
                    timer.step_end(details="åŒ¹é…å™¨æœªå¯ç”¨")

            except Exception as e:
                logger.warning(f"å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ ç»Ÿä¸€AIæ˜ å°„ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                timer.step_end(details=f"å¤±è´¥: {e}")
        else:
            logger.info("â—‹ å¤–éƒ¨æ§åˆ¶-æœç´¢åª’ä½“ ç»Ÿä¸€AIæ˜ å°„: åŠŸèƒ½æœªå¯ç”¨")

        timer.step_start("ç»“æœç¼“å­˜")
        search_id = str(uuid.uuid4())
        indexed_results = [ControlSearchResultItem(**r.model_dump(), resultIndex=i) for i, r in enumerate(sorted_results)]
        await crud.set_cache(session, f"control_search_{search_id}", [r.model_dump() for r in sorted_results], 600)
        timer.step_end()

        timer.finish()  # æ‰“å°è®¡æ—¶æŠ¥å‘Š
        return ControlSearchResponse(searchId=search_id, results=indexed_results)
    finally:
        await manager.release_search_lock(api_key)

@router.post("/import/direct", status_code=status.HTTP_202_ACCEPTED, summary="ç›´æ¥å¯¼å…¥æœç´¢ç»“æœ", response_model=ControlTaskResponse)
async def direct_import(
    payload: ControlDirectImportRequest,
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    config_manager: ConfigManager = Depends(get_config_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter),
    title_recognition_manager = Depends(get_title_recognition_manager)
):
    """
    ### åŠŸèƒ½
    åœ¨æ‰§è¡Œ`/search`åï¼Œä½¿ç”¨è¿”å›çš„`searchId`å’Œæ‚¨é€‰æ‹©çš„ç»“æœç´¢å¼•ï¼ˆ`resultIndex`ï¼‰æ¥ç›´æ¥å¯¼å…¥å¼¹å¹•ã€‚

    ### å·¥ä½œæµç¨‹
    è¿™æ˜¯ä¸€ä¸ªç®€å•ã€ç›´æ¥çš„å¯¼å…¥æ–¹å¼ã€‚å®ƒä¼šä¸ºé€‰å®šçš„åª’ä½“åˆ›å»ºä¸€ä¸ªåå°å¯¼å…¥ä»»åŠ¡ã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨è¯·æ±‚ä¸­é™„åŠ å…ƒæ•°æ®IDï¼ˆå¦‚`tmdbId`ï¼‰æ¥è¦†ç›–æˆ–è¡¥å……ä½œå“ä¿¡æ¯ã€‚
    """
    cache_key = f"control_search_{payload.searchId}"
    cached_results_raw = await crud.get_cache(session, cache_key)
    
    if cached_results_raw is None:
        raise HTTPException(status_code=404, detail="æœç´¢ä¼šè¯å·²è¿‡æœŸæˆ–æ— æ•ˆï¼Œè¯·é‡æ–°æœç´¢ã€‚")
    
    try:
        cached_results = [models.ProviderSearchInfo.model_validate(r) for r in cached_results_raw]
    except Exception:
        raise HTTPException(status_code=500, detail="æ— æ³•è§£æç¼“å­˜çš„æœç´¢ç»“æœã€‚")

    if not (0 <= payload.resultIndex < len(cached_results)):
        raise HTTPException(status_code=400, detail="æä¾›çš„ result_index æ— æ•ˆã€‚")
        
    item_to_import = cached_results[payload.resultIndex] # type: ignore

    # å…³é”®ä¿®å¤ï¼šæ¢å¤å¹¶å®Œå–„åœ¨ä»»åŠ¡æäº¤å‰çš„é‡å¤æ£€æŸ¥ã€‚
    # è¿™ç¡®ä¿äº†ç›´æ¥å¯¼å…¥çš„è¡Œä¸ºä¸UIå¯¼å…¥å®Œå…¨ä¸€è‡´ã€‚
    duplicate_reason = await crud.check_duplicate_import(
        session=session,
        provider=item_to_import.provider,
        media_id=item_to_import.mediaId,
        anime_title=item_to_import.title,
        media_type=item_to_import.type,
        season=item_to_import.season,
        year=item_to_import.year,
        is_single_episode=item_to_import.currentEpisodeIndex is not None,
        episode_index=item_to_import.currentEpisodeIndex,
        title_recognition_manager=title_recognition_manager
    )
    if duplicate_reason:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=duplicate_reason
        )

    # ä¿®æ­£ï¼šä¸ºä»»åŠ¡æ ‡é¢˜æ·»åŠ å­£/é›†ä¿¡æ¯ï¼Œä»¥ç¡®ä¿å…¶å”¯ä¸€æ€§ï¼Œé˜²æ­¢å› ä»»åŠ¡åé‡å¤è€Œæäº¤å¤±è´¥ã€‚
    title_parts = [f"å¤–éƒ¨APIå¯¼å…¥: {item_to_import.title} ({item_to_import.provider})"]
    if item_to_import.currentEpisodeIndex is not None and item_to_import.season is not None:
        title_parts.append(f"S{item_to_import.season:02d}E{item_to_import.currentEpisodeIndex:02d}")
    task_title = " ".join(title_parts)

    # ä¿®æ­£ï¼šä¸ºå•é›†å¯¼å…¥ä»»åŠ¡ç”Ÿæˆæ›´å…·ä½“çš„å”¯ä¸€é”®ï¼Œä»¥å…è®¸å¯¹åŒä¸€ä½œå“çš„ä¸åŒå•é›†è¿›è¡Œæ’é˜Ÿã€‚
    # è¿™ä¿®å¤äº†åœ¨ä¸€æ¬¡å•é›†å¯¼å…¥å®Œæˆåï¼Œç«‹å³ä¸ºåŒä¸€ä½œå“æäº¤å¦ä¸€æ¬¡å•é›†å¯¼å…¥æ—¶ï¼Œå› ä»»åŠ¡é”®å†²çªè€Œè¢«æ‹’ç»çš„é—®é¢˜ã€‚
    unique_key = f"import-{item_to_import.provider}-{item_to_import.mediaId}"
    if item_to_import.currentEpisodeIndex is not None:
        unique_key += f"-ep{item_to_import.currentEpisodeIndex}"

    try:
        task_coro = lambda session, cb: tasks.generic_import_task(
            provider=item_to_import.provider,
            mediaId=item_to_import.mediaId,
            animeTitle=item_to_import.title,
            # ä¿®æ­£ï¼šä¼ é€’ä»æœç´¢ç»“æœä¸­è·å–çš„å¹´ä»½å’Œæµ·æŠ¥URL
            mediaType=item_to_import.type,
            season=item_to_import.season,
            year=item_to_import.year,
            currentEpisodeIndex=item_to_import.currentEpisodeIndex,
            imageUrl=item_to_import.imageUrl,
            doubanId=payload.doubanId,
            config_manager=config_manager,
            metadata_manager=metadata_manager, tmdbId=payload.tmdbId, imdbId=payload.imdbId,
            tvdbId=payload.tvdbId, bangumiId=payload.bangumiId,
            progress_callback=cb, session=session, manager=manager, task_manager=task_manager,
            rate_limiter=rate_limiter, title_recognition_manager=title_recognition_manager
        )
        task_id, _ = await task_manager.submit_task(task_coro, task_title, unique_key=unique_key)
        return {"message": "å¯¼å…¥ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"æäº¤ç›´æ¥å¯¼å…¥ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æäº¤ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")

@router.get("/episodes", response_model=List[models.ProviderEpisodeInfo], summary="è·å–æœç´¢ç»“æœçš„åˆ†é›†åˆ—è¡¨")
async def get_episodes(
    searchId: str = Query(..., description="æ¥è‡ª/searchæ¥å£çš„searchId"),
    result_index: int = Query(..., ge=0, description="è¦è·å–åˆ†é›†çš„ç»“æœçš„ç´¢å¼•"),
    session: AsyncSession = Depends(get_db_session),
    manager: ScraperManager = Depends(get_scraper_manager),
):
    """
    ### åŠŸèƒ½
    åœ¨æ‰§è¡Œ`/search`åï¼Œè·å–æŒ‡å®šæœç´¢ç»“æœçš„å®Œæ•´åˆ†é›†åˆ—è¡¨ã€‚

    ### å·¥ä½œæµç¨‹
    æ­¤æ¥å£ä¸»è¦ç”¨äºâ€œç¼–è¾‘åå¯¼å…¥â€çš„åœºæ™¯ã€‚æ‚¨å¯ä»¥å…ˆè·å–åŸå§‹çš„åˆ†é›†åˆ—è¡¨ï¼Œåœ¨æ‚¨çš„å®¢æˆ·ç«¯è¿›è¡Œä¿®æ”¹ï¼ˆä¾‹å¦‚ï¼Œåˆ é™¤é¢„å‘Šã€è°ƒæ•´é¡ºåºï¼‰ï¼Œç„¶åå†é€šè¿‡`/import/edited`æ¥å£æäº¤ä¿®æ”¹åçš„åˆ—è¡¨è¿›è¡Œå¯¼å…¥ã€‚
    """
    cache_key = f"control_search_{searchId}"
    cached_results_raw = await crud.get_cache(session, cache_key)
    
    if cached_results_raw is None:
        raise HTTPException(status_code=404, detail="æœç´¢ä¼šè¯å·²è¿‡æœŸæˆ–æ— æ•ˆï¼Œè¯·é‡æ–°æœç´¢ã€‚")
    
    try:
        cached_results = [models.ProviderSearchInfo.model_validate(r) for r in cached_results_raw]
    except Exception:
        raise HTTPException(status_code=500, detail="æ— æ³•è§£æç¼“å­˜çš„æœç´¢ç»“æœã€‚")

    if not (0 <= result_index < len(cached_results)):
        raise HTTPException(status_code=400, detail="æä¾›çš„ result_index æ— æ•ˆã€‚")
        
    item_to_fetch = cached_results[result_index]
    
    scraper = manager.get_scraper(item_to_fetch.provider)
    try:
        return await scraper.get_episodes(item_to_fetch.mediaId, db_media_type=item_to_fetch.type)
    except httpx.RequestError as e:
        logger.error(f"è·å–åˆ†é›†åˆ—è¡¨æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯ (provider={item_to_fetch.provider}, media_id={item_to_fetch.mediaId}): {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=f"ä» {item_to_fetch.provider} è·å–åˆ†é›†åˆ—è¡¨æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")

@router.post("/import/edited", status_code=status.HTTP_202_ACCEPTED, summary="å¯¼å…¥ç¼–è¾‘åçš„åˆ†é›†åˆ—è¡¨", response_model=ControlTaskResponse)
async def edited_import(
    payload: ControlEditedImportRequest,
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter),
    config_manager: ConfigManager = Depends(get_config_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    title_recognition_manager = Depends(get_title_recognition_manager)
):
    """
    ### åŠŸèƒ½
    å¯¼å…¥ä¸€ä¸ªç»è¿‡ç”¨æˆ·ç¼–è¾‘å’Œè°ƒæ•´çš„åˆ†é›†åˆ—è¡¨ã€‚

    ### å·¥ä½œæµç¨‹
    è¿™æ˜¯æœ€çµæ´»çš„å¯¼å…¥æ–¹å¼ã€‚å®ƒå…è®¸æ‚¨å®Œå…¨æ§åˆ¶è¦å¯¼å…¥çš„åˆ†é›†ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€é¡ºåºç­‰ã€‚æ‚¨å¯ä»¥åœ¨è¯·æ±‚ä¸­è¦†ç›–ä½œå“æ ‡é¢˜å’Œé™„åŠ å…ƒæ•°æ®IDã€‚
    """
    cache_key = f"control_search_{payload.searchId}"
    cached_results_raw = await crud.get_cache(session, cache_key)
    
    if cached_results_raw is None:
        raise HTTPException(status_code=404, detail="æœç´¢ä¼šè¯å·²è¿‡æœŸæˆ–æ— æ•ˆï¼Œè¯·é‡æ–°æœç´¢ã€‚")
    
    try:
        cached_results = [models.ProviderSearchInfo.model_validate(r) for r in cached_results_raw]
    except Exception:
        raise HTTPException(status_code=500, detail="æ— æ³•è§£æç¼“å­˜çš„æœç´¢ç»“æœã€‚")

    if not (0 <= payload.resultIndex < len(cached_results)):
        raise HTTPException(status_code=400, detail="æä¾›çš„ result_index æ— æ•ˆã€‚")
        
    item_to_import = cached_results[payload.resultIndex]

    # å…³é”®ä¿®å¤ï¼šæ¢å¤å¹¶å®Œå–„åœ¨ä»»åŠ¡æäº¤å‰çš„é‡å¤æ£€æŸ¥ã€‚
    # å¯¹äºç¼–è¾‘åå¯¼å…¥ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥æ¯ä¸ªå•é›†æ˜¯å¦å·²å­˜åœ¨ï¼ˆå¿…é¡»æ˜¯ç›¸åŒæ•°æ®æºï¼‰
    # æ£€æŸ¥æ•°æ®æºæ˜¯å¦å·²å­˜åœ¨
    source_exists = await crud.check_source_exists_by_media_id(session, item_to_import.provider, item_to_import.mediaId)

    if source_exists:
        # æ•°æ®æºå·²å­˜åœ¨ï¼Œæ£€æŸ¥æ¯ä¸ªè¦å¯¼å…¥çš„å•é›†æ˜¯å¦å·²æœ‰å¼¹å¹•ï¼ˆå¿…é¡»æ˜¯ç›¸åŒ provider + media_idï¼‰
        existing_episodes = []
        for episode in payload.episodes:
            # ä½¿ç”¨ç²¾ç¡®æ£€æŸ¥ï¼šprovider + media_id + episode_index
            stmt = select(orm_models.Episode.id).join(
                orm_models.AnimeSource, orm_models.Episode.sourceId == orm_models.AnimeSource.id
            ).where(
                orm_models.AnimeSource.providerName == item_to_import.provider,
                orm_models.AnimeSource.mediaId == item_to_import.mediaId,
                orm_models.Episode.episodeIndex == episode.episodeIndex,
                orm_models.Episode.danmakuFilePath.isnot(None),
                orm_models.Episode.commentCount > 0
            ).limit(1)
            result = await session.execute(stmt)
            if result.scalar_one_or_none() is not None:
                existing_episodes.append(episode.episodeIndex)

        # å¦‚æœæ‰€æœ‰é›†éƒ½å·²å­˜åœ¨ï¼Œåˆ™é˜»æ­¢å¯¼å…¥
        if len(existing_episodes) == len(payload.episodes):
            episode_list = ", ".join(map(str, existing_episodes))
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT,
                detail=f"æ‰€æœ‰è¦å¯¼å…¥çš„åˆ†é›† ({episode_list}) éƒ½å·²åœ¨è¯¥æ•°æ®æºï¼ˆ{item_to_import.provider}ï¼‰ä¸­å­˜åœ¨å¼¹å¹•"
            )
        # å¦‚æœéƒ¨åˆ†é›†å·²å­˜åœ¨ï¼Œç»™å‡ºè­¦å‘Šä½†å…è®¸å¯¼å…¥
        elif existing_episodes:
            episode_list = ", ".join(map(str, existing_episodes))
            logger.warning(f"å¤–éƒ¨APIç¼–è¾‘å¯¼å…¥: åˆ†é›† {episode_list} å·²åœ¨è¯¥æ•°æ®æºï¼ˆ{item_to_import.provider}ï¼‰ä¸­å­˜åœ¨ï¼Œå°†è·³è¿‡è¿™äº›åˆ†é›†")


    # æ„å»ºç¼–è¾‘å¯¼å…¥è¯·æ±‚
    edited_request = models.EditedImportRequest(
        provider=item_to_import.provider,
        mediaId=item_to_import.mediaId,
        # ä¿®æ­£ï¼šä¼ é€’ä»æœç´¢ç»“æœä¸­è·å–çš„å¹´ä»½å’Œæµ·æŠ¥URL
        animeTitle=payload.title or item_to_import.title,
        mediaType=item_to_import.type,
        season=item_to_import.season,
        year=item_to_import.year,
        episodes=payload.episodes,
        tmdbId=payload.tmdbId,
        imdbId=payload.imdbId,
        tvdbId=payload.tvdbId,
        doubanId=payload.doubanId,
        bangumiId=payload.bangumiId,
        tmdbEpisodeGroupId=payload.tmdbEpisodeGroupId,
        imageUrl=item_to_import.imageUrl
    )

    # ä¿®æ­£ï¼šä¸ºä»»åŠ¡æ ‡é¢˜æ·»åŠ å­£/é›†ä¿¡æ¯ï¼Œä»¥ç¡®ä¿å…¶å”¯ä¸€æ€§ï¼Œé˜²æ­¢å› ä»»åŠ¡åé‡å¤è€Œæäº¤å¤±è´¥ã€‚
    title_parts = [f"å¤–éƒ¨APIç¼–è¾‘åå¯¼å…¥: {edited_request.animeTitle} ({edited_request.provider})"]
    if edited_request.season is not None:
        title_parts.append(f"S{edited_request.season:02d}")
    if payload.episodes:
        episode_indices = sorted([ep.episodeIndex for ep in payload.episodes])
        if len(episode_indices) == 1:
            title_parts.append(f"E{episode_indices[0]:02d}")
        else:
            title_parts.append(f"({len(episode_indices)}é›†)")
    task_title = " ".join(title_parts)

    # ä¿®æ­£ï¼šä½¿ unique_key æ›´å…·ä½“ï¼Œä»¥å…è®¸å¯¹åŒä¸€åª’ä½“çš„ä¸åŒåˆ†é›†åˆ—è¡¨è¿›è¡Œæ’é˜Ÿå¯¼å…¥ã€‚
    # è¿™è§£å†³äº†åœ¨ä¸€æ¬¡å¯¼å…¥å®Œæˆåï¼Œæ— æ³•ç«‹å³ä¸ºåŒä¸€åª’ä½“æäº¤å¦ä¸€æ¬¡å¯¼å…¥çš„é—®é¢˜ã€‚
    episode_indices_str = ",".join(sorted([str(ep.episodeIndex) for ep in payload.episodes]))
    episodes_hash = hashlib.md5(episode_indices_str.encode('utf-8')).hexdigest()[:8]
    unique_key = f"import-{edited_request.provider}-{edited_request.mediaId}-{episodes_hash}"

    try:
        task_coro = lambda session, cb: tasks.edited_import_task(
            request_data=edited_request, progress_callback=cb, session=session,
            config_manager=config_manager, manager=manager, rate_limiter=rate_limiter,
            metadata_manager=metadata_manager, title_recognition_manager=title_recognition_manager
        )
        task_id, _ = await task_manager.submit_task(task_coro, task_title, unique_key=unique_key)
        return {"message": "ç¼–è¾‘åå¯¼å…¥ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except HTTPException as e:
        # ä¿®æ­£ï¼šæ•è·æ­£ç¡®çš„ HTTPException å¹¶é‡æ–°æŠ›å‡º
        raise e
    except Exception as e:
        # æ•è·å…¶ä»–æ„å¤–é”™è¯¯
        logger.error(f"æäº¤ç¼–è¾‘åå¯¼å…¥ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æäº¤ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")

@router.post("/import/xml", status_code=status.HTTP_202_ACCEPTED, summary="ä»XML/æ–‡æœ¬å¯¼å…¥å¼¹å¹•", response_model=ControlTaskResponse)
async def xml_import(
    payload: ControlXmlImportRequest,
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter)
):
    """
    ### åŠŸèƒ½
    ä¸ºä¸€ä¸ªå·²å­˜åœ¨çš„æ•°æ®æºï¼Œå¯¼å…¥æŒ‡å®šé›†æ•°çš„å¼¹å¹•ï¼ˆé€šè¿‡XMLæˆ–çº¯æ–‡æœ¬å†…å®¹ï¼‰ã€‚
    ### å·¥ä½œæµç¨‹
    1.  æ‚¨éœ€è¦æä¾›ä¸€ä¸ªå·²å­˜åœ¨äºç³»ç»Ÿä¸­çš„ `sourceId`ã€‚
    2.  æä¾›è¦å¯¼å…¥çš„ `episodeIndex` (é›†æ•°) å’Œå¼¹å¹• `content`ã€‚
    3.  ç³»ç»Ÿä¼šä¸ºè¯¥æ•°æ®æºåˆ›å»ºä¸€ä¸ªåå°ä»»åŠ¡ï¼Œå°†å†…å®¹è§£æå¹¶å¯¼å…¥åˆ°æŒ‡å®šåˆ†é›†ã€‚
    
    æ­¤æ¥å£éå¸¸é€‚åˆç”¨äºå¯¹å·²æœ‰çš„æ•°æ®æºè¿›è¡Œå•é›†è¡¥å…¨æˆ–æ›´æ–°ã€‚
    """
    source_info = await crud.get_anime_source_info(session, payload.sourceId)
    if not source_info:
        raise HTTPException(status_code=404, detail=f"æ•°æ®æº ID: {payload.sourceId} æœªæ‰¾åˆ°ã€‚")

    # This type of import should only be for 'custom' provider
    if source_info["providerName"] != 'custom':
        raise HTTPException(status_code=400, detail=f"XML/æ–‡æœ¬å¯¼å…¥ä»…æ”¯æŒ 'custom' ç±»å‹çš„æºï¼Œä½†ç›®æ ‡æºç±»å‹ä¸º '{source_info['providerName']}'ã€‚")

    anime_id = source_info["animeId"]
    anime_title = source_info["title"]

    task_title = f"å¤–éƒ¨API XMLå¯¼å…¥: {anime_title} - ç¬¬ {payload.episodeIndex} é›†"
    unique_key = f"manual-import-{payload.sourceId}-{payload.episodeIndex}"

    try:
        task_coro = lambda s, cb: tasks.manual_import_task(
            sourceId=payload.sourceId,
            animeId=anime_id,
            title=payload.title,
            episodeIndex=payload.episodeIndex,
            content=payload.content,
            providerName='custom',
            progress_callback=cb,
            session=s,
            manager=manager,
            rate_limiter=rate_limiter
        )
        task_id, _ = await task_manager.submit_task(task_coro, task_title, unique_key=unique_key)
        return {"message": "XMLå¯¼å…¥ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"æäº¤XMLå¯¼å…¥ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æäº¤ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")


@router.post("/import/url", status_code=status.HTTP_202_ACCEPTED, summary="ä»URLå¯¼å…¥", response_model=ControlTaskResponse)
async def url_import(
    payload: ControlUrlImportRequest,
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter)
):
    """
    ### åŠŸèƒ½
    ä¸ºä¸€ä¸ªå·²å­˜åœ¨çš„æ•°æ®æºï¼Œå¯¼å…¥æŒ‡å®šé›†æ•°çš„å¼¹å¹•ã€‚
    ### å·¥ä½œæµç¨‹
    1.  æ‚¨éœ€è¦æä¾›ä¸€ä¸ªå·²å­˜åœ¨äºç³»ç»Ÿä¸­çš„ `sourceId`ã€‚
    2.  æä¾›è¦å¯¼å…¥çš„ `episodeIndex` (é›†æ•°) å’ŒåŒ…å«å¼¹å¹•çš„è§†é¢‘é¡µé¢ `url`ã€‚
    3.  ç³»ç»Ÿä¼šä¸ºè¯¥æ•°æ®æºåˆ›å»ºä¸€ä¸ªåå°ä»»åŠ¡ï¼Œç²¾ç¡®åœ°è·å–å¹¶å¯¼å…¥æŒ‡å®šé›†æ•°çš„å¼¹å¹•ã€‚
    
    æ­¤æ¥å£éå¸¸é€‚åˆç”¨äºå¯¹å·²æœ‰çš„æ•°æ®æºè¿›è¡Œå•é›†è¡¥å…¨æˆ–æ›´æ–°ã€‚
    """
    source_info = await crud.get_anime_source_info(session, payload.sourceId)
    if not source_info:
        raise HTTPException(status_code=404, detail=f"æ•°æ®æº ID: {payload.sourceId} æœªæ‰¾åˆ°ã€‚")

    provider_name = source_info["providerName"]
    anime_id = source_info["animeId"]
    anime_title = source_info["title"]

    scraper = manager.get_scraper(provider_name)
    if not hasattr(scraper, 'get_info_from_url'):
        raise HTTPException(status_code=400, detail=f"æ•°æ®æº '{provider_name}' ä¸æ”¯æŒä»URLå¯¼å…¥ã€‚")

    task_title = f"å¤–éƒ¨API URLå¯¼å…¥: {anime_title} - ç¬¬ {payload.episodeIndex} é›† ({provider_name})"
    unique_key = f"manual-import-{payload.sourceId}-{payload.episodeIndex}"

    try:
        task_coro = lambda s, cb: tasks.manual_import_task(
            sourceId=payload.sourceId,
            animeId=anime_id,
            title=payload.title,
            episodeIndex=payload.episodeIndex,
            content=payload.url,
            providerName=provider_name,
            progress_callback=cb,
            session=s,
            manager=manager,
            rate_limiter=rate_limiter
        )
        task_id, _ = await task_manager.submit_task(task_coro, task_title, unique_key=unique_key)
        return {"message": "URLå¯¼å…¥ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"æäº¤URLå¯¼å…¥ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æäº¤ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")

# --- å…ƒä¿¡æ¯æœç´¢ ---

@router.get("/metadata/search", response_model=ControlMetadataSearchResponse, summary="æŸ¥æ‰¾å…ƒæ•°æ®ä¿¡æ¯")
async def search_metadata_source(
    provider: str = Query(..., description="è¦æŸ¥è¯¢çš„å…ƒæ•°æ®æºï¼Œä¾‹å¦‚: 'tmdb', 'bangumi'ã€‚"),
    keyword: Optional[str] = Query(None, description="æŒ‰å…³é”®è¯æœç´¢ã€‚'keyword' å’Œ 'id' å¿…é¡»æä¾›ä¸€ä¸ªã€‚"),
    id: Optional[str] = Query(None, description="æŒ‰IDç²¾ç¡®æŸ¥æ‰¾ã€‚'keyword' å’Œ 'id' å¿…é¡»æä¾›ä¸€ä¸ªã€‚"),
    mediaType: Optional[AutoImportMediaType] = Query(None, description="åª’ä½“ç±»å‹ã€‚å¯é€‰å€¼: 'tv_series', 'movie'ã€‚"),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager)
):
    """
    ### åŠŸèƒ½
    ä»æŒ‡å®šçš„å…ƒæ•°æ®æºï¼ˆå¦‚TMDB, Bangumiï¼‰ä¸­æŸ¥æ‰¾åª’ä½“ä¿¡æ¯ã€‚

    ### å·¥ä½œæµç¨‹
    1.  æä¾› `provider` æ¥æŒ‡å®šè¦æŸ¥è¯¢çš„æºã€‚
    2.  æä¾› `keyword` æˆ– `id` ä¸­çš„ä¸€ä¸ªæ¥è¿›è¡Œæœç´¢ã€‚
    3.  å¯¹äºæŸäº›æºï¼ˆå¦‚TMDBï¼‰ï¼Œå¯èƒ½éœ€è¦æä¾› `mediaType` æ¥åŒºåˆ†ç”µè§†å‰§å’Œç”µå½±ã€‚

    ### è¿”å›
    è¿”å›ä¸€ä¸ªåŒ…å«å…ƒæ•°æ®è¯¦æƒ…çš„åˆ—è¡¨ã€‚å¦‚æœé€šè¿‡IDæŸ¥æ‰¾ä¸”æˆåŠŸï¼Œåˆ—è¡¨ä¸­å°†åªæœ‰ä¸€ä¸ªå…ƒç´ ã€‚
    """
    if not keyword and not id:
        raise HTTPException(status_code=400, detail="å¿…é¡»æä¾› 'keyword' æˆ– 'id' å‚æ•°ä¹‹ä¸€ã€‚")
    if keyword and id:
        raise HTTPException(status_code=400, detail="ä¸èƒ½åŒæ—¶æä¾› 'keyword' å’Œ 'id' å‚æ•°ã€‚")

    # --- æ–°å¢ï¼šå°†é€šç”¨åª’ä½“ç±»å‹æ˜ å°„åˆ°ç‰¹å®šäºæä¾›å•†çš„ç±»å‹ ---
    provider_media_type: Optional[str] = None
    if mediaType:
        if provider == 'tmdb':
            provider_media_type = 'tv' if mediaType == AutoImportMediaType.TV_SERIES else 'movie'
        elif provider == 'tvdb':
            provider_media_type = 'series' if mediaType == AutoImportMediaType.TV_SERIES else 'movies'
        # å¯¹äºå…¶ä»–æºï¼Œå¦‚æœå®ƒä»¬ä½¿ç”¨ 'tv_series'/'movie'ï¼Œåˆ™æ— éœ€æ˜ å°„
        # å¦åˆ™ï¼Œéœ€è¦åœ¨æ­¤å¤„æ·»åŠ æ›´å¤š case
    # --- æ˜ å°„ç»“æŸ ---

    # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç”¨æˆ·ï¼Œå› ä¸ºå…ƒæ•°æ®ç®¡ç†å™¨çš„æ ¸å¿ƒæ–¹æ³•éœ€è¦å®ƒ
    user = models.User(id=0, username="control_api")
    results = []

    try:
        if id:
            details = await metadata_manager.get_details(provider, id, user, mediaType=provider_media_type)
            if details:
                results.append(details)
        elif keyword:
            results = await metadata_manager.search(provider, keyword, user, mediaType=provider_media_type)
    except HTTPException as e:
        raise e # é‡æ–°æŠ›å‡ºå·²çŸ¥çš„HTTPå¼‚å¸¸
    except Exception as e:
        logger.error(f"ä»å…ƒæ•°æ®æº '{provider}' æœç´¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä»å…ƒæ•°æ®æº '{provider}' æœç´¢æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯ã€‚")

    return ControlMetadataSearchResponse(results=results)

# --- åª’ä½“åº“ç®¡ç† ---

@router.get("/library", response_model=List[models.LibraryAnimeInfo], summary="è·å–åª’ä½“åº“åˆ—è¡¨")
async def get_library(session: AsyncSession = Depends(get_db_session)):
    """è·å–å½“å‰å¼¹å¹•åº“ä¸­æ‰€æœ‰å·²æ”¶å½•çš„ä½œå“åˆ—è¡¨ã€‚"""
    paginated_results = await crud.get_library_anime(session)
    return [models.LibraryAnimeInfo.model_validate(item) for item in paginated_results["list"]]

@router.get("/library/search", response_model=List[models.LibraryAnimeInfo], summary="æœç´¢åª’ä½“åº“")
async def search_library(
    keyword: str = Query(..., description="æœç´¢å…³é”®è¯"),
    session: AsyncSession = Depends(get_db_session)
):
    """æ ¹æ®å…³é”®è¯æœç´¢å¼¹å¹•åº“ä¸­å·²æ”¶å½•çš„ä½œå“ã€‚"""
    paginated_results = await crud.get_library_anime(session, keyword=keyword)
    return [models.LibraryAnimeInfo.model_validate(item) for item in paginated_results["list"]]

@router.post("/library/anime", response_model=ControlAnimeDetailsResponse, status_code=status.HTTP_201_CREATED, summary="è‡ªå®šä¹‰åˆ›å»ºå½±è§†æ¡ç›®")
async def create_anime_entry(
    payload: ControlAnimeCreateRequest,
    session: AsyncSession = Depends(get_db_session),
    title_recognition_manager = Depends(get_title_recognition_manager)
):
    """
    ### åŠŸèƒ½
    åœ¨æ•°æ®åº“ä¸­æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªæ–°çš„å½±è§†ä½œå“æ¡ç›®ã€‚
    ### å·¥ä½œæµç¨‹
    1.  æ¥æ”¶ä½œå“çš„æ ‡é¢˜ã€ç±»å‹ã€å­£åº¦ç­‰åŸºæœ¬ä¿¡æ¯ã€‚
    2.  ï¼ˆå¯é€‰ï¼‰æ¥æ”¶TMDBã€Bangumiç­‰å…ƒæ•°æ®IDå’Œå…¶ä»–åˆ«åã€‚
    3.  åœ¨æ•°æ®åº“ä¸­åˆ›å»ºå¯¹åº”çš„ `anime`, `anime_metadata`, `anime_aliases` è®°å½•ã€‚
    4.  è¿”å›æ–°åˆ›å»ºçš„ä½œå“çš„å®Œæ•´ä¿¡æ¯ã€‚
    """
    # Check for duplicates first
    # ä¿®æ­£ï¼šä¸ºéç”µè§†å‰§ç±»å‹ä½¿ç”¨é»˜è®¤å­£åº¦1è¿›è¡Œé‡å¤æ£€æŸ¥
    season_for_check = payload.season if payload.type == AutoImportMediaType.TV_SERIES else 1
    existing_anime = await crud.find_anime_by_title_season_year(
        session, payload.title, season_for_check, payload.year, title_recognition_manager
    )
    if existing_anime:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail="å·²å­˜åœ¨åŒååŒå­£åº¦çš„ä½œå“ã€‚"
        )
    
    # ä¿®æ­£ï¼šä¸ºéç”µè§†å‰§ç±»å‹ä½¿ç”¨é»˜è®¤å­£åº¦1è¿›è¡Œåˆ›å»º
    season_for_create = payload.season if payload.type == AutoImportMediaType.TV_SERIES else 1
    new_anime_id = await crud.get_or_create_anime(
        session,
        title=payload.title,
        media_type=payload.type.value,
        season=season_for_create,
        year=payload.year,
        image_url=None, # No image URL when creating manually
        local_image_path=None,
        title_recognition_manager=title_recognition_manager
    )

    await crud.update_metadata_if_empty(
        session, new_anime_id,
        tmdb_id=payload.tmdbId, imdb_id=payload.imdbId, tvdb_id=payload.tvdbId,
        douban_id=payload.doubanId, bangumi_id=payload.bangumiId
    )
    
    await crud.update_anime_aliases(session, new_anime_id, payload)
    await session.commit()

    new_details = await crud.get_anime_full_details(session, new_anime_id)
    if not new_details:
        raise HTTPException(status_code=500, detail="åˆ›å»ºä½œå“åæ— æ³•è·å–å…¶è¯¦ç»†ä¿¡æ¯ã€‚")

    return ControlAnimeDetailsResponse.model_validate(new_details)

@router.get("/library/anime/{animeId}", response_model=ControlAnimeDetailsResponse, summary="è·å–ä½œå“è¯¦æƒ…")
async def get_anime_details(animeId: int, session: AsyncSession = Depends(get_db_session)):
    """è·å–å¼¹å¹•åº“ä¸­å•ä¸ªä½œå“çš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ‰€æœ‰å…ƒæ•°æ®IDå’Œåˆ«åã€‚"""
    details = await crud.get_anime_full_details(session, animeId)
    if not details: raise HTTPException(404, "ä½œå“æœªæ‰¾åˆ°")
    return ControlAnimeDetailsResponse.model_validate(details)

@router.get("/library/anime/{animeId}/sources", response_model=List[models.SourceInfo], summary="è·å–ä½œå“çš„æ‰€æœ‰æ•°æ®æº")
async def get_anime_sources(animeId: int, session: AsyncSession = Depends(get_db_session)):
    """è·å–æŒ‡å®šä½œå“å·²å…³è”çš„æ‰€æœ‰å¼¹å¹•æºåˆ—è¡¨ã€‚"""
    # First, check if the anime exists to provide a proper 404.
    anime_exists = await crud.get_anime_full_details(session, animeId)
    if not anime_exists:
        raise HTTPException(status_code=404, detail="ä½œå“æœªæ‰¾åˆ°")
    return await crud.get_anime_sources(session, animeId)

@router.post("/library/anime/{animeId}/sources", response_model=models.SourceInfo, status_code=status.HTTP_201_CREATED, summary="ä¸ºä½œå“æ·»åŠ æ•°æ®æº")
async def add_source(
    animeId: int,
    payload: models.SourceCreate,
    session: AsyncSession = Depends(get_db_session)
):
    """
    ### åŠŸèƒ½
    ä¸ºä¸€ä¸ªå·²å­˜åœ¨çš„ä½œå“æ‰‹åŠ¨å…³è”ä¸€ä¸ªæ–°çš„æ•°æ®æºã€‚

    ### å·¥ä½œæµç¨‹
    1.  æä¾›ä¸€ä¸ªå·²å­˜åœ¨äºå¼¹å¹•åº“ä¸­çš„ `animeId`ã€‚
    2.  åœ¨è¯·æ±‚ä½“ä¸­æä¾› `providerName` å’Œ `mediaId`ã€‚
    3.  ç³»ç»Ÿä¼šå°†æ­¤æ•°æ®æºå…³è”åˆ°æŒ‡å®šçš„ä½œå“ã€‚

    ### ä½¿ç”¨åœºæ™¯
    -   **æ·»åŠ è‡ªå®šä¹‰æº**: æ‚¨å¯ä»¥ä¸ºä»»ä½•ä½œå“æ·»åŠ ä¸€ä¸ª `custom` ç±»å‹çš„æºï¼Œä»¥ä¾¿åç»­é€šè¿‡ `/import/xml` æ¥å£ä¸ºå…¶ä¸Šä¼ å¼¹å¹•æ–‡ä»¶ã€‚
        -   `providerName`: "custom"
        -   `mediaId`: ä»»æ„å”¯ä¸€çš„å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ `custom_123`ã€‚
    -   **æ‰‹åŠ¨å…³è”åˆ®å‰Šæº**: å¦‚æœè‡ªåŠ¨æœç´¢æœªèƒ½æ‰¾åˆ°æ­£ç¡®çš„ç»“æœï¼Œæ‚¨å¯ä»¥é€šè¿‡æ­¤æ¥å£æ‰‹åŠ¨å°†ä¸€ä¸ªå·²çŸ¥çš„ `providerName` å’Œ `mediaId` å…³è”åˆ°ä½œå“ä¸Šã€‚
    """
    anime = await crud.get_anime_full_details(session, animeId)
    if not anime:
        raise HTTPException(status_code=404, detail="ä½œå“æœªæ‰¾åˆ°")

    try:
        source_id = await crud.link_source_to_anime(session, animeId, payload.providerName, payload.mediaId)
        await session.commit()
        # After committing, fetch the full source info including counts
        all_sources = await crud.get_anime_sources(session, animeId)
        newly_created_source = next((s for s in all_sources if s['sourceId'] == source_id), None)
        if not newly_created_source:
            # This should not happen if creation was successful
            raise HTTPException(status_code=500, detail="åˆ›å»ºæ•°æ®æºåæ— æ³•ç«‹å³è·å–å…¶ä¿¡æ¯ã€‚")
        return models.SourceInfo.model_validate(newly_created_source)
    except exc.IntegrityError:
        await session.rollback()
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="è¯¥æ•°æ®æºå·²å­˜åœ¨äºæ­¤ä½œå“ä¸‹ï¼Œæ— æ³•é‡å¤æ·»åŠ ã€‚")

@router.put("/library/anime/{animeId}", response_model=ControlActionResponse, summary="ç¼–è¾‘ä½œå“ä¿¡æ¯")
async def edit_anime(animeId: int, payload: models.AnimeDetailUpdate, session: AsyncSession = Depends(get_db_session)):
    """æ›´æ–°å¼¹å¹•åº“ä¸­å•ä¸ªä½œå“çš„è¯¦ç»†ä¿¡æ¯ã€‚"""
    if not await crud.update_anime_details(session, animeId, payload):
        raise HTTPException(404, "ä½œå“æœªæ‰¾åˆ°")
    return {"message": "ä½œå“ä¿¡æ¯æ›´æ–°æˆåŠŸã€‚"}

@router.delete("/library/anime/{animeId}", status_code=202, summary="åˆ é™¤ä½œå“", response_model=ControlTaskResponse)
async def delete_anime(animeId: int, session: AsyncSession = Depends(get_db_session), task_manager: TaskManager = Depends(get_task_manager)):
    """æäº¤ä¸€ä¸ªåå°ä»»åŠ¡ï¼Œä»¥åˆ é™¤å¼¹å¹•åº“ä¸­çš„ä¸€ä¸ªä½œå“åŠå…¶æ‰€æœ‰å…³è”çš„æ•°æ®æºã€åˆ†é›†å’Œå¼¹å¹•ã€‚"""
    details = await crud.get_anime_full_details(session, animeId)
    if not details: raise HTTPException(404, "ä½œå“æœªæ‰¾åˆ°")
    try:
        unique_key = f"delete-anime-{animeId}"
        task_id, _ = await task_manager.submit_task(
            lambda s, cb: tasks.delete_anime_task(animeId, s, cb),
            f"å¤–éƒ¨APIåˆ é™¤ä½œå“: {details['title']}",
            unique_key=unique_key, run_immediately=True
        )
        return {"message": "åˆ é™¤ä½œå“ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except ValueError as e:
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(e))

@router.delete("/library/source/{sourceId}", status_code=202, summary="åˆ é™¤æ•°æ®æº", response_model=ControlTaskResponse)
async def delete_source(sourceId: int, session: AsyncSession = Depends(get_db_session), task_manager: TaskManager = Depends(get_task_manager)):
    """æäº¤ä¸€ä¸ªåå°ä»»åŠ¡ï¼Œä»¥åˆ é™¤ä¸€ä¸ªå·²å…³è”çš„æ•°æ®æºåŠå…¶æ‰€æœ‰åˆ†é›†å’Œå¼¹å¹•ã€‚"""
    info = await crud.get_anime_source_info(session, sourceId)
    if not info: raise HTTPException(404, "æ•°æ®æºæœªæ‰¾åˆ°")
    try:
        unique_key = f"delete-source-{sourceId}"
        task_id, _ = await task_manager.submit_task(
            lambda s, cb: tasks.delete_source_task(sourceId, s, cb),
            f"å¤–éƒ¨APIåˆ é™¤æº: {info['title']} ({info['providerName']})",
            unique_key=unique_key, run_immediately=True
        )
        return {"message": "åˆ é™¤æºä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except ValueError as e:
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(e))

@router.put("/library/source/{sourceId}/favorite", response_model=ControlActionResponse, summary="ç²¾ç¡®æ ‡è®°æ•°æ®æº")
async def favorite_source(sourceId: int, session: AsyncSession = Depends(get_db_session)):
    """åˆ‡æ¢æ•°æ®æºçš„â€œç²¾ç¡®æ ‡è®°â€çŠ¶æ€ã€‚ä¸€ä¸ªä½œå“åªèƒ½æœ‰ä¸€ä¸ªç²¾ç¡®æ ‡è®°çš„æºï¼Œå®ƒå°†åœ¨è‡ªåŠ¨åŒ¹é…æ—¶è¢«ä¼˜å…ˆä½¿ç”¨ã€‚"""
    new_status = await crud.toggle_source_favorite_status(session, sourceId)
    if new_status is None:
        raise HTTPException(404, "æ•°æ®æºæœªæ‰¾åˆ°")
    message = "æ•°æ®æºå·²æ ‡è®°ä¸ºç²¾ç¡®ã€‚" if new_status else "æ•°æ®æºå·²å–æ¶ˆç²¾ç¡®æ ‡è®°ã€‚"
    return {"message": message}

@router.get("/library/source/{sourceId}/episodes", response_model=List[models.EpisodeDetail], summary="è·å–æºçš„åˆ†é›†åˆ—è¡¨")
async def get_source_episodes(sourceId: int, session: AsyncSession = Depends(get_db_session)):
    """è·å–æŒ‡å®šæ•°æ®æºä¸‹æ‰€æœ‰å·²æ”¶å½•çš„åˆ†é›†åˆ—è¡¨ã€‚"""
    paginated_result = await crud.get_episodes_for_source(session, sourceId)
    return paginated_result.get("episodes", [])

@router.put("/library/episode/{episodeid}", response_model=ControlActionResponse, summary="ç¼–è¾‘åˆ†é›†ä¿¡æ¯")
async def edit_episode(episodeid: int, payload: models.EpisodeInfoUpdate, session: AsyncSession = Depends(get_db_session)):
    """æ›´æ–°å•ä¸ªåˆ†é›†çš„æ ‡é¢˜ã€é›†æ•°å’Œå®˜æ–¹é“¾æ¥ã€‚"""
    if not await crud.update_episode_info(session, episodeid, payload):
        raise HTTPException(404, "åˆ†é›†æœªæ‰¾åˆ°")
    return {"message": "åˆ†é›†ä¿¡æ¯æ›´æ–°æˆåŠŸã€‚"}

@router.post("/library/episode/{episodeId}/refresh", status_code=202, summary="åˆ·æ–°åˆ†é›†å¼¹å¹•", response_model=ControlTaskResponse)
async def refresh_episode(
    episodeId: int,
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    manager: ScraperManager = Depends(get_scraper_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter),
    config_manager: ConfigManager = Depends(get_config_manager),
):
    """æäº¤ä¸€ä¸ªåå°ä»»åŠ¡ï¼Œä¸ºå•ä¸ªåˆ†é›†é‡æ–°ä»å…¶æºç½‘ç«™è·å–æœ€æ–°çš„å¼¹å¹•ã€‚"""
    info = await crud.get_episode_for_refresh(session, episodeId)
    if not info: raise HTTPException(404, "åˆ†é›†æœªæ‰¾åˆ°")

    # ä½¿ç”¨ unique_key é˜²æ­¢é‡å¤æäº¤ï¼Œä¹Ÿä¾¿äºå¼¹å¹•è·å–æ—¶æ£€æµ‹åˆ·æ–°çŠ¶æ€
    unique_key = f"refresh-episode-{episodeId}"

    task_id, _ = await task_manager.submit_task(
        lambda s, cb: tasks.refresh_episode_task(episodeId, s, manager, rate_limiter, cb, config_manager),
        f"å¤–éƒ¨APIåˆ·æ–°åˆ†é›†: {info['title']}",
        unique_key=unique_key
    )
    return {"message": "åˆ·æ–°åˆ†é›†ä»»åŠ¡å·²æäº¤", "taskId": task_id}

@router.delete("/library/episode/{episodeId}", status_code=202, summary="åˆ é™¤åˆ†é›†", response_model=ControlTaskResponse)
async def delete_episode(episodeId: int, session: AsyncSession = Depends(get_db_session), task_manager: TaskManager = Depends(get_task_manager)):
    """æäº¤ä¸€ä¸ªåå°ä»»åŠ¡ï¼Œä»¥åˆ é™¤å•ä¸ªåˆ†é›†åŠå…¶æ‰€æœ‰å¼¹å¹•ã€‚"""
    info = await crud.get_episode_for_refresh(session, episodeId)
    if not info: raise HTTPException(404, "åˆ†é›†æœªæ‰¾åˆ°")
    try:
        unique_key = f"delete-episode-{episodeId}"
        task_id, _ = await task_manager.submit_task(
            lambda s, cb: tasks.delete_episode_task(episodeId, s, cb),
            f"å¤–éƒ¨APIåˆ é™¤åˆ†é›†: {info['title']}",
            unique_key=unique_key, run_immediately=True
        )
        return {"message": "åˆ é™¤åˆ†é›†ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except ValueError as e:
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(e))

# --- å¼¹å¹•ç®¡ç† ---

@router.get("/danmaku/{episodeId}", response_model=models.CommentResponse, summary="è·å–å¼¹å¹•")
async def get_danmaku(episodeId: int, session: AsyncSession = Depends(get_db_session)):
    """è·å–æŒ‡å®šåˆ†é›†çš„æ‰€æœ‰å¼¹å¹•ï¼Œè¿”å›dandanplayå…¼å®¹æ ¼å¼ã€‚ç”¨äºå¼¹å¹•è°ƒæ•´ï¼Œä¸å—è¾“å‡ºé™åˆ¶æ§åˆ¶ã€‚"""
    if not await crud.check_episode_exists(session, episodeId): raise HTTPException(404, "åˆ†é›†æœªæ‰¾åˆ°")
    comments = await crud.fetch_comments(session, episodeId)
    return models.CommentResponse(count=len(comments), comments=[models.Comment.model_validate(c) for c in comments])

@router.post("/danmaku/{episodeId}", status_code=202, summary="è¦†ç›–å¼¹å¹•", response_model=ControlTaskResponse)
async def overwrite_danmaku(
    episodeId: int,
    payload: models.DanmakuUpdateRequest,
    task_manager: TaskManager = Depends(get_task_manager),
    config_manager: ConfigManager = Depends(get_config_manager)
):
    """æäº¤ä¸€ä¸ªåå°ä»»åŠ¡ï¼Œç”¨è¯·æ±‚ä½“ä¸­æä¾›çš„å¼¹å¹•åˆ—è¡¨å®Œå…¨è¦†ç›–æŒ‡å®šåˆ†é›†çš„ç°æœ‰å¼¹å¹•ã€‚"""
    async def overwrite_task(session: AsyncSession, cb: Callable):
        await cb(10, "æ¸…ç©ºä¸­...")
        await crud.clear_episode_comments(session, episodeId)
        await cb(50, f"æ’å…¥ {len(payload.comments)} æ¡æ–°å¼¹å¹•...")

        comments_to_insert = []
        for c in payload.comments:
            comment_dict = c.model_dump()
            try:
                # ä» 'p' å­—æ®µè§£ææ—¶é—´æˆ³ï¼Œå¹¶æ·»åŠ åˆ°å­—å…¸ä¸­
                timestamp_str = comment_dict['p'].split(',')[0]
                comment_dict['t'] = float(timestamp_str)
            except (IndexError, ValueError):
                comment_dict['t'] = 0.0 # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ™é»˜è®¤ä¸º0
            comments_to_insert.append(comment_dict)

        added = await crud.save_danmaku_for_episode(session, episodeId, comments_to_insert, config_manager)
        raise TaskSuccess(f"å¼¹å¹•è¦†ç›–å®Œæˆï¼Œæ–°å¢ {added} æ¡ã€‚")
    try:
        task_id, _ = await task_manager.submit_task(overwrite_task, f"å¤–éƒ¨APIè¦†ç›–å¼¹å¹• (åˆ†é›†ID: {episodeId})")
        return {"message": "å¼¹å¹•è¦†ç›–ä»»åŠ¡å·²æäº¤", "taskId": task_id}
    except ValueError as e:
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(e))

# --- Token ç®¡ç† ---

@router.get("/tokens", response_model=List[models.ApiTokenInfo], summary="è·å–æ‰€æœ‰Token")
async def get_tokens(session: AsyncSession = Depends(get_db_session)):
    """è·å–æ‰€æœ‰ä¸ºdandanplayå®¢æˆ·ç«¯åˆ›å»ºçš„API Tokenã€‚"""
    tokens = await crud.get_all_api_tokens(session)
    return [models.ApiTokenInfo.model_validate(t) for t in tokens]

@router.post("/tokens", response_model=models.ApiTokenInfo, status_code=201, summary="åˆ›å»ºToken")
async def create_token(payload: models.ApiTokenCreate, session: AsyncSession = Depends(get_db_session)):
    """
    åˆ›å»ºä¸€ä¸ªæ–°çš„API Tokenã€‚

    ### è¯·æ±‚ä½“è¯´æ˜
    - `name`: (string, å¿…éœ€) Tokençš„åç§°ï¼Œç”¨äºåœ¨UIä¸­è¯†åˆ«ã€‚
    - `validityPeriod`: (string, å¿…éœ€) Tokençš„æœ‰æ•ˆæœŸã€‚
        - å¡«å…¥æ•°å­—ï¼ˆå¦‚ "30", "90"ï¼‰è¡¨ç¤ºæœ‰æ•ˆæœŸä¸ºå¤šå°‘å¤©ã€‚
        - å¡«å…¥ "permanent" è¡¨ç¤ºæ°¸ä¹…æœ‰æ•ˆã€‚
    """
    token_str = secrets.token_urlsafe(16)
    try:
        token_id = await crud.create_api_token(session, payload.name, token_str, payload.validityPeriod, payload.dailyCallLimit)
        new_token = await crud.get_api_token_by_id(session, token_id)
        return models.ApiTokenInfo.model_validate(new_token)
    except ValueError as e:
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(e))

@router.get("/tokens/{tokenId}", response_model=models.ApiTokenInfo, summary="è·å–å•ä¸ªTokenè¯¦æƒ…")
async def get_token(tokenId: int, session: AsyncSession = Depends(get_db_session)):
    """è·å–å•ä¸ªAPI Tokençš„è¯¦ç»†ä¿¡æ¯ã€‚"""
    token = await crud.get_api_token_by_id(session, tokenId)
    if not token: raise HTTPException(404, "Tokenæœªæ‰¾åˆ°")
    return models.ApiTokenInfo.model_validate(token)

@router.get("/tokens/{tokenId}/logs", response_model=List[models.TokenAccessLog], summary="è·å–Tokenè®¿é—®æ—¥å¿—")
async def get_token_logs(tokenId: int, session: AsyncSession = Depends(get_db_session)):
    """è·å–å•ä¸ªAPI Tokenæœ€è¿‘çš„è®¿é—®æ—¥å¿—ã€‚"""
    logs = await crud.get_token_access_logs(session, tokenId)
    return [models.TokenAccessLog.model_validate(log) for log in logs]

@router.put("/tokens/{tokenId}/toggle", response_model=ControlActionResponse, summary="å¯ç”¨/ç¦ç”¨Token")
async def toggle_token(tokenId: int, session: AsyncSession = Depends(get_db_session)):
    """åˆ‡æ¢API Tokençš„å¯ç”¨/ç¦ç”¨çŠ¶æ€ã€‚"""
    new_status = await crud.toggle_api_token(session, tokenId)
    if new_status is None:
        raise HTTPException(404, "Tokenæœªæ‰¾åˆ°")
    message = "Token å·²å¯ç”¨ã€‚" if new_status else "Token å·²ç¦ç”¨ã€‚"
    return {"message": message}

class ControlApiTokenUpdate(BaseModel):
    name: str = Field(..., min_length=1, max_length=50, description="Tokençš„æè¿°æ€§åç§°")
    dailyCallLimit: int = Field(..., description="æ¯æ—¥è°ƒç”¨æ¬¡æ•°é™åˆ¶, -1 è¡¨ç¤ºæ— é™")
    validityPeriod: str = Field("custom", description="æ–°çš„æœ‰æ•ˆæœŸ: 'permanent', 'custom', '30d' ç­‰ã€‚'custom' è¡¨ç¤ºä¸æ”¹å˜å½“å‰æœ‰æ•ˆæœŸã€‚")

@router.put("/tokens/{tokenId}", response_model=ControlActionResponse, summary="æ›´æ–°Tokenä¿¡æ¯")
async def update_token(
    tokenId: int,
    payload: ControlApiTokenUpdate,
    session: AsyncSession = Depends(get_db_session)
):
    """æ›´æ–°æŒ‡å®šAPI Tokençš„åç§°ã€æ¯æ—¥è°ƒç”¨ä¸Šé™å’Œæœ‰æ•ˆæœŸã€‚"""
    updated = await crud.update_api_token(
        session,
        token_id=tokenId,
        name=payload.name,
        daily_call_limit=payload.dailyCallLimit,
        validity_period=payload.validityPeriod
    )
    if not updated:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Token not found")
    return {"message": "Tokenä¿¡æ¯æ›´æ–°æˆåŠŸã€‚"}

@router.post("/tokens/{tokenId}/reset", response_model=ControlActionResponse, summary="é‡ç½®Tokenè°ƒç”¨æ¬¡æ•°")
async def reset_token_counter(
    tokenId: int,
    session: AsyncSession = Depends(get_db_session)
):
    """å°†æŒ‡å®šAPI Tokençš„ä»Šæ—¥è°ƒç”¨æ¬¡æ•°é‡ç½®ä¸º0ã€‚"""
    reset_ok = await crud.reset_token_counter(session, tokenId)
    if not reset_ok:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Token not found")
    return {"message": "Tokenè°ƒç”¨æ¬¡æ•°å·²é‡ç½®ä¸º0ã€‚"}


@router.delete("/tokens/{tokenId}", response_model=ControlActionResponse, summary="åˆ é™¤Token")
async def delete_token(tokenId: int, session: AsyncSession = Depends(get_db_session)):
    """åˆ é™¤ä¸€ä¸ªAPI Tokenã€‚"""
    if not await crud.delete_api_token(session, tokenId):
        raise HTTPException(404, "Tokenæœªæ‰¾åˆ°")
    return {"message": "Token åˆ é™¤æˆåŠŸã€‚"}

# --- è®¾ç½®ç®¡ç† ---

@router.get("/settings/danmaku-output", response_model=DanmakuOutputSettings, summary="è·å–å¼¹å¹•è¾“å‡ºè®¾ç½®")
async def get_danmaku_output_settings(session: AsyncSession = Depends(get_db_session)):
    """è·å–å…¨å±€çš„å¼¹å¹•è¾“å‡ºè®¾ç½®ï¼Œå¦‚è¾“å‡ºä¸Šé™å’Œæ˜¯å¦åˆå¹¶è¾“å‡ºã€‚"""
    limit = await crud.get_config_value(session, 'danmakuOutputLimitPerSource', '-1')
    merge_enabled = await crud.get_config_value(session, 'danmakuMergeOutputEnabled', 'false')
    return DanmakuOutputSettings(limit_per_source=int(limit), merge_output_enabled=(merge_enabled.lower() == 'true'))

@router.put("/settings/danmaku-output", response_model=ControlActionResponse, summary="æ›´æ–°å¼¹å¹•è¾“å‡ºè®¾ç½®")
async def update_danmaku_output_settings(payload: DanmakuOutputSettings, session: AsyncSession = Depends(get_db_session), config_manager: ConfigManager = Depends(get_config_manager)):
    """æ›´æ–°å…¨å±€çš„å¼¹å¹•è¾“å‡ºè®¾ç½®ï¼ŒåŒ…æ‹¬è¾“å‡ºä¸Šé™å’Œåˆå¹¶è¾“å‡ºé€‰é¡¹ã€‚"""
    await crud.update_config_value(session, 'danmakuOutputLimitPerSource', str(payload.limitPerSource)) # type: ignore
    await crud.update_config_value(session, 'danmakuMergeOutputEnabled', str(payload.mergeOutputEnabled).lower()) # type: ignore
    config_manager.invalidate('danmakuOutputLimitPerSource')
    config_manager.invalidate('danmakuMergeOutputEnabled')
    return {"message": "å¼¹å¹•è¾“å‡ºè®¾ç½®å·²æ›´æ–°ã€‚"}



# --- ä»»åŠ¡ç®¡ç† ---

@router.get("/tasks", response_model=List[models.TaskInfo], summary="è·å–åå°ä»»åŠ¡åˆ—è¡¨")
async def get_tasks(
    search: Optional[str] = Query(None, description="æŒ‰æ ‡é¢˜æœç´¢"),
    status: str = Query("all", description="æŒ‰çŠ¶æ€è¿‡æ»¤: all, in_progress, completed"),
    session: AsyncSession = Depends(get_db_session),
):
    """è·å–åå°ä»»åŠ¡çš„åˆ—è¡¨å’ŒçŠ¶æ€ï¼Œæ”¯æŒæŒ‰æ ‡é¢˜æœç´¢å’ŒæŒ‰çŠ¶æ€è¿‡æ»¤ã€‚"""
    # ä¿®æ­£ï¼šä¸º get_tasks_from_history æä¾›æ‰€æœ‰å¿…éœ€å‚æ•°ï¼ŒåŒ…æ‹¬ queue_type_filter
    paginated_result = await crud.get_tasks_from_history(session, search, status, queue_type_filter="all", page=1, page_size=1000)
    return [models.TaskInfo.model_validate(t) for t in paginated_result["list"]]

@router.get("/tasks/{taskId}", response_model=models.TaskInfo, summary="è·å–å•ä¸ªä»»åŠ¡çŠ¶æ€")
async def get_task_status(
    taskId: str,
    session: AsyncSession = Depends(get_db_session)
):
    """è·å–å•ä¸ªåå°ä»»åŠ¡çš„è¯¦ç»†çŠ¶æ€ã€‚"""
    task_details = await crud.get_task_details_from_history(session, taskId)
    if not task_details:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="ä»»åŠ¡æœªæ‰¾åˆ°ã€‚")
    return models.TaskInfo.model_validate(task_details)

@router.delete("/tasks/{taskId}", response_model=ControlActionResponse, summary="åˆ é™¤ä¸€ä¸ªå†å²ä»»åŠ¡")
async def delete_task(
    taskId: str,
    force: bool = False,
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
):
    """
    ### åŠŸèƒ½
    åˆ é™¤ä¸€ä¸ªåå°ä»»åŠ¡ã€‚
    - **æ’é˜Ÿä¸­**: ä»é˜Ÿåˆ—ä¸­ç§»é™¤ã€‚
    - **è¿è¡Œä¸­/å·²æš‚åœ**: å°è¯•ä¸­æ­¢ä»»åŠ¡ï¼Œç„¶ååˆ é™¤ã€‚
    - **å·²å®Œæˆ/å¤±è´¥**: ä»å†å²è®°å½•ä¸­åˆ é™¤ã€‚
    - **force=true**: å¼ºåˆ¶åˆ é™¤ï¼Œè·³è¿‡ä¸­æ­¢é€»è¾‘ç›´æ¥åˆ é™¤å†å²è®°å½•ã€‚
    """
    task = await crud.get_task_from_history_by_id(session, taskId)
    if not task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="ä»»åŠ¡æœªæ‰¾åˆ°ã€‚")

    task_status = task['status']

    if force:
        # å¼ºåˆ¶åˆ é™¤æ¨¡å¼ï¼šä½¿ç”¨SQLç›´æ¥åˆ é™¤ï¼Œç»•è¿‡å¯èƒ½çš„é”å®šé—®é¢˜
        logger.info(f"å¼ºåˆ¶åˆ é™¤ä»»åŠ¡ {taskId}ï¼ŒçŠ¶æ€: {task_status}")
        if await crud.force_delete_task_from_history(session, taskId):
            return {"message": f"å¼ºåˆ¶åˆ é™¤ä»»åŠ¡ {taskId} æˆåŠŸã€‚"}
        else:
            return {"message": "å¼ºåˆ¶åˆ é™¤å¤±è´¥ï¼Œä»»åŠ¡å¯èƒ½å·²è¢«å¤„ç†ã€‚"}

    # æ­£å¸¸åˆ é™¤æ¨¡å¼
    if task_status == TaskStatus.PENDING:
        if await task_manager.cancel_pending_task(taskId):
            logger.info(f"å·²ä»é˜Ÿåˆ—ä¸­å–æ¶ˆå¾…å¤„ç†ä»»åŠ¡ {taskId}ã€‚")
    elif task_status in [TaskStatus.RUNNING, TaskStatus.PAUSED]:
        if await task_manager.abort_current_task(taskId):
            logger.info(f"å·²å‘é€ä¸­æ­¢ä¿¡å·åˆ°ä»»åŠ¡ {taskId}ã€‚")

    if await crud.delete_task_from_history(session, taskId):
        return {"message": f"åˆ é™¤ä»»åŠ¡ {taskId} çš„è¯·æ±‚å·²å¤„ç†ã€‚"}
    else:
        return {"message": "ä»»åŠ¡å¯èƒ½å·²è¢«å¤„ç†æˆ–ä¸å­˜åœ¨äºå†å²è®°å½•ä¸­ã€‚"}

@router.post("/tasks/{taskId}/abort", response_model=ControlActionResponse, summary="ä¸­æ­¢æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡")
async def abort_task(
    taskId: str,
    force: bool = False,
    task_manager: TaskManager = Depends(get_task_manager),
    session: AsyncSession = Depends(get_db_session)
):
    """
    å°è¯•ä¸­æ­¢ä¸€ä¸ªå½“å‰æ­£åœ¨è¿è¡Œæˆ–å·²æš‚åœçš„ä»»åŠ¡ã€‚
    - force=false: æ­£å¸¸ä¸­æ­¢ï¼Œå‘ä»»åŠ¡å‘é€å–æ¶ˆä¿¡å·
    - force=true: å¼ºåˆ¶ä¸­æ­¢ï¼Œç›´æ¥å°†ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥çŠ¶æ€
    """
    if force:
        # å¼ºåˆ¶ä¸­æ­¢ï¼šå…ˆå°è¯•å–æ¶ˆåç¨‹ï¼Œå†å°†ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥
        from . import crud
        task = await crud.get_task_from_history_by_id(session, taskId)
        if not task:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="ä»»åŠ¡ä¸å­˜åœ¨")

        # å…ˆå°è¯•å–æ¶ˆæ­£åœ¨è¿è¡Œçš„åç¨‹ï¼ˆå¦‚æœä»»åŠ¡æ­£åœ¨è¿è¡Œï¼‰
        # è¿™æ˜¯å…³é”®ï¼šç¡®ä¿ worker ä¸ä¼šç»§ç»­ç­‰å¾…è¢«ç»ˆæ­¢çš„ä»»åŠ¡
        await task_manager.abort_current_task(taskId)

        # ç„¶åæ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸ºå¤±è´¥
        success = await crud.force_fail_task(session, taskId)
        if success:
            return {"message": "ä»»åŠ¡å·²å¼ºåˆ¶æ ‡è®°ä¸ºå¤±è´¥çŠ¶æ€ã€‚"}
        else:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="å¼ºåˆ¶ä¸­æ­¢ä»»åŠ¡å¤±è´¥")
    else:
        # æ­£å¸¸ä¸­æ­¢
        if not await task_manager.abort_current_task(taskId):
            raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="ä¸­æ­¢ä»»åŠ¡å¤±è´¥ï¼Œå¯èƒ½ä»»åŠ¡å·²å®Œæˆæˆ–ä¸æ˜¯å½“å‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ã€‚")
        return {"message": "ä¸­æ­¢ä»»åŠ¡çš„è¯·æ±‚å·²å‘é€ã€‚"}

@router.post("/tasks/{taskId}/pause", response_model=ControlActionResponse, summary="æš‚åœæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡")
async def pause_task(taskId: str, task_manager: TaskManager = Depends(get_task_manager)):
    """æš‚åœä¸€ä¸ªå½“å‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ã€‚ä»»åŠ¡å°†åœ¨ä¸‹ä¸€æ¬¡è¿›åº¦æ›´æ–°æ—¶æš‚åœã€‚"""
    if not await task_manager.pause_task(taskId):
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="æš‚åœä»»åŠ¡å¤±è´¥ï¼Œå¯èƒ½ä»»åŠ¡æœªåœ¨è¿è¡Œã€‚")
    return {"message": "ä»»åŠ¡å·²æš‚åœã€‚"}

@router.post("/tasks/{taskId}/resume", response_model=ControlActionResponse, summary="æ¢å¤å·²æš‚åœçš„ä»»åŠ¡")
async def resume_task(taskId: str, task_manager: TaskManager = Depends(get_task_manager)):
    """æ¢å¤ä¸€ä¸ªå·²æš‚åœçš„ä»»åŠ¡ã€‚"""
    if not await task_manager.resume_task(taskId):
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail="æ¢å¤ä»»åŠ¡å¤±è´¥ï¼Œå¯èƒ½ä»»åŠ¡æœªè¢«æš‚åœã€‚")
    return {"message": "ä»»åŠ¡å·²æ¢å¤ã€‚"}

@router.get("/tasks/{taskId}/execution", response_model=ExecutionTaskResponse, summary="è·å–è°ƒåº¦ä»»åŠ¡è§¦å‘çš„æ‰§è¡Œä»»åŠ¡IDå’ŒçŠ¶æ€")
async def get_execution_task_id(
    taskId: str,
    session: AsyncSession = Depends(get_db_session)
):
    """
    åœ¨è°ƒç”¨ `/import/auto` ç­‰æ¥å£åï¼Œä½¿ç”¨è¿”å›çš„è°ƒåº¦ä»»åŠ¡IDæ¥æŸ¥è¯¢å…¶è§¦å‘çš„ã€
    çœŸæ­£æ‰§è¡Œä¸‹è½½å¯¼å…¥å·¥ä½œçš„ä»»åŠ¡IDå’ŒçŠ¶æ€ã€‚

    è¿”å›å­—æ®µè¯´æ˜:
    - `schedulerTaskId`: è°ƒåº¦ä»»åŠ¡ID (è¾“å…¥çš„taskId)
    - `executionTaskId`: æ‰§è¡Œä»»åŠ¡ID,å¦‚æœè°ƒåº¦ä»»åŠ¡å°šæœªè§¦å‘æ‰§è¡Œä»»åŠ¡åˆ™ä¸ºnull
    - `status`: ä»»åŠ¡çŠ¶æ€,å¯èƒ½çš„å€¼:
        - `è¿è¡Œä¸­`: ä»»åŠ¡æ­£åœ¨æ‰§è¡Œ
        - `å·²å®Œæˆ`: ä»»åŠ¡å·²æˆåŠŸå®Œæˆ
        - `å¤±è´¥`: ä»»åŠ¡æ‰§è¡Œå¤±è´¥
        - `å·²å–æ¶ˆ`: ä»»åŠ¡å·²è¢«å–æ¶ˆ
        - `ç­‰å¾…ä¸­`: ä»»åŠ¡ç­‰å¾…æ‰§è¡Œ
        - `å·²æš‚åœ`: ä»»åŠ¡å·²æš‚åœ
        - `null`: è°ƒåº¦ä»»åŠ¡å°šæœªå®Œæˆ,æ— æ³•è·å–çŠ¶æ€

    æ‚¨å¯ä»¥è½®è¯¢æ­¤æ¥å£ï¼Œç›´åˆ°è·å–åˆ° `executionTaskId` å’Œæœ€ç»ˆçŠ¶æ€ã€‚
    """
    execution_id, status = await crud.get_execution_task_id_from_scheduler_task(session, taskId)
    return ExecutionTaskResponse(schedulerTaskId=taskId, executionTaskId=execution_id, status=status)

async def _get_rate_limit_status_data(
    session: AsyncSession,
    scraper_manager: ScraperManager,
    rate_limiter: RateLimiter
) -> models.ControlRateLimitStatusResponse:
    """
    è·å–æµæ§çŠ¶æ€æ•°æ®çš„æ ¸å¿ƒé€»è¾‘ï¼ˆå¯è¢«æ™®é€šå“åº”å’ŒSSEæµå¤ç”¨ï¼‰
    """
    # åœ¨è·å–çŠ¶æ€å‰ï¼Œå…ˆè§¦å‘ä¸€æ¬¡å…¨å±€æµæ§çš„æ£€æŸ¥ï¼Œè¿™ä¼šå¼ºåˆ¶é‡ç½®è¿‡æœŸçš„è®¡æ•°å™¨
    try:
        await rate_limiter.check("__control_api_status_check__")
    except RateLimitExceededError:
        # æˆ‘ä»¬åªå…³å¿ƒæ£€æŸ¥å’Œé‡ç½®çš„å‰¯ä½œç”¨ï¼Œä¸å…³å¿ƒå®ƒæ˜¯å¦çœŸçš„è¶…é™ï¼Œæ‰€ä»¥å¿½ç•¥æ­¤é”™è¯¯
        pass
    except Exception as e:
        # è®°å½•å…¶ä»–æ½œåœ¨é”™è¯¯ï¼Œä½†ä¸ä¸­æ–­çŠ¶æ€è·å–
        logger.error(f"åœ¨è·å–æµæ§çŠ¶æ€æ—¶ï¼Œæ£€æŸ¥å…¨å±€æµæ§å¤±è´¥: {e}")

    global_enabled = rate_limiter.enabled
    global_limit = rate_limiter.global_limit
    period_seconds = rate_limiter.global_period_seconds

    all_states = await crud.get_all_rate_limit_states(session)
    states_map = {s.providerName: s for s in all_states}

    global_state = states_map.get("__global__")
    seconds_until_reset = 0
    if global_state:
        time_since_reset = get_now().replace(tzinfo=None) - global_state.lastResetTime
        seconds_until_reset = max(0, int(period_seconds - time_since_reset.total_seconds()))

    # è·å–åå¤‡æµæ§çŠ¶æ€
    fallback_match_state = states_map.get("__fallback_match__")
    fallback_search_state = states_map.get("__fallback_search__")
    match_fallback_count = fallback_match_state.requestCount if fallback_match_state else 0
    search_fallback_count = fallback_search_state.requestCount if fallback_search_state else 0
    fallback_total = match_fallback_count + search_fallback_count
    fallback_limit = 50  # å›ºå®š50æ¬¡

    provider_items = []
    all_scrapers_raw = await crud.get_all_scraper_settings(session)
    # ä¿®æ­£ï¼šåœ¨æ˜¾ç¤ºæµæ§çŠ¶æ€æ—¶ï¼Œæ’é™¤ä¸äº§ç”Ÿç½‘ç»œè¯·æ±‚çš„ 'custom' æº
    all_scrapers = [s for s in all_scrapers_raw if s['providerName'] != 'custom']

    # æ”¶é›†æ‰€æœ‰åå¤‡ç›¸å…³çš„providerçŠ¶æ€ï¼ˆç”¨äºè®¡ç®—fallbackCountï¼‰
    fallback_provider_names = set()
    for state_name in states_map.keys():
        if state_name.startswith("__fallback_") and state_name.endswith("__"):
            # è¿™äº›æ˜¯åå¤‡ç±»å‹çš„çŠ¶æ€ï¼Œè·³è¿‡
            continue
        if state_name not in ["__global__"] and state_name in [s['providerName'] for s in all_scrapers]:
            fallback_provider_names.add(state_name)

    for scraper_setting in all_scrapers:
        provider_name = scraper_setting['providerName']
        provider_state = states_map.get(provider_name)
        total_count = provider_state.requestCount if provider_state else 0

        # è®¡ç®—åå¤‡è°ƒç”¨è®¡æ•°ï¼ˆè¿™éœ€è¦ä»åå¤‡æµæ§çš„è¯¦ç»†è®°å½•ä¸­è·å–ï¼Œæš‚æ—¶è®¾ä¸º0ï¼‰
        # TODO: éœ€è¦åœ¨æ•°æ®åº“ä¸­è®°å½•æ¯ä¸ªproviderçš„åå¤‡è°ƒç”¨æ¬¡æ•°
        fallback_count = 0
        direct_count = total_count - fallback_count

        quota: Union[int, str] = "âˆ"
        try:
            scraper_instance = scraper_manager.get_scraper(provider_name)
            provider_quota = getattr(scraper_instance, 'rate_limit_quota', None)
            if provider_quota is not None and provider_quota > 0:
                quota = provider_quota
        except ValueError:
            pass

        provider_items.append(models.ControlRateLimitProviderStatus(
            providerName=provider_name,
            directCount=direct_count,
            fallbackCount=fallback_count,
            requestCount=total_count,
            quota=quota
        ))

    # ä¿®æ­£ï¼šå°†ç§’æ•°è½¬æ¢ä¸ºå¯è¯»çš„å­—ç¬¦ä¸²ä»¥åŒ¹é…å“åº”æ¨¡å‹
    global_period_str = f"{period_seconds} ç§’"

    return models.ControlRateLimitStatusResponse(
        globalEnabled=global_enabled,
        globalRequestCount=global_state.requestCount if global_state else 0,
        globalLimit=global_limit,
        globalPeriod=global_period_str,
        secondsUntilReset=seconds_until_reset,
        fallbackTotalCount=fallback_total,
        fallbackTotalLimit=fallback_limit,
        fallbackMatchCount=match_fallback_count,
        fallbackSearchCount=search_fallback_count,
        providers=provider_items
    )


@router.get("/rate-limit/status", summary="è·å–æµæ§çŠ¶æ€")
async def get_rate_limit_status(
    request: Request,
    stream: bool = Query(False, description="æ˜¯å¦ä½¿ç”¨SSEæµå¼æ¨é€(æ¯ç§’æ›´æ–°)"),
    session: AsyncSession = Depends(get_db_session),
    scraper_manager: ScraperManager = Depends(get_scraper_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter)
):
    """
    è·å–æ‰€æœ‰æµæ§è§„åˆ™çš„å½“å‰çŠ¶æ€ï¼ŒåŒ…æ‹¬å…¨å±€å’Œå„æºçš„é…é¢ä½¿ç”¨æƒ…å†µã€‚

    ## ä½¿ç”¨æ–¹å¼

    ### æ™®é€šJSONå“åº” (é»˜è®¤)
    - è¯·æ±‚: `GET /api/control/rate-limit/status`
    - å“åº”: å•æ¬¡JSONå¯¹è±¡
    - Content-Type: `application/json`

    ### SSEæµå¼æ¨é€
    - è¯·æ±‚: `GET /api/control/rate-limit/status?stream=true`
    - å“åº”: `text/event-stream`ï¼Œæ¯ç§’æ¨é€ä¸€æ¬¡çŠ¶æ€æ›´æ–°
    - äº‹ä»¶æ ¼å¼: `data: {JSONå¯¹è±¡}`
    - å®¢æˆ·ç«¯æ–­å¼€è¿æ¥æ—¶è‡ªåŠ¨åœæ­¢æ¨é€

    ## å‚æ•°
    - **stream**: æ˜¯å¦ä½¿ç”¨SSEæµå¼æ¨é€ã€‚é»˜è®¤Falseè¿”å›JSONï¼ŒTrueè¿”å›text/event-stream

    ## å“åº”æ ¼å¼
    ä¸¤ç§æ¨¡å¼è¿”å›çš„æ•°æ®ç»“æ„å®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯ä¼ è¾“æ–¹å¼ä¸åŒ:
    - æ™®é€šæ¨¡å¼: ä¸€æ¬¡æ€§è¿”å›å®Œæ•´JSON
    - SSEæ¨¡å¼: æ¯ç§’æ¨é€ä¸€æ¬¡ç›¸åŒæ ¼å¼çš„JSONæ•°æ®
    """
    if not stream:
        # æ™®é€šJSONå“åº”
        return await _get_rate_limit_status_data(session, scraper_manager, rate_limiter)

    # SSEæµå¼å“åº”
    async def event_generator():
        """SSEäº‹ä»¶ç”Ÿæˆå™¨ï¼Œæ¯ç§’æ¨é€ä¸€æ¬¡æµæ§çŠ¶æ€"""
        session_factory = request.app.state.db_session_factory
        try:
            while True:
                try:
                    # ä¸ºæ¯æ¬¡å¾ªç¯åˆ›å»ºæ–°çš„session
                    async with session_factory() as loop_session:
                        # è·å–æµæ§çŠ¶æ€æ•°æ®
                        status_data = await _get_rate_limit_status_data(loop_session, scraper_manager, rate_limiter)

                        # è½¬æ¢ä¸ºå­—å…¸å¹¶åºåˆ—åŒ–ä¸ºJSON (ä¸æ™®é€šJSONå“åº”æ ¼å¼ä¸€è‡´)
                        status_dict = status_data.model_dump(mode='json')

                        # å‘é€çŠ¶æ€æ›´æ–°äº‹ä»¶ (ç›´æ¥æ¨é€çŠ¶æ€å¯¹è±¡,ä¸åŒ…è£…typeå­—æ®µ)
                        yield f"data: {json.dumps(status_dict, ensure_ascii=False)}\n\n"

                    # ç­‰å¾…1ç§’åå†æ¬¡æ¨é€
                    await asyncio.sleep(1)

                except Exception as e:
                    logger.error(f"SSEæµæ§çŠ¶æ€æ¨é€å‡ºé”™: {e}", exc_info=True)
                    # é”™è¯¯æ—¶æ¨é€ç©ºå¯¹è±¡,é¿å…å®¢æˆ·ç«¯è§£æå¤±è´¥
                    await asyncio.sleep(1)

        except asyncio.CancelledError:
            logger.info("SSEæµæ§çŠ¶æ€æ¨é€å·²å–æ¶ˆ(å®¢æˆ·ç«¯æ–­å¼€è¿æ¥)")
            raise

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


# --- å®šæ—¶ä»»åŠ¡ç®¡ç† ---

@router.get("/scheduler/tasks", response_model=List[Dict[str, Any]], summary="è·å–æ‰€æœ‰å®šæ—¶ä»»åŠ¡")
async def list_scheduled_tasks(
    scheduler_manager: SchedulerManager = Depends(get_scheduler_manager),
):
    """è·å–æ‰€æœ‰å·²é…ç½®çš„å®šæ—¶ä»»åŠ¡åŠå…¶å½“å‰çŠ¶æ€ã€‚"""
    tasks = await scheduler_manager.get_all_tasks()
    # ä¿®æ­£ï¼šå°† 'id' é”®é‡å‘½åä¸º 'taskId' ä»¥ä¿æŒAPIä¸€è‡´æ€§
    return tasks

@router.get("/scheduler/{taskId}/last_result", response_model=models.TaskInfo, summary="è·å–å®šæ—¶ä»»åŠ¡çš„æœ€è¿‘ä¸€æ¬¡è¿è¡Œç»“æœ")
async def get_scheduled_task_last_result(
    taskId: str = Path(..., description="å®šæ—¶ä»»åŠ¡çš„ID"),
    session: AsyncSession = Depends(get_db_session),
):
    """
    è·å–æŒ‡å®šå®šæ—¶ä»»åŠ¡çš„æœ€è¿‘ä¸€æ¬¡è¿è¡Œç»“æœã€‚
    å¦‚æœä»»åŠ¡ä»æœªè¿è¡Œè¿‡ï¼Œå°†è¿”å› 404 Not Foundã€‚
    """
    result = await crud.get_last_run_result_for_scheduled_task(session, taskId)
    if not result:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="æœªæ‰¾åˆ°è¯¥å®šæ—¶ä»»åŠ¡çš„è¿è¡Œè®°å½•ã€‚")
    return models.TaskInfo.model_validate(result)

# --- é€šç”¨é…ç½®ç®¡ç†æ¥å£ ---

# å®šä¹‰å¯é€šè¿‡å¤–éƒ¨APIç®¡ç†çš„é…ç½®é¡¹ç™½åå•
ALLOWED_CONFIG_KEYS = {
    # Webhookç›¸å…³é…ç½®
    "webhookEnabled": {"type": "boolean", "description": "æ˜¯å¦å…¨å±€å¯ç”¨ Webhook åŠŸèƒ½"},
    "webhookDelayedImportEnabled": {"type": "boolean", "description": "æ˜¯å¦ä¸º Webhook è§¦å‘çš„å¯¼å…¥å¯ç”¨å»¶æ—¶"},
    "webhookDelayedImportHours": {"type": "integer", "description": "Webhook å»¶æ—¶å¯¼å…¥çš„å°æ—¶æ•°"},
    "webhookFilterMode": {"type": "string", "description": "Webhook æ ‡é¢˜è¿‡æ»¤æ¨¡å¼ (blacklist/whitelist)"},
    "webhookFilterRegex": {"type": "string", "description": "ç”¨äºè¿‡æ»¤ Webhook æ ‡é¢˜çš„æ­£åˆ™è¡¨è¾¾å¼"},
    # è¯†åˆ«è¯é…ç½®
    "titleRecognition": {"type": "text", "description": "è‡ªå®šä¹‰è¯†åˆ«è¯é…ç½®å†…å®¹ï¼Œæ”¯æŒå±è”½è¯ã€æ›¿æ¢ã€é›†æ•°åç§»ã€å­£åº¦åç§»ç­‰è§„åˆ™"},
    # AIé…ç½®
    "aiMatchPrompt": {"type": "text", "description": "AIæ™ºèƒ½åŒ¹é…æç¤ºè¯"},
    "aiRecognitionPrompt": {"type": "text", "description": "AIè¾…åŠ©è¯†åˆ«æç¤ºè¯"},
    "aiAliasValidationPrompt": {"type": "text", "description": "AIåˆ«åéªŒè¯æç¤ºè¯"},
}

class ConfigItem(BaseModel):
    key: str
    value: str
    type: str
    description: str

class ConfigUpdateRequest(BaseModel):
    key: str
    value: str

class ConfigResponse(BaseModel):
    configs: List[ConfigItem]

class HelpResponse(BaseModel):
    available_keys: List[str]
    description: str

@router.get("/config", response_model=Union[ConfigResponse, HelpResponse], summary="è·å–å¯é…ç½®çš„å‚æ•°åˆ—è¡¨æˆ–å¸®åŠ©ä¿¡æ¯")
async def get_allowed_configs(
    type: Optional[str] = Query(None, description="è¯·æ±‚ç±»å‹ï¼Œä½¿ç”¨ 'help' è·å–å¯ç”¨é…ç½®é¡¹åˆ—è¡¨"),
    config_manager: ConfigManager = Depends(get_config_manager),
    title_recognition_manager = Depends(get_title_recognition_manager),
    session: AsyncSession = Depends(get_db_session)
):
    """
    è·å–æ‰€æœ‰å¯é€šè¿‡å¤–éƒ¨APIç®¡ç†çš„é…ç½®é¡¹åŠå…¶å½“å‰å€¼ã€‚

    å‚æ•°:
    - type: å¯é€‰å‚æ•°
      - ä¸æä¾›æˆ–ä¸ºç©º: è¿”å›æ‰€æœ‰é…ç½®é¡¹åŠå…¶å½“å‰å€¼
      - "help": è¿”å›æ‰€æœ‰å¯ç”¨çš„é…ç½®é¡¹é”®ååˆ—è¡¨
    """
    # å¦‚æœè¯·æ±‚ç±»å‹æ˜¯ helpï¼Œè¿”å›å¸®åŠ©ä¿¡æ¯
    if type == "help":
        return HelpResponse(
            available_keys=list(ALLOWED_CONFIG_KEYS.keys()),
            description="å¯é€šè¿‡å¤–éƒ¨APIç®¡ç†çš„é…ç½®é¡¹åˆ—è¡¨ã€‚ä½¿ç”¨ä¸å¸¦ type å‚æ•°çš„è¯·æ±‚è·å–è¯¦ç»†é…ç½®ä¿¡æ¯ã€‚"
        )

    # é»˜è®¤è¡Œä¸ºï¼šè¿”å›æ‰€æœ‰é…ç½®é¡¹åŠå…¶å½“å‰å€¼
    configs = []

    for key, meta in ALLOWED_CONFIG_KEYS.items():
        # ç‰¹æ®Šå¤„ç†è¯†åˆ«è¯é…ç½®
        if key == "titleRecognition":
            if title_recognition_manager:
                # ä»æ•°æ®åº“è·å–è¯†åˆ«è¯é…ç½®
                from ..orm_models import TitleRecognition
                result = await session.execute(select(TitleRecognition).limit(1))
                title_recognition = result.scalar_one_or_none()
                current_value = title_recognition.content if title_recognition else ""
            else:
                current_value = ""
        else:
            # æ ¹æ®ç±»å‹è®¾ç½®é»˜è®¤å€¼
            if meta["type"] == "boolean":
                default_value = "false"
            elif meta["type"] == "integer":
                default_value = "0"
            else:  # string
                default_value = ""

            current_value = await config_manager.get(key, default_value)

        configs.append(ConfigItem(
            key=key,
            value=str(current_value),
            type=meta["type"],
            description=meta["description"]
        ))

    return ConfigResponse(configs=configs)

@router.put("/config", status_code=status.HTTP_204_NO_CONTENT, summary="æ›´æ–°æŒ‡å®šé…ç½®é¡¹")
async def update_config(
    request: ConfigUpdateRequest,
    config_manager: ConfigManager = Depends(get_config_manager),
    title_recognition_manager = Depends(get_title_recognition_manager)
):
    """
    æ›´æ–°æŒ‡å®šçš„é…ç½®é¡¹ã€‚
    åªå…è®¸æ›´æ–°ç™½åå•ä¸­å®šä¹‰çš„é…ç½®é¡¹ã€‚
    """
    if request.key not in ALLOWED_CONFIG_KEYS:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"é…ç½®é¡¹ '{request.key}' ä¸åœ¨å…è®¸çš„é…ç½®åˆ—è¡¨ä¸­ã€‚å…è®¸çš„é…ç½®é¡¹: {list(ALLOWED_CONFIG_KEYS.keys())}"
        )

    config_meta = ALLOWED_CONFIG_KEYS[request.key]

    # æ ¹æ®ç±»å‹éªŒè¯å€¼
    if config_meta["type"] == "boolean":
        if request.value.lower() not in ["true", "false"]:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"é…ç½®é¡¹ '{request.key}' çš„å€¼å¿…é¡»æ˜¯ 'true' æˆ– 'false'"
            )
    elif config_meta["type"] == "integer":
        try:
            int(request.value)
        except ValueError:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"é…ç½®é¡¹ '{request.key}' çš„å€¼å¿…é¡»æ˜¯æ•´æ•°"
            )

    # ç‰¹æ®Šå¤„ç†è¯†åˆ«è¯é…ç½®
    if request.key == "titleRecognition":
        if title_recognition_manager is None:
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="è¯†åˆ«è¯ç®¡ç†å™¨æœªåˆå§‹åŒ–")

        # æ›´æ–°è¯†åˆ«è¯é…ç½®
        warnings = await title_recognition_manager.update_recognition_rules(request.value)
        if warnings:
            logger.warning(f"å¤–éƒ¨APIæ›´æ–°è¯†åˆ«è¯é…ç½®æ—¶å‘ç° {len(warnings)} ä¸ªè­¦å‘Š: {warnings}")

        logger.info(f"å¤–éƒ¨APIæ›´æ–°äº†è¯†åˆ«è¯é…ç½®ï¼Œå…± {len(title_recognition_manager.recognition_rules)} æ¡è§„åˆ™")
    else:
        # æ›´æ–°æ™®é€šé…ç½®
        await config_manager.setValue(request.key, request.value)
        logger.info(f"å¤–éƒ¨APIæ›´æ–°äº†é…ç½®é¡¹ '{request.key}' ä¸º '{request.value}'")

    return
