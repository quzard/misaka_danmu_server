"""
弹弹Play 兼容 API 的匹配功能

通过文件名匹配弹幕库，支持库内直接匹配和后备匹配。
"""

import asyncio
import json
import logging
import re
import time
from typing import Dict, List, Optional, Any

from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession
from fastapi import APIRouter, Depends, HTTPException, status
from thefuzz import fuzz

from src.db import crud, orm_models, get_db_session, sync_postgres_sequence, ConfigManager
from src.core import get_now
from src.services import ScraperManager, TaskManager, MetadataSourceManager
from src.utils import (
    parse_search_keyword, unified_search,
    ai_type_and_season_mapping_and_correction, title_contains_season_name,
    SearchTimer, SEARCH_TYPE_FALLBACK_MATCH, SubStepTiming
)
from src.rate_limiter import RateLimiter
from src.ai import AIMatcherManager
from src.api.control.dependencies import get_title_recognition_manager
from src.api.dependencies import get_ai_matcher_manager

# 从 ai 模块导入需要的常量
from src.ai.ai_prompts import DEFAULT_AI_MATCH_PROMPT

# 从 orm_models 导入需要的模型
AnimeSource = orm_models.AnimeSource

# 同包内相对导入
from .models import (
    DandanMatchInfo, DandanMatchResponse,
    DandanBatchMatchRequestItem, DandanBatchMatchRequest
)
from .constants import (
    DANDAN_TYPE_MAPPING, DANDAN_TYPE_DESC_MAPPING,
    FALLBACK_SEARCH_CACHE_PREFIX, FALLBACK_SEARCH_CACHE_TTL,
    METADATA_KEYWORDS_PATTERN
)
from .helpers import (
    get_db_cache, set_db_cache,
    store_episode_mapping, find_existing_anime_by_bangumi_id,
    get_next_real_anime_id, _get_next_virtual_anime_id, _generate_episode_id
)
from .route_handler import get_token_from_path, DandanApiRoute
from .dependencies import (
    get_config_manager,
    get_task_manager,
    get_rate_limiter,
    get_scraper_manager,
    get_metadata_manager,
)

logger = logging.getLogger(__name__)


def parse_filename_for_match(filename: str) -> Optional[Dict[str, Any]]:
    """
    使用正则表达式从文件名中解析出番剧标题和集数。
    这是一个简化的实现，用于 dandanplay 兼容接口。
    """
    # 移除文件扩展名 - 只有当后缀是真正的视频扩展名时才移除
    VIDEO_EXTENSIONS = {'mkv', 'mp4', 'avi', 'wmv', 'flv', 'ts', 'm2ts', 'rmvb', 'rm', 'mov', 'webm', 'mpg', 'mpeg', 'vob', 'iso', 'bdmv'}
    name_without_ext = filename
    if '.' in filename:
        parts = filename.rsplit('.', 1)
        if len(parts) == 2 and parts[1].lower() in VIDEO_EXTENSIONS:
            name_without_ext = parts[0]

    # 模式1: SXXEXX 格式 (e.g., "Some.Anime.S01E02.1080p.mkv")
    s_e_pattern = re.compile(
        r"^(?P<title>.+?)"
        r"[\s._-]*"
        r"[Ss](?P<season>\d{1,2})"
        r"[Ee](?P<episode>\d{1,4})"
        r"\b",
        re.IGNORECASE
    )
    match = s_e_pattern.search(name_without_ext)
    if match:
        data = match.groupdict()
        title = data["title"].replace(".", " ").replace("_", " ").strip()
        title = re.sub(r'\[.*?\]', '', title).strip() # 移除字幕组标签
        # 新增：移除标题中的年份并清理多余空格
        title = re.sub(r'\b(19|20)\d{2}\b', '', title).strip()
        title = re.sub(r'\s+', ' ', title).strip(' -')
        return {
            "title": title,
            "season": int(data["season"]),
            "episode": int(data["episode"])
        }

    # 模式2: 只有季度 (e.g., "Some Anime S03", "Some Anime Season 2")
    season_only_patterns = [
        # S01, S02, S03 等格式
        re.compile(r"^(?P<title>.+?)[\s._-]+[Ss](?P<season>\d{1,2})(?:\s|$)", re.IGNORECASE),
        # Season 1, Season 2 等格式
        re.compile(r"^(?P<title>.+?)[\s._-]+Season[\s._-]*(?P<season>\d{1,2})(?:\s|$)", re.IGNORECASE),
    ]
    for pattern in season_only_patterns:
        match = pattern.search(name_without_ext)
        if match:
            data = match.groupdict()
            title = data["title"].replace(".", " ").replace("_", " ").strip()
            # 清理标题中的元数据
            title = re.sub(r'\[.*?\]|\(.*?\)|\【.*?\】', '', title).strip()
            title = METADATA_KEYWORDS_PATTERN.sub('', title).strip()
            # 移除标题中的年份并清理多余空格
            title = re.sub(r'\b(19|20)\d{2}\b', '', title).strip()
            title = re.sub(r'\s+', ' ', title).strip(' -')
            return {
                "title": title,
                "season": int(data["season"]),
                "episode": None,  # 只有季度，没有集数
            }

    # 模式3: 只有集数 (e.g., "[Subs] Some Anime - 02 [1080p].mkv")
    ep_only_patterns = [
        re.compile(r"^(?P<title>.+?)\s*[-_]\s*\b(?P<episode>\d{1,4})\b", re.IGNORECASE),
        re.compile(r"^(?P<title>.+?)\s+\b(?P<episode>\d{1,4})\b", re.IGNORECASE),
    ]
    for pattern in ep_only_patterns:
        match = pattern.search(name_without_ext)
        if match:
            data = match.groupdict()
            title = data["title"]
            # 清理标题中的元数据
            title = re.sub(r'\[.*?\]|\(.*?\)|\【.*?\】', '', title).strip()
            title = METADATA_KEYWORDS_PATTERN.sub('', title).strip()
            title = title.replace("_", " ").replace(".", " ").strip()
            # 新增：移除标题中的年份并清理多余空格
            title = re.sub(r'\b(19|20)\d{2}\b', '', title).strip()
            title = re.sub(r'\s+', ' ', title).strip(' -')
            return {
                "title": title,
                "season": None, # 此模式无法识别季度
                "episode": int(data["episode"]),
            }
    
    # 模式4: 电影或单文件视频 (没有集数)
    title = name_without_ext
    title = re.sub(r'\[.*?\]|\(.*?\)|\【.*?\】', '', title).strip()
    title = METADATA_KEYWORDS_PATTERN.sub('', title).strip()
    title = title.replace("_", " ").replace(".", " ").strip()
    # 移除年份, 兼容括号内和独立两种形式
    title = re.sub(r'\(\s*(19|20)\d{2}\s*\)', '', title).strip()
    title = re.sub(r'\b(19|20)\d{2}\b', '', title).strip()
    title = re.sub(r'\s+', ' ', title).strip(' -')

    if title:
        return {
            "title": title,
            "season": None,  # 电影不设置季度
            "episode": None, # 电影不设置集数
            "is_movie": True # 标记为电影
        }

    return None


async def get_match_for_item(
    item: DandanBatchMatchRequestItem,
    session: AsyncSession,
    task_manager: TaskManager,
    scraper_manager: ScraperManager,
    metadata_manager: MetadataSourceManager,
    config_manager: ConfigManager,
    ai_matcher_manager: AIMatcherManager,
    rate_limiter: RateLimiter,
    title_recognition_manager,
    current_token: Optional[str] = None
) -> DandanMatchResponse:
    """
    通过文件名匹配弹幕库的核心逻辑。此接口不使用文件Hash。
    优先进行库内直接匹配，失败后回退到TMDB剧集组映射。
    新增：如果所有匹配都失败，且启用了后备机制，则触发自动搜索导入任务。
    """
    logger.info(f"执行匹配逻辑, 文件名: '{item.fileName}'")
    parsed_info = parse_filename_for_match(item.fileName)
    logger.info(f"文件名解析结果: {parsed_info}")
    if not parsed_info:
        response = DandanMatchResponse(isMatched=False)
        logger.info(f"发送匹配响应 (解析失败): {response.model_dump_json(indent=2)}")
        return response

    # --- 步骤 1: 优先进行库内直接搜索 ---
    logger.info("正在进行库内直接搜索...")
    results = await crud.search_episodes_in_library(
        session, parsed_info["title"], parsed_info.get("episode"), parsed_info.get("season")
    )
    logger.info(f"直接搜索为 '{parsed_info['title']}' (季:{parsed_info.get('season')} 集:{parsed_info.get('episode')}) 找到 {len(results)} 条记录")
    
    if results:
        # 对结果进行严格的标题过滤，避免模糊匹配带来的问题
        normalized_search_title = parsed_info["title"].replace("：", ":").replace(" ", "")
        exact_matches = []
        for r in results:
            all_titles_to_check = [
                r.get('animeTitle'), r.get('nameEn'), r.get('nameJp'), r.get('nameRomaji'),
                r.get('aliasCn1'), r.get('aliasCn2'), r.get('aliasCn3'),
            ]
            aliases_to_check = {t for t in all_titles_to_check if t}
            # 使用normalized_search_title进行更精确的匹配
            if any(fuzz.partial_ratio(alias.replace("：", ":").replace(" ", ""), normalized_search_title) > 85 for alias in aliases_to_check):
                exact_matches.append(r)

        if len(exact_matches) < len(results):
            logger.info(f"过滤掉 {len(results) - len(exact_matches)} 条模糊匹配的结果。")
            results = exact_matches

        if results:
            # 优先处理被精确标记的源
            favorited_results = [r for r in results if r.get('isFavorited')]
            if favorited_results:
                res = favorited_results[0]
                dandan_type = DANDAN_TYPE_MAPPING.get(res.get('type'), "other")
                dandan_type_desc = DANDAN_TYPE_DESC_MAPPING.get(res.get('type'), "其他")
                match = DandanMatchInfo(
                    episodeId=res['episodeId'], animeId=res['animeId'], animeTitle=res['animeTitle'],
                    episodeTitle=res['episodeTitle'], type=dandan_type, typeDescription=dandan_type_desc,
                    imageUrl=res.get('imageUrl')
                )
                response = DandanMatchResponse(isMatched=True, matches=[match])
                logger.info(f"发送匹配响应 (精确标记匹配): {response.model_dump_json(indent=2)}")
                return response

            # 如果没有精确标记，检查所有匹配项是否都指向同一个番剧ID
            first_animeId = results[0]['animeId']
            all_from_same_anime = all(res['animeId'] == first_animeId for res in results)

            if all_from_same_anime:
                res = results[0]
                dandan_type = DANDAN_TYPE_MAPPING.get(res.get('type'), "other")
                dandan_type_desc = DANDAN_TYPE_DESC_MAPPING.get(res.get('type'), "其他")
                match = DandanMatchInfo(
                    episodeId=res['episodeId'], animeId=res['animeId'], animeTitle=res['animeTitle'],
                    episodeTitle=res['episodeTitle'], type=dandan_type, typeDescription=dandan_type_desc,
                    imageUrl=res.get('imageUrl')
                )
                response = DandanMatchResponse(isMatched=True, matches=[match])
                logger.info(f"发送匹配响应 (单一作品匹配): {response.model_dump_json(indent=2)}")
                return response

            # 如果匹配到了多个不同的番剧，则返回所有结果让用户选择
            matches = []
            for res in results:
                dandan_type = DANDAN_TYPE_MAPPING.get(res.get('type'), "other")
                dandan_type_desc = DANDAN_TYPE_DESC_MAPPING.get(res.get('type'), "其他")
                matches.append(DandanMatchInfo(
                    episodeId=res['episodeId'], animeId=res['animeId'], animeTitle=res['animeTitle'],
                    episodeTitle=res['episodeTitle'], type=dandan_type, typeDescription=dandan_type_desc,
                    imageUrl=res.get('imageUrl')
                ))
            response = DandanMatchResponse(isMatched=False, matches=matches)
            logger.info(f"发送匹配响应 (多个匹配): {response.model_dump_json(indent=2)}")
            return response

    # --- 步骤 2: 如果直接搜索无果，则回退到 TMDB 映射 ---
    # 注意：TMDB映射仅适用于TV系列，电影跳过此步骤
    if not parsed_info.get("is_movie"):
        logger.info("直接搜索未找到精确匹配，回退到 TMDB 映射匹配。")
        potential_animes = await crud.find_animes_for_matching(session, parsed_info["title"])
        logger.info(f"为标题 '{parsed_info['title']}' 找到 {len(potential_animes)} 个可能的库内作品进行TMDB匹配。")

        for anime in potential_animes:
            if anime.get("tmdbId") and anime.get("tmdbEpisodeGroupId"):
                logger.info(f"正在为作品 ID {anime['animeId']} (TMDB ID: {anime['tmdbId']}) 尝试 TMDB 映射匹配...")
                tmdb_results = await crud.find_episode_via_tmdb_mapping(
                    session,
                    tmdb_id=anime["tmdbId"],
                    group_id=anime["tmdbEpisodeGroupId"],
                    custom_season=parsed_info.get("season"),
                    custom_episode=parsed_info.get("episode")
                )
                if tmdb_results:
                    logger.info(f"TMDB 映射匹配成功，找到 {len(tmdb_results)} 个结果。")
                    res = tmdb_results[0]
                    dandan_type = DANDAN_TYPE_MAPPING.get(res.get('type'), "other")
                    dandan_type_desc = DANDAN_TYPE_DESC_MAPPING.get(res.get('type'), "其他")
                    match = DandanMatchInfo(
                        episodeId=res['episodeId'], animeId=res['animeId'], animeTitle=res['animeTitle'],
                        episodeTitle=res['episodeTitle'], type=dandan_type, typeDescription=dandan_type_desc,
                        imageUrl=res.get('imageUrl')
                    )
                    response = DandanMatchResponse(isMatched=True, matches=[match])
                    logger.info(f"发送匹配响应 (TMDB 映射匹配): {response.model_dump_json(indent=2)}")
                    return response
    else:
        logger.info("检测到电影文件，跳过 TMDB 映射匹配。")

    # --- 步骤 3: 如果所有方法都失败 ---
    # 新增：后备机制 (Fallback Mechanism)
    fallback_enabled_str = await config_manager.get("matchFallbackEnabled", "false")
    if fallback_enabled_str.lower() == 'true':
        # 检查Token是否被允许使用匹配后备功能
        if current_token:
            try:
                # 获取当前token的信息
                token_stmt = select(orm_models.ApiToken).where(orm_models.ApiToken.token == current_token)
                token_result = await session.execute(token_stmt)
                current_token_obj = token_result.scalar_one_or_none()

                if current_token_obj:
                    # 获取允许的token列表
                    allowed_tokens_str = await config_manager.get("matchFallbackTokens", "[]")
                    allowed_token_ids = json.loads(allowed_tokens_str)

                    # 如果配置了允许的token列表且当前token不在列表中，跳过后备机制
                    if allowed_token_ids and current_token_obj.id not in allowed_token_ids:
                        logger.info(f"Token '{current_token_obj.name}' (ID: {current_token_obj.id}) 未被授权使用匹配后备功能，跳过后备机制。")
                        response = DandanMatchResponse(isMatched=False, matches=[])
                        logger.info(f"发送匹配响应 (Token未授权): {response.model_dump_json(indent=2)}")
                        return response
                    else:
                        logger.info(f"Token '{current_token_obj.name}' (ID: {current_token_obj.id}) 已被授权使用匹配后备功能。")
            except (json.JSONDecodeError, Exception) as e:
                logger.warning(f"检查匹配后备Token授权时发生错误: {e}，继续执行后备机制")

        # 检查黑名单
        blacklist_pattern = await config_manager.get("matchFallbackBlacklist", "")
        if blacklist_pattern.strip():
            try:
                if re.search(blacklist_pattern, item.fileName, re.IGNORECASE):
                    logger.info(f"文件 '{item.fileName}' 匹配黑名单规则 '{blacklist_pattern}'，跳过后备机制。")
                    response = DandanMatchResponse(isMatched=False, matches=[])
                    logger.info(f"发送匹配响应 (黑名单过滤): {response.model_dump_json(indent=2)}")
                    return response
            except re.error as e:
                logger.warning(f"黑名单正则表达式 '{blacklist_pattern}' 格式错误: {e}，忽略黑名单检查")

        # 方案C: 防重复机制 - 检查5分钟内是否已完成过相同的后备任务
        recent_fallback_key = f"recent_fallback_{parsed_info['title']}_{parsed_info.get('season')}_{parsed_info.get('episode')}"
        recent_fallback_data = await get_db_cache(session, FALLBACK_SEARCH_CACHE_PREFIX, recent_fallback_key)
        if recent_fallback_data:
            cached_time = recent_fallback_data.get("timestamp", 0)
            if time.time() - cached_time < 300:  # 5分钟内
                logger.info(f"检测到5分钟内已完成的后备任务，直接返回缓存结果")
                cached_response = recent_fallback_data.get("response")
                if cached_response:
                    return cached_response

        logger.info(f"匹配失败，已启用后备机制，正在为 '{item.fileName}' 创建自动搜索任务。")

        # 将匹配后备逻辑包装成协程工厂
        match_fallback_result = {"response": None}  # 用于存储结果

        async def match_fallback_coro_factory(session_inner: AsyncSession, progress_callback):
            """匹配后备任务的协程工厂"""
            # 初始化计时器并开始计时
            match_timer = SearchTimer(SEARCH_TYPE_FALLBACK_MATCH, item.fileName, logger)
            match_timer.start()

            try:
                match_timer.step_start("初始化与解析")
                # 构造 auto_search_and_import_task 需要的 payload
                # 根据 is_movie 标记判断媒体类型
                media_type_for_fallback = "movie" if parsed_info.get("is_movie") else "tv_series"

                logger.info(f"开始匹配后备流程: {item.fileName}")

                # 解析搜索关键词，提取纯标题
                search_parsed_info = parse_search_keyword(parsed_info["title"])
                base_title = search_parsed_info["title"]
                is_movie = parsed_info.get("is_movie", False)
                season = parsed_info.get("season") or 1
                # 电影不设置episode_number,保持为None
                episode_number = None if is_movie else (parsed_info.get("episode") or 1)

                # 匹配后备 AI映射配置检查
                match_fallback_tmdb_enabled = await config_manager.get("matchFallbackEnableTmdbSeasonMapping", "false")
                if match_fallback_tmdb_enabled.lower() != "true":
                    logger.info("○ 匹配后备 统一AI映射: 功能未启用")
                match_timer.step_end()

                # 步骤1：使用统一的搜索函数
                match_timer.step_start("弹幕源搜索")
                logger.info(f"步骤1：全网搜索 '{base_title}'")

                # 使用统一的搜索函数（与 WebUI 搜索保持一致的过滤策略）
                all_results = await unified_search(
                    search_term=base_title,
                    session=session_inner,
                    scraper_manager=scraper_manager,
                    metadata_manager=metadata_manager,  # 启用元数据别名扩展
                    use_alias_expansion=True,  # 启用别名扩展
                    use_alias_filtering=True,  # 启用别名过滤
                    use_title_filtering=True,  # 启用标题过滤
                    use_source_priority_sorting=False,  # 仅按相似度排序
                    strict_filtering=True,  # 使用严格过滤模式
                    alias_similarity_threshold=70,  # 与 WebUI 一致的别名相似度阈值
                    progress_callback=progress_callback
                )

                # 收集单源搜索耗时信息
                from src.utils.search_timer import SubStepTiming
                source_timing_sub_steps = [
                    SubStepTiming(name=name, duration_ms=dur, result_count=cnt)
                    for name, dur, cnt in scraper_manager.last_search_timing
                ]

                if not all_results:
                    logger.warning(f"匹配后备失败：没有找到任何搜索结果")
                    match_timer.step_end(details="无结果", sub_steps=source_timing_sub_steps)
                    match_timer.finish()  # 打印计时报告
                    response = DandanMatchResponse(isMatched=False, matches=[])
                    match_fallback_result["response"] = response
                    return

                match_timer.step_end(details=f"{len(all_results)}个结果", sub_steps=source_timing_sub_steps)
                logger.info(f"搜索完成，共 {len(all_results)} 个结果")

                # 使用统一的AI类型和季度映射修正函数
                if match_fallback_tmdb_enabled.lower() == "true":
                    try:
                        match_timer.step_start("AI映射修正")
                        # 获取AI匹配器
                        ai_matcher = await ai_matcher_manager.get_matcher()
                        if ai_matcher:
                            logger.info(f"○ 匹配后备 开始统一AI映射修正: '{base_title}' ({len(all_results)} 个结果)")

                            # 使用新的统一函数进行类型和季度修正
                            mapping_result = await ai_type_and_season_mapping_and_correction(
                                search_title=base_title,
                                search_results=all_results,
                                metadata_manager=metadata_manager,
                                ai_matcher=ai_matcher,
                                logger=logger,
                                similarity_threshold=60.0
                            )

                            # 应用修正结果
                            if mapping_result['total_corrections'] > 0:
                                logger.info(f"✓ 匹配后备 统一AI映射成功: 总计修正了 {mapping_result['total_corrections']} 个结果")
                                logger.info(f"  - 类型修正: {len(mapping_result['type_corrections'])} 个")
                                logger.info(f"  - 季度修正: {len(mapping_result['season_corrections'])} 个")

                                # 更新搜索结果（已经直接修改了all_results）
                                all_results = mapping_result['corrected_results']
                                match_timer.step_end(details=f"修正{mapping_result['total_corrections']}个")
                            else:
                                logger.info(f"○ 匹配后备 统一AI映射: 未找到需要修正的信息")
                                match_timer.step_end(details="无修正")
                        else:
                            logger.warning("○ 匹配后备 AI映射: AI匹配器未启用或初始化失败")
                            match_timer.step_end(details="匹配器未启用")

                    except Exception as e:
                        logger.warning(f"匹配后备 统一AI映射任务执行失败: {e}")
                        match_timer.step_end(details=f"失败: {e}")
                else:
                    logger.info("○ 匹配后备 统一AI映射: 功能未启用")

                # 步骤2：智能排序 (类型匹配优先)
                match_timer.step_start("智能排序与匹配")
                logger.info(f"步骤2：智能排序 (类型匹配优先)")

                # 确定目标类型
                target_type = "movie" if is_movie else "tv_series"

                # 获取源的优先级顺序
                source_settings = await crud.get_all_scraper_settings(session_inner)
                source_order_map = {s['providerName']: s['displayOrder'] for s in source_settings}

                def calculate_match_score(result):
                    """计算匹配分数，分数越高越优先"""
                    score = 0

                    # 1. 类型匹配 (最高优先级，+1000分)
                    if result.type == target_type:
                        score += 1000
                        logger.debug(f"  - {result.provider} - {result.title}: 类型匹配 +1000")

                    # 2. 标题相似度 (0-100分)
                    similarity = fuzz.token_set_ratio(base_title, result.title)
                    score += similarity
                    logger.debug(f"  - {result.provider} - {result.title}: 相似度{similarity} +{similarity}")

                    # 3. 年份匹配 (如果有年份信息，+50分)
                    # TODO: 从parsed_info中获取年份信息

                    return score

                # 按分数排序 (分数高的在前)，相同分数时按源优先级排序
                sorted_results = sorted(
                    all_results,
                    key=lambda r: (calculate_match_score(r), -source_order_map.get(r.provider, 999)),
                    reverse=True
                )

                # 打印排序后的结果列表
                logger.info(f"排序后的搜索结果列表 (按匹配分数):")
                for idx, result in enumerate(sorted_results, 1):
                    score = calculate_match_score(result)
                    type_match = "✓" if result.type == target_type else "✗"
                    logger.info(f"  {idx}. [{type_match}] {result.provider} - {result.title} (ID: {result.mediaId}, 类型: {result.type}, 年份: {result.year or 'N/A'}, 分数: {score:.0f})")

                # 步骤3：自动选择最佳源
                logger.info(f"步骤3：自动选择最佳源")

                # 获取精确标记信息 (AI匹配和传统匹配都需要)
                favorited_info = {}
                async with scraper_manager._session_factory() as ai_session:
                    for result in sorted_results:
                        # 查找是否有相同provider和mediaId的源被标记
                        stmt = (
                            select(AnimeSource.isFavorited)
                            .where(
                                AnimeSource.providerName == result.provider,
                                AnimeSource.mediaId == result.mediaId
                            )
                            .limit(1)
                        )
                        result_row = await ai_session.execute(stmt)
                        is_favorited = result_row.scalar_one_or_none()
                        if is_favorited:
                            key = f"{result.provider}:{result.mediaId}"
                            favorited_info[key] = True

                # 检查是否启用AI匹配
                ai_match_enabled = (await config_manager.get("aiMatchEnabled", "false")).lower() == 'true'

                # 如果启用AI匹配，尝试使用AI选择
                ai_selected_index = None
                if ai_match_enabled:
                    try:
                        # 动态注册AI提示词配置(如果不存在则创建,使用硬编码默认值)
                        await crud.initialize_configs(session_inner, {
                            "aiMatchPrompt": (DEFAULT_AI_MATCH_PROMPT, "AI智能匹配提示词")
                        })

                        # 构建查询信息
                        query_info = {
                            "title": base_title,
                            "season": season,
                            "episode": episode_number,
                            "year": None,  # 匹配后备场景通常没有年份信息
                            "type": "movie" if is_movie else "tv_series"
                        }

                        # 使用AIMatcherManager进行匹配
                        ai_selected_index = await ai_matcher_manager.select_best_match(
                            query_info, sorted_results, favorited_info
                        )

                        if ai_selected_index is None:
                            # 检查是否启用传统匹配兜底
                            ai_fallback_enabled = (await config_manager.get("aiFallbackEnabled", "true")).lower() == 'true'
                            if ai_fallback_enabled:
                                logger.info("AI匹配未找到合适结果，降级到传统匹配")
                            else:
                                logger.warning("AI匹配未找到合适结果，且传统匹配兜底已禁用，将不使用任何结果")

                    except Exception as e:
                        # 检查是否启用传统匹配兜底
                        ai_fallback_enabled = (await config_manager.get("aiFallbackEnabled", "true")).lower() == 'true'
                        if ai_fallback_enabled:
                            logger.error(f"AI匹配失败，降级到传统匹配: {e}", exc_info=True)
                        else:
                            logger.error(f"AI匹配失败，且传统匹配兜底已禁用: {e}", exc_info=True)
                        ai_selected_index = None

                # 检查是否启用顺延机制
                fallback_enabled = (await config_manager.get("externalApiFallbackEnabled", "false")).lower() == 'true'

                best_match = None

                # 如果AI选择成功，使用AI选择的结果
                if ai_selected_index is not None:
                    best_match = sorted_results[ai_selected_index]
                    logger.info(f"  - 使用AI选择的结果: {best_match.provider} - {best_match.title}")
                elif ai_match_enabled:
                    # AI匹配已启用但失败，检查是否允许降级到传统匹配
                    ai_fallback_enabled = (await config_manager.get("aiFallbackEnabled", "true")).lower() == 'true'
                    if not ai_fallback_enabled:
                        logger.warning("AI匹配失败且传统匹配兜底已禁用，匹配后备失败")
                        return DandanMatchResponse(isMatched=False, matches=[])
                    # 允许降级，继续使用传统匹配
                    logger.info("AI匹配失败，使用传统匹配兜底")
                    # 传统匹配: 优先查找精确标记源 (需验证类型匹配和标题相似度)
                    favorited_match = None
                    for result in sorted_results:
                        key = f"{result.provider}:{result.mediaId}"
                        if favorited_info.get(key):
                            # 验证类型匹配和标题相似度
                            type_matched = result.type == target_type
                            similarity = fuzz.token_set_ratio(base_title, result.title)
                            logger.info(f"  - 找到精确标记源: {result.provider} - {result.title} "
                                       f"(类型: {result.type}, 类型匹配: {'✓' if type_matched else '✗'}, 相似度: {similarity}%)")

                            # 必须满足：类型匹配 AND 相似度 >= 70%
                            if type_matched and similarity >= 70:
                                favorited_match = result
                                logger.info(f"  - 精确标记源验证通过 (类型匹配: ✓, 相似度: {similarity}% >= 70%)")
                                break
                            else:
                                logger.warning(f"  - 精确标记源验证失败 (类型匹配: {'✓' if type_matched else '✗'}, "
                                             f"相似度: {similarity}% {'<' if similarity < 70 else '>='} 70%)，跳过")

                    if favorited_match:
                        best_match = favorited_match
                        logger.info(f"  - 使用精确标记源: {best_match.provider} - {best_match.title}")
                    elif not fallback_enabled:
                        # 顺延机制关闭，验证第一个结果是否满足条件
                        if sorted_results:
                            first_result = sorted_results[0]
                            type_matched = first_result.type == target_type
                            similarity = fuzz.token_set_ratio(base_title, first_result.title)
                            score = calculate_match_score(first_result)

                            # 必须满足：类型匹配 AND 相似度 >= 70%
                            if type_matched and similarity >= 70:
                                best_match = first_result
                                logger.info(f"  - 传统匹配成功: {first_result.provider} - {first_result.title} "
                                           f"(类型匹配: ✓, 相似度: {similarity}%, 总分: {score})")
                            else:
                                best_match = None
                                logger.warning(f"  - 传统匹配失败: 第一个结果不满足条件 "
                                             f"(类型匹配: {'✓' if type_matched else '✗'}, 相似度: {similarity}%, 要求: ≥70%)")
                        else:
                            best_match = None
                            logger.warning("  - 传统匹配失败: 没有搜索结果")
                else:
                    # AI未启用，使用传统匹配
                    # 传统匹配: 优先查找精确标记源 (需验证类型匹配和标题相似度)
                    favorited_match = None
                    for result in sorted_results:
                        key = f"{result.provider}:{result.mediaId}"
                        if favorited_info.get(key):
                            # 验证类型匹配和标题相似度
                            type_matched = result.type == target_type
                            similarity = fuzz.token_set_ratio(base_title, result.title)
                            logger.info(f"  - 找到精确标记源: {result.provider} - {result.title} "
                                       f"(类型: {result.type}, 类型匹配: {'✓' if type_matched else '✗'}, 相似度: {similarity}%)")

                            # 必须满足：类型匹配 AND 相似度 >= 70%
                            if type_matched and similarity >= 70:
                                favorited_match = result
                                logger.info(f"  - 精确标记源验证通过 (类型匹配: ✓, 相似度: {similarity}% >= 70%)")
                                break
                            else:
                                logger.warning(f"  - 精确标记源验证失败 (类型匹配: {'✓' if type_matched else '✗'}, "
                                             f"相似度: {similarity}% {'<' if similarity < 70 else '>='} 70%)，跳过")

                    if favorited_match:
                        best_match = favorited_match
                        logger.info(f"  - 使用精确标记源: {best_match.provider} - {best_match.title}")
                    elif not fallback_enabled:
                        # 顺延机制关闭，验证第一个结果是否满足条件
                        if sorted_results:
                            first_result = sorted_results[0]
                            type_matched = first_result.type == target_type
                            similarity = fuzz.token_set_ratio(base_title, first_result.title)
                            score = calculate_match_score(first_result)

                            # 必须满足：类型匹配 AND 相似度 >= 70%
                            if type_matched and similarity >= 70:
                                best_match = first_result
                                logger.info(f"  - 传统匹配成功: {first_result.provider} - {first_result.title} "
                                           f"(类型匹配: ✓, 相似度: {similarity}%, 总分: {score})")
                            else:
                                best_match = None
                                logger.warning(f"  - 传统匹配失败: 第一个结果不满足条件 "
                                             f"(类型匹配: {'✓' if type_matched else '✗'}, 相似度: {similarity}%, 要求: ≥70%)")
                        else:
                            best_match = None
                            logger.warning("  - 传统匹配失败: 没有搜索结果")

                if best_match is None and fallback_enabled:
                    # 顺延机制启用：依次验证候选源 (按分数从高到低)
                    logger.info(f"  - 顺延机制启用，依次验证候选源")
                    for attempt, candidate in enumerate(sorted_results, 1):
                        logger.info(f"    {attempt}. 正在验证: {candidate.provider} - {candidate.title} (ID: {candidate.mediaId}, 类型: {candidate.type})")
                        try:
                            scraper = scraper_manager.get_scraper(candidate.provider)
                            if not scraper:
                                logger.warning(f"    {attempt}. {candidate.provider} - 无法获取scraper，跳过")
                                continue

                            # 获取分集列表进行验证
                            episodes = await scraper.get_episodes(candidate.mediaId, db_media_type=candidate.type)
                            if not episodes:
                                logger.warning(f"    {attempt}. {candidate.provider} - 没有分集列表，跳过")
                                continue

                            # 如果用户搜索的是电影，只匹配电影类型的候选源
                            if is_movie:
                                if candidate.type != "movie":
                                    logger.warning(f"    {attempt}. {candidate.provider} - 类型不匹配 (搜索电影，但候选源是{candidate.type})，跳过")
                                    continue
                                logger.info(f"    {attempt}. {candidate.provider} - 验证通过 (电影)")
                            # 如果指定了集数，检查是否有目标集数
                            elif episode_number is not None:
                                target_episode = None
                                for ep in episodes:
                                    if ep.episodeIndex == episode_number:
                                        target_episode = ep
                                        break

                                if not target_episode:
                                    logger.warning(f"    {attempt}. {candidate.provider} - 没有第 {episode_number} 集，跳过")
                                    continue

                                logger.info(f"    {attempt}. {candidate.provider} - 验证通过")
                            else:
                                logger.info(f"    {attempt}. {candidate.provider} - 验证通过")
                            best_match = candidate
                            break
                        except Exception as e:
                            logger.warning(f"    {attempt}. {candidate.provider} - 验证失败: {e}")
                            continue

                if not best_match:
                    logger.warning(f"匹配后备失败：所有候选源都无法提供有效分集")
                    match_timer.step_end(details="无有效分集")
                    match_timer.finish()  # 打印计时报告
                    response = DandanMatchResponse(isMatched=False, matches=[])
                    match_fallback_result["response"] = response
                    return

                # 步骤4：应用入库后处理规则
                logger.info(f"步骤4：应用入库后处理规则")
                final_title = best_match.title
                final_season = season if season is not None else 1  # 默认为第1季
                if title_recognition_manager:
                    converted_title, converted_season, was_converted, _ = await title_recognition_manager.apply_storage_postprocessing(
                        best_match.title, season, best_match.provider
                    )
                    if was_converted:
                        final_title = converted_title
                        final_season = converted_season if converted_season is not None else 1
                        logger.info(f"  - 应用入库后处理: '{best_match.title}' S{season or 1:02d} -> '{final_title}' S{final_season:02d}")

                # 步骤5：分配虚拟animeId和真实episodeId
                logger.info(f"步骤5：分配虚拟animeId和真实episodeId")

                # 分配虚拟animeId
                virtual_anime_id = await _get_next_virtual_anime_id(session_inner)
                logger.info(f"  - 分配虚拟animeId: {virtual_anime_id}")

                # 分配真实anime_id（用于生成episodeId）
                # 注意：Anime和AnimeSource已经在orm_models中定义，这里直接使用
                stmt = select(orm_models.Anime.id, orm_models.Anime.title).where(
                    orm_models.Anime.title == final_title,
                    orm_models.Anime.season == final_season
                )
                result = await session_inner.execute(stmt)
                existing_db_anime = result.mappings().first()

                if existing_db_anime:
                    real_anime_id = existing_db_anime['id']
                    logger.info(f"  - 复用已存在的番剧: '{final_title}' (real_anime_id={real_anime_id})")
                else:
                    real_anime_id = await get_next_real_anime_id(session_inner)
                    logger.info(f"  - 分配新的real_anime_id: {real_anime_id}")

                # 获取或创建source，以获取正确的source_order
                # 检查是否已有该源
                source_stmt = select(orm_models.AnimeSource.id, orm_models.AnimeSource.sourceOrder).where(
                    orm_models.AnimeSource.animeId == real_anime_id,
                    orm_models.AnimeSource.providerName == best_match.provider,
                    orm_models.AnimeSource.mediaId == best_match.mediaId
                )
                source_result = await session_inner.execute(source_stmt)
                existing_source = source_result.mappings().first()

                if existing_source:
                    source_order = existing_source['sourceOrder']
                    logger.info(f"  - 复用已存在的源: source_order={source_order}")
                else:
                    # 查找当前最大的source_order
                    max_order_stmt = select(func.max(orm_models.AnimeSource.sourceOrder)).where(orm_models.AnimeSource.animeId == real_anime_id)
                    max_order_result = await session_inner.execute(max_order_stmt)
                    current_max_order = max_order_result.scalar_one_or_none()
                    logger.debug(f"  - 查询到的最大source_order: {current_max_order}")
                    source_order = (current_max_order or 0) + 1
                    logger.info(f"  - 分配新的source_order: {source_order}")

                # 生成真实episodeId (电影使用1作为episode_number)
                final_episode_number = 1 if is_movie else episode_number
                real_episode_id = _generate_episode_id(real_anime_id, source_order, final_episode_number)
                logger.info(f"  - 生成真实episodeId: {real_episode_id}")

                # 步骤6：存储映射关系到数据库缓存
                mapping_key = f"fallback_anime_{virtual_anime_id}"
                mapping_data = {
                    "real_anime_id": real_anime_id,
                    "provider": best_match.provider,
                    "mediaId": best_match.mediaId,
                    "final_title": final_title,
                    "final_season": final_season,
                    "media_type": best_match.type,
                    "imageUrl": best_match.imageUrl,
                    "year": best_match.year,
                    "timestamp": time.time()
                }
                await set_db_cache(session, FALLBACK_SEARCH_CACHE_PREFIX, mapping_key, mapping_data, FALLBACK_SEARCH_CACHE_TTL)

                # 存储episodeId映射
                episode_mapping_key = f"fallback_episode_{real_episode_id}"
                episode_mapping_data = {
                    "virtual_anime_id": virtual_anime_id,
                    "real_anime_id": real_anime_id,
                    "provider": best_match.provider,
                    "mediaId": best_match.mediaId,
                    "episode_number": final_episode_number,  # 使用final_episode_number (电影为1)
                    "final_title": final_title,
                    "final_season": final_season,
                    "media_type": best_match.type,
                    "imageUrl": best_match.imageUrl,
                    "year": best_match.year,
                    "timestamp": time.time()
                }
                await set_db_cache(session, FALLBACK_SEARCH_CACHE_PREFIX, episode_mapping_key, episode_mapping_data, FALLBACK_SEARCH_CACHE_TTL)

                logger.info(f"匹配后备完成: virtual_anime_id={virtual_anime_id}, real_anime_id={real_anime_id}, episodeId={real_episode_id}")

                # 方案A: 写入数据库 - 创建anime和episode记录
                try:
                    logger.info("开始将后备匹配结果写入数据库...")

                    # 检查anime是否已存在
                    stmt = select(orm_models.Anime).where(orm_models.Anime.id == real_anime_id)
                    result = await session_inner.execute(stmt)
                    existing_anime = result.scalar_one_or_none()

                    if not existing_anime:
                        # 创建anime条目
                        logger.info(f"创建anime条目: id={real_anime_id}, title='{final_title}'")
                        new_anime = orm_models.Anime(
                            id=real_anime_id,
                            title=final_title,
                            type=best_match.type,
                            season=final_season,
                            imageUrl=best_match.imageUrl,
                            year=best_match.year,
                            createdAt=get_now()
                        )
                        session_inner.add(new_anime)
                        await session_inner.flush()
                        # 同步PostgreSQL序列
                        await sync_postgres_sequence(session_inner)
                    else:
                        logger.info(f"anime条目已存在: id={real_anime_id}, title='{existing_anime.title}'")

                    # 创建或获取source关联
                    source_id = await crud.link_source_to_anime(session_inner, real_anime_id, best_match.provider, best_match.mediaId)
                    logger.info(f"source_id={source_id}, provider={best_match.provider}, mediaId={best_match.mediaId}")

                    # 创建episode记录
                    episode_title = f"第{final_episode_number}集" if not is_movie else final_title
                    episode_db_id = await crud.create_episode_if_not_exists(
                        session_inner,
                        real_anime_id,
                        source_id,
                        final_episode_number,
                        episode_title,
                        None,  # url
                        f"fallback_{best_match.provider}_{best_match.mediaId}_{final_episode_number}"  # provider_episode_id
                    )
                    logger.info(f"episode记录已创建/获取: episode_db_id={episode_db_id}")

                    # 提交数据库更改
                    await session_inner.commit()
                    logger.info("后备匹配结果已成功写入数据库")

                except Exception as db_error:
                    logger.error(f"写入数据库失败: {db_error}", exc_info=True)
                    await session_inner.rollback()
                    # 即使数据库写入失败,也继续返回结果(依赖缓存)

                # 返回真实的匹配结果
                match_result = DandanMatchInfo(
                    episodeId=real_episode_id,
                    animeId=virtual_anime_id,  # 返回虚拟animeId
                    animeTitle=final_title,
                    episodeTitle=f"第{episode_number}集" if not parsed_info.get("is_movie") else final_title,
                    type="tvseries" if not parsed_info.get("is_movie") else "movie",
                    typeDescription="匹配成功",
                    imageUrl=best_match.imageUrl
                )
                response = DandanMatchResponse(isMatched=True, matches=[match_result])
                logger.info(f"发送匹配响应 (匹配后备): episodeId={real_episode_id}, animeId={virtual_anime_id}")

                # 存储到防重复缓存
                recent_fallback_key = f"recent_fallback_{parsed_info['title']}_{parsed_info.get('season')}_{parsed_info.get('episode')}"
                recent_fallback_data = {
                    "response": response,
                    "timestamp": time.time()
                }
                await set_db_cache(session, FALLBACK_SEARCH_CACHE_PREFIX, recent_fallback_key, recent_fallback_data, 300)  # 5分钟TTL

                match_timer.step_end(details="匹配成功")
                match_timer.finish()  # 打印计时报告
                match_fallback_result["response"] = response
            except Exception as e:
                logger.error(f"匹配后备失败: {e}", exc_info=True)
                match_timer.step_end(details=f"失败: {e}")
                match_timer.finish()  # 打印计时报告
                response = DandanMatchResponse(isMatched=False, matches=[])
                match_fallback_result["response"] = response

        # 提交匹配后备任务(立即执行模式)
        try:
            task_title = f"匹配后备: {item.fileName}"
            task_id, done_event = await task_manager.submit_task(
                match_fallback_coro_factory,
                task_title,
                run_immediately=True,
                queue_type="fallback"
            )
            logger.info(f"匹配后备任务已提交: {task_id}")

            # 等待任务完成(最多30秒)
            try:
                await asyncio.wait_for(done_event.wait(), timeout=30.0)
                logger.info(f"匹配后备任务完成: {task_id}")
            except asyncio.TimeoutError:
                logger.warning(f"匹配后备任务超时: {task_id}")
                match_fallback_result["response"] = DandanMatchResponse(isMatched=False, matches=[])

            # 返回结果
            if match_fallback_result["response"]:
                return match_fallback_result["response"]
            else:
                return DandanMatchResponse(isMatched=False, matches=[])
        except Exception as e:
            logger.error(f"提交匹配后备任务失败: {e}", exc_info=True)
            return DandanMatchResponse(isMatched=False, matches=[])

    response = DandanMatchResponse(isMatched=False, matches=[])
    logger.info(f"发送匹配响应 (所有方法均未匹配): {response.model_dump_json(indent=2)}")
    return response


# ==================== 路由定义 ====================

# 创建匹配路由器
match_router = APIRouter(route_class=DandanApiRoute)


@match_router.post(
    "/match",
    response_model=DandanMatchResponse,
    summary="[dandanplay兼容] 匹配单个文件"
)
async def match_single_file(
    request: DandanBatchMatchRequestItem,
    token: str = Depends(get_token_from_path),
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    scraper_manager: ScraperManager = Depends(get_scraper_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    config_manager: ConfigManager = Depends(get_config_manager),
    ai_matcher_manager: AIMatcherManager = Depends(get_ai_matcher_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter),
    title_recognition_manager = Depends(get_title_recognition_manager)
):
    """
    通过文件名匹配弹幕库。此接口不使用文件Hash。
    优先进行库内直接匹配，失败后回退到TMDB剧集组映射。
    """
    return await get_match_for_item(
        request, session, task_manager, scraper_manager,
        metadata_manager, config_manager, ai_matcher_manager, rate_limiter, title_recognition_manager,
        current_token=token
    )


@match_router.post(
    "/match/batch",
    response_model=List[DandanMatchResponse],
    summary="[dandanplay兼容] 批量匹配文件"
)
async def match_batch_files(
    request: DandanBatchMatchRequest,
    token: str = Depends(get_token_from_path),
    session: AsyncSession = Depends(get_db_session),
    task_manager: TaskManager = Depends(get_task_manager),
    scraper_manager: ScraperManager = Depends(get_scraper_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    config_manager: ConfigManager = Depends(get_config_manager),
    ai_matcher_manager: AIMatcherManager = Depends(get_ai_matcher_manager),
    rate_limiter: RateLimiter = Depends(get_rate_limiter),
    title_recognition_manager = Depends(get_title_recognition_manager)
):
    """
    批量匹配文件。
    """
    if len(request.requests) > 32:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="批量匹配请求不能超过32个文件。")

    tasks = [
        get_match_for_item(
            item, session, task_manager, scraper_manager, metadata_manager, config_manager, ai_matcher_manager, rate_limiter, title_recognition_manager,
            current_token=token
        ) for item in request.requests
    ]
    results = await asyncio.gather(*tasks)
    return results