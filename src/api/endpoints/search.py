"""
Searchç›¸å…³çš„APIç«¯ç‚¹
"""

import re
from typing import Optional, List, Any, Dict, Callable, Union
import asyncio
import secrets
import hashlib
import importlib
import string
import time
import json
from urllib.parse import urlparse, urlunparse, quote, unquote
import logging

from datetime import datetime
from sqlalchemy import update, select, func, exc
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload
import httpx
from ...rate_limiter import RateLimiter, RateLimitExceededError
from ...config_manager import ConfigManager
from pydantic import BaseModel, Field, model_validator
from fastapi import APIRouter, Body, Depends, HTTPException, Query, Request, status, Response
from fastapi.security import OAuth2PasswordRequestForm

from ... import crud, models, orm_models, security, scraper_manager
from src import models as api_models
from ...log_manager import get_logs
from ...task_manager import TaskManager, TaskSuccess, TaskStatus
from ...metadata_manager import MetadataSourceManager
from ...scraper_manager import ScraperManager
from ... import tasks
from ...utils import parse_search_keyword
from ...season_mapper import ai_type_and_season_mapping_and_correction
from ...webhook_manager import WebhookManager
from ...image_utils import download_image
from ...scheduler import SchedulerManager
from ...title_recognition import TitleRecognitionManager
from ..._version import APP_VERSION
from thefuzz import fuzz
from ...config import settings
from ...timezone import get_now
from ...database import get_db_session
from ...search_utils import unified_search
from ...search_timer import SearchTimer, SEARCH_TYPE_HOME

logger = logging.getLogger(__name__)


from ..dependencies import (
    get_scraper_manager, get_task_manager, get_scheduler_manager,
    get_webhook_manager, get_metadata_manager, get_config_manager,
    get_rate_limiter, get_title_recognition_manager, get_ai_matcher_manager
)
from ...ai.ai_matcher_manager import AIMatcherManager
from ...season_mapper import title_contains_season_name, ai_season_mapping_and_correction

from ..ui_models import (
    UITaskResponse, UIProviderSearchResponse, RefreshPosterRequest,
    ReassociationRequest, BulkDeleteEpisodesRequest, BulkDeleteRequest,
    ProxyTestResult, ProxyTestRequest, FullProxyTestResponse,
    TitleRecognitionContent, TitleRecognitionUpdateResponse,
    ApiTokenUpdate, CustomDanmakuPathRequest, CustomDanmakuPathResponse,
    MatchFallbackTokensResponse, ConfigValueResponse, ConfigValueRequest,
    TmdbReverseLookupConfig, TmdbReverseLookupConfigRequest,
    ImportFromUrlRequest, GlobalFilterSettings,
    RateLimitProviderStatus, FallbackRateLimitStatus, RateLimitStatusResponse,
    WebhookSettings, WebhookTaskItem, PaginatedWebhookTasksResponse,
    AITestRequest, AITestResponse
)
router = APIRouter()

@router.get(
    "/search/anime",
    response_model=models.AnimeSearchResponse,
    summary="æœç´¢æœ¬åœ°æ•°æ®åº“ä¸­çš„èŠ‚ç›®ä¿¡æ¯",
)
async def search_anime_local(
    keyword: str = Query(..., min_length=1, description="æœç´¢å…³é”®è¯"),
    session: AsyncSession = Depends(get_db_session)
):
    db_results = await crud.search_anime(session, keyword)
    animes = [
        models.AnimeInfo(animeId=item["id"], animeTitle=item["title"], type=item["type"])
        for item in db_results
    ]
    return models.AnimeSearchResponse(animes=animes)

@router.get("/search/provider", response_model=UIProviderSearchResponse, summary="ä»å¤–éƒ¨æ•°æ®æºæœç´¢èŠ‚ç›®")
async def search_anime_provider(
    request: Request,
    keyword: str = Query(..., min_length=1, description="æœç´¢å…³é”®è¯"),
    manager: ScraperManager = Depends(get_scraper_manager),
    current_user: models.User = Depends(security.get_current_user),
    session: AsyncSession = Depends(get_db_session),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    title_recognition_manager: TitleRecognitionManager = Depends(get_title_recognition_manager),
    config_manager: ConfigManager = Depends(get_config_manager),
    ai_matcher_manager: AIMatcherManager = Depends(get_ai_matcher_manager)
):
    """
    ä»æ‰€æœ‰å·²é…ç½®çš„æ•°æ®æºï¼ˆå¦‚è…¾è®¯ã€Bç«™ç­‰ï¼‰æœç´¢èŠ‚ç›®ä¿¡æ¯ã€‚
    æ­¤æ¥å£å®ç°äº†æ™ºèƒ½çš„æŒ‰å­£ç¼“å­˜æœºåˆ¶ï¼Œå¹¶ä¿ç•™äº†åŸæœ‰çš„åˆ«åæœç´¢ã€è¿‡æ»¤å’Œæ’åºé€»è¾‘ã€‚
    """
    # ğŸš€ V2.1.6: åˆ›å»ºæœç´¢è®¡æ—¶å™¨
    timer = SearchTimer(SEARCH_TYPE_HOME, keyword, logger)
    timer.start()

    try:
        timer.step_start("å…³é”®è¯è§£æ")
        parsed_keyword = parse_search_keyword(keyword)
        original_title = parsed_keyword["title"]
        season_to_filter = parsed_keyword["season"]
        episode_to_filter = parsed_keyword["episode"]
        timer.step_end()

        # åº”ç”¨æœç´¢é¢„å¤„ç†è§„åˆ™
        timer.step_start("é¢„å¤„ç†è§„åˆ™åº”ç”¨")
        search_title = original_title
        search_season = season_to_filter
        if title_recognition_manager:
            processed_title, processed_episode, processed_season, preprocessing_applied = await title_recognition_manager.apply_search_preprocessing(original_title, episode_to_filter, season_to_filter)
            if preprocessing_applied:
                search_title = processed_title
                logger.info(f"âœ“ WebUIæœç´¢é¢„å¤„ç†: '{original_title}' -> '{search_title}'")
                # å¦‚æœé›†æ•°å‘ç”Ÿäº†å˜åŒ–ï¼Œæ›´æ–°episode_to_filter
                if processed_episode != episode_to_filter:
                    episode_to_filter = processed_episode
                    logger.info(f"âœ“ WebUIé›†æ•°é¢„å¤„ç†: {parsed_keyword['episode']} -> {episode_to_filter}")
                # å¦‚æœå­£æ•°å‘ç”Ÿäº†å˜åŒ–ï¼Œæ›´æ–°season_to_filter
                if processed_season != season_to_filter:
                    search_season = processed_season
                    season_to_filter = processed_season
                    logger.info(f"âœ“ WebUIå­£åº¦é¢„å¤„ç†: {parsed_keyword['season']} -> {season_to_filter}")
            else:
                logger.info(f"â—‹ WebUIæœç´¢é¢„å¤„ç†æœªç”Ÿæ•ˆ: '{original_title}'")
        timer.step_end()

        # --- æ–°å¢ï¼šæŒ‰å­£ç¼“å­˜é€»è¾‘ ---
        timer.step_start("ç¼“å­˜æ£€æŸ¥")
        # ç¼“å­˜é”®åŸºäºæ ¸å¿ƒæ ‡é¢˜å’Œå­£åº¦ï¼Œå…è®¸åœ¨åŒä¸€å­£çš„ä¸åŒåˆ†é›†æœç´¢ä¸­å¤ç”¨ç¼“å­˜
        cache_key = f"provider_search_{search_title}_{season_to_filter or 'all'}"
        supplemental_cache_key = f"supplemental_search_{search_title}"
        cached_results_data = await crud.get_cache(session, cache_key)
        cached_supplemental_results = await crud.get_cache(session, supplemental_cache_key)

        if cached_results_data is not None and cached_supplemental_results is not None:
            logger.info(f"æœç´¢ç¼“å­˜å‘½ä¸­: '{cache_key}'")
            timer.step_end(details="ç¼“å­˜å‘½ä¸­")
            # ç¼“å­˜æ•°æ®å·²æ’åºå’Œè¿‡æ»¤ï¼Œåªéœ€æ›´æ–°å½“å‰è¯·æ±‚çš„é›†æ•°ä¿¡æ¯
            results = [models.ProviderSearchInfo.model_validate(item) for item in cached_results_data]
            for item in results:
                item.currentEpisodeIndex = episode_to_filter

            timer.finish()  # æ‰“å°è®¡æ—¶æŠ¥å‘Š
            return UIProviderSearchResponse(
                results=[item.model_dump() for item in results],
                supplemental_results=[models.ProviderSearchInfo.model_validate(item).model_dump() for item in cached_supplemental_results],
                search_season=season_to_filter,
                search_episode=episode_to_filter
            )

        timer.step_end(details="ç¼“å­˜æœªå‘½ä¸­")
        logger.info(f"æœç´¢ç¼“å­˜æœªå‘½ä¸­: '{cache_key}'ï¼Œæ­£åœ¨æ‰§è¡Œå®Œæ•´æœç´¢æµç¨‹...")
        # --- ç¼“å­˜é€»è¾‘ç»“æŸ ---

        # V2.1.6: ä½¿ç”¨ç»Ÿä¸€çš„ai_type_and_season_mapping_and_correctionå‡½æ•°

        # è·å–AIåŒ¹é…å™¨ç”¨äºç»Ÿä¸€çš„å­£åº¦æ˜ å°„
        ai_matcher = await ai_matcher_manager.get_matcher() if ai_matcher_manager else None

        episode_info = {
            "season": season_to_filter,
            "episode": episode_to_filter
        } if episode_to_filter is not None else None

        logger.info(f"ç”¨æˆ· '{current_user.username}' æ­£åœ¨æœç´¢: '{keyword}' (è§£æä¸º: title='{search_title}', season={season_to_filter}, episode={episode_to_filter})")

        

        # ç¬¬ä¸€æ¬¡æ£€æŸ¥:åœ¨æ‰€æœ‰æœç´¢ä¹‹å‰æ£€æŸ¥æ˜¯å¦æœ‰å¼¹å¹•æº
        if not manager.has_enabled_scrapers:
            logger.warning("âŒ æ²¡æœ‰å¯ç”¨çš„å¼¹å¹•æœç´¢æºï¼Œç»ˆæ­¢æœ¬æ¬¡æœç´¢")
            logger.info("è¯·åœ¨'æœç´¢æº-å¼¹å¹•æœç´¢æº'é¡µé¢ä¸­è‡³å°‘å¯ç”¨ä¸€ä¸ªå¼¹å¹•æºï¼Œå¦‚æœæ²¡æœ‰å¼¹å¹•æºè¯·ä»èµ„æºä»“åº“ä¸­åŠ è½½")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="æ²¡æœ‰å¯ç”¨çš„å¼¹å¹•æœç´¢æºï¼Œè¯·åœ¨â€œæœç´¢æºâ€é¡µé¢ä¸­å¯ç”¨è‡³å°‘ä¸€ä¸ªã€‚"
            )

        # --- åŸæœ‰çš„å¤æ‚æœç´¢æµç¨‹å¼€å§‹ ---
        # 1. è·å–åˆ«åå’Œè¡¥å……ç»“æœ
        # ä¿®æ­£ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å¯ç”¨çš„è¾…åŠ©æºæˆ–å¼ºåˆ¶è¾…åŠ©æº
        has_any_aux_source = await metadata_manager.has_any_enabled_aux_source()

        # ğŸš€ V2.1.6ä¼˜åŒ–: æå‰å¯åŠ¨å…ƒæ•°æ®æŸ¥è¯¢ï¼Œä¸æœç´¢å¹¶è¡Œ
        metadata_prefetch_task = None
        if ai_matcher and metadata_manager:
            async def prefetch_metadata():
                try:
                    from ...season_mapper import _get_cached_metadata_search
                    return await _get_cached_metadata_search(search_title, metadata_manager, logger)
                except Exception:
                    return None
            metadata_prefetch_task = asyncio.create_task(prefetch_metadata())

        if not has_any_aux_source:
            logger.info("æœªé…ç½®æˆ–æœªå¯ç”¨ä»»ä½•æœ‰æ•ˆçš„è¾…åŠ©æœç´¢æºï¼Œç›´æ¥è¿›è¡Œå…¨ç½‘æœç´¢ã€‚")
            supplemental_results = []
            # ä¿®æ­£:å˜é‡åç»Ÿä¸€
            timer.step_start("å¼¹å¹•æºæœç´¢")
            all_results = await manager.search_all([search_title], episode_info=episode_info)
            # æ”¶é›†å•æºæœç´¢è€—æ—¶ä¿¡æ¯
            from ...search_timer import SubStepTiming
            source_timing_sub_steps = [
                SubStepTiming(name=name, duration_ms=dur, result_count=cnt)
                for name, dur, cnt in manager.last_search_timing
            ]
            timer.step_end(details=f"{len(all_results)}ä¸ªç»“æœ", sub_steps=source_timing_sub_steps)
            logger.info(f"ç›´æ¥æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(all_results)} ä¸ªåŸå§‹ç»“æœã€‚")
            filter_aliases = {search_title} # ç¡®ä¿è‡³å°‘æœ‰åŸå§‹æ ‡é¢˜ç”¨äºåç»­å¤„ç†
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¼¹å¹•æº - åœ¨è¾…åŠ©æœç´¢ä¹‹å‰å…ˆæ£€æŸ¥
            if not manager.has_enabled_scrapers:
                logger.warning("âŒ è¾…åŠ©æœç´¢å·²å¯ç”¨ï¼Œä½†æ²¡æœ‰å¯ç”¨çš„å¼¹å¹•æœç´¢æºï¼Œç»ˆæ­¢æœ¬æ¬¡æœç´¢")
                logger.info("è¯·åœ¨'æœç´¢æº-å¼¹å¹•æœç´¢æº'é¡µé¢ä¸­è‡³å°‘å¯ç”¨ä¸€ä¸ªå¼¹å¹•æºï¼Œå¦‚æœæ²¡æœ‰å¼¹å¹•æºè¯·ä»èµ„æºä»“åº“ä¸­åŠ è½½")
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail='æ²¡æœ‰å¯ç”¨çš„å¼¹å¹•æœç´¢æºï¼Œè¯·åœ¨"æœç´¢æº-å¼¹å¹•æœç´¢æº"é¡µé¢ä¸‹ä¸­è‡³å°‘å¯ç”¨ä¸€ä¸ªï¼Œå¦‚æœæ²¡æœ‰å¼¹å¹•æºå°±ä»èµ„æºä»“åº“ä¸­åŠ è½½å¼¹å¹•æºã€‚'
                )

            logger.info("ä¸€ä¸ªæˆ–å¤šä¸ªå…ƒæ•°æ®æºå·²å¯ç”¨è¾…åŠ©æœç´¢ï¼Œå¼€å§‹æ‰§è¡Œ...")
            # ä¿®æ­£ï¼šå¢åŠ ä¸€ä¸ªâ€œé˜²ç«å¢™â€æ¥éªŒè¯ä»å…ƒæ•°æ®æºè¿”å›çš„åˆ«åï¼Œé˜²æ­¢å› æ¨¡ç³ŠåŒ¹é…å¯¼è‡´çš„ç»“æœæ±¡æŸ“ã€‚
            # ä¼˜åŒ–ï¼šå¹¶è¡Œæ‰§è¡Œè¾…åŠ©æœç´¢å’Œä¸»æœç´¢
            logger.info(f"å°†ä½¿ç”¨è§£æåçš„æ ‡é¢˜ '{search_title}' è¿›è¡Œå…¨ç½‘æœç´¢...")

            timer.step_start("å¹¶è¡Œæœç´¢(å¼¹å¹•æº+è¾…åŠ©æº)")
            # 1. å¹¶è¡Œå¯åŠ¨ä¸¤ä¸ªä»»åŠ¡
            main_task = asyncio.create_task(
                manager.search_all([search_title], episode_info=episode_info)
            )

            supp_task = asyncio.create_task(
                metadata_manager.search_supplemental_sources(search_title, current_user)
            )

            # 2. ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡éƒ½å®Œæˆ
            all_results, (all_possible_aliases, supplemental_results) = await asyncio.gather(
                main_task, supp_task
            )
            timer.step_end(details=f"å¼¹å¹•{len(all_results)}ä¸ª+è¾…åŠ©{len(supplemental_results)}ä¸ª")

            timer.step_start("åˆ«åéªŒè¯ä¸è¿‡æ»¤")
            # 3. éªŒè¯æ¯ä¸ªåˆ«åä¸åŸå§‹æœç´¢è¯çš„ç›¸ä¼¼åº¦
            validated_aliases = set()
            for alias in all_possible_aliases:
                # ä½¿ç”¨ token_set_ratio å¹¶è®¾ç½®ä¸€ä¸ªåˆç†çš„é˜ˆå€¼ï¼ˆä¾‹å¦‚70ï¼‰ï¼Œä»¥å…è®¸å°çš„å·®å¼‚ä½†è¿‡æ»¤æ‰å®Œå…¨ä¸ç›¸å…³çš„ç»“æœã€‚
                if fuzz.token_set_ratio(search_title, alias) > 70:
                    validated_aliases.add(alias)
                else:
                    logger.debug(f"åˆ«åéªŒè¯ï¼šå·²ä¸¢å¼ƒä½ç›¸ä¼¼åº¦çš„åˆ«å '{alias}' (ä¸ '{search_title}' ç›¸æ¯”)")
            
            # 4. ä½¿ç”¨ç»è¿‡éªŒè¯çš„åˆ«ååˆ—è¡¨è¿›è¡Œåç»­æ“ä½œ
            filter_aliases = validated_aliases
            filter_aliases.add(search_title) # ç¡®ä¿åŸå§‹æœç´¢è¯æ€»æ˜¯åœ¨åˆ—è¡¨ä¸­
            logger.info(f"æ‰€æœ‰è¾…åŠ©æœç´¢å®Œæˆï¼Œæœ€ç»ˆåˆ«åé›†å¤§å°: {len(filter_aliases)}")

            # æ–°å¢ï¼šæ ¹æ®æ‚¨çš„è¦æ±‚ï¼Œæ‰“å°æœ€ç»ˆçš„åˆ«ååˆ—è¡¨ä»¥ä¾›è°ƒè¯•
            logger.info(f"ç”¨äºè¿‡æ»¤çš„åˆ«ååˆ—è¡¨: {list(filter_aliases)}")

            def normalize_for_filtering(title: str) -> str:
                if not title: return ""
                title = re.sub(r'[\[ã€(ï¼ˆ].*?[\]ã€‘)ï¼‰]', '', title)
                return title.lower().replace(" ", "").replace("ï¼š", ":").strip()

            # ä¿®æ­£ï¼šé‡‡ç”¨æ›´æ™ºèƒ½çš„ä¸¤é˜¶æ®µè¿‡æ»¤ç­–ç•¥
            # é˜¶æ®µ1ï¼šåŸºäºåŸå§‹æœç´¢è¯è¿›è¡Œåˆæ­¥ã€å®½æ¾çš„è¿‡æ»¤ï¼Œä»¥ç¡®ä¿æ‰€æœ‰ç›¸å…³ç³»åˆ—ï¼ˆåŒ…æ‹¬ä¸åŒå­£åº¦å’Œå‰§åœºç‰ˆï¼‰éƒ½è¢«ä¿ç•™ã€‚
            # åªæœ‰å½“ç”¨æˆ·æ˜ç¡®æŒ‡å®šå­£åº¦æ—¶ï¼Œæˆ‘ä»¬æ‰è¿›è¡Œæ›´ä¸¥æ ¼çš„è¿‡æ»¤ã€‚
            normalized_filter_aliases = {normalize_for_filtering(alias) for alias in filter_aliases if alias}
            filtered_results = []
            excluded_results = []
            for item in all_results:
                normalized_item_title = normalize_for_filtering(item.title)
                if not normalized_item_title: continue

                # æ£€æŸ¥æœç´¢ç»“æœæ˜¯å¦ä¸ä»»ä½•ä¸€ä¸ªåˆ«ååŒ¹é…
                # token_set_ratio æ“…é•¿å¤„ç†å•è¯é¡ºåºä¸åŒå’Œéƒ¨åˆ†å•è¯åŒ¹é…çš„æƒ…å†µã€‚
                # ä¿®æ­£ï¼šä½¿ç”¨ partial_ratio æ¥æ›´å¥½åœ°åŒ¹é…ç»­ä½œå’Œå¤–ä¼  (e.g., "åˆ€å‰‘ç¥åŸŸ" vs "åˆ€å‰‘ç¥åŸŸå¤–ä¼ ")
                # 85 çš„é˜ˆå€¼å¯ä»¥åœ¨ä¿ç•™å¼ºç›¸å…³çš„åŒæ—¶ï¼Œè¿‡æ»¤æ‰å¤§éƒ¨åˆ†æ— å…³ç»“æœã€‚
                if any(fuzz.partial_ratio(normalized_item_title, alias) > 85 for alias in normalized_filter_aliases):
                    filtered_results.append(item)
                else:
                    excluded_results.append(item)

            # èšåˆæ‰“å°è¿‡æ»¤ç»“æœ
            filter_log_lines = [f"åˆ«åè¿‡æ»¤ç»“æœ (ä¿ç•™ {len(filtered_results)}/{len(all_results)}):"]
            for item in excluded_results:
                filter_log_lines.append(f"  - å·²è¿‡æ»¤: {item.title}")
            for item in filtered_results:
                filter_log_lines.append(f"  - {item.title}")
            logger.info("\n".join(filter_log_lines))
            timer.step_end(details=f"ä¿ç•™{len(filtered_results)}ä¸ª")
            results = filtered_results

    except httpx.RequestError as e:
        error_message = f"æœç´¢ '{keyword}' æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}"
        logger.error(error_message, exc_info=True)
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=error_message)

    # è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ ¹æ®æ ‡é¢˜ä¿®æ­£åª’ä½“ç±»å‹
    def is_movie_by_title(title: str) -> bool:
        if not title:
            return False
        # å…³é”®è¯åˆ—è¡¨ï¼Œä¸åŒºåˆ†å¤§å°å†™
        movie_keywords = ["å‰§åœºç‰ˆ", "åŠ‡å ´ç‰ˆ", "movie", "æ˜ ç”»"]
        title_lower = title.lower()
        return any(keyword in title_lower for keyword in movie_keywords)

    # æ–°å¢é€»è¾‘ï¼šæ ¹æ®æ ‡é¢˜å…³é”®è¯ä¿®æ­£åª’ä½“ç±»å‹
    for item in results:
        if item.type == 'tv_series' and is_movie_by_title(item.title):
            logger.info(f"æ ‡é¢˜ '{item.title}' åŒ…å«ç”µå½±å…³é”®è¯ï¼Œç±»å‹ä» 'tv_series' ä¿®æ­£ä¸º 'movie'ã€‚")
            item.type = 'movie'

    # å¦‚æœç”¨æˆ·åœ¨æœç´¢è¯ä¸­æ˜ç¡®æŒ‡å®šäº†å­£åº¦ï¼Œåˆ™å¯¹ç»“æœè¿›è¡Œè¿‡æ»¤
    if season_to_filter:
        original_count = len(results)
        # å½“æŒ‡å®šå­£åº¦æ—¶ï¼Œæˆ‘ä»¬åªå…³å¿ƒç”µè§†å‰§ç±»å‹
        filtered_by_type = [item for item in results if item.type == 'tv_series']
        
        # ç„¶ååœ¨ç”µè§†å‰§ç±»å‹ä¸­ï¼Œæˆ‘ä»¬æŒ‰å­£åº¦å·è¿‡æ»¤
        filtered_by_season = []
        for item in filtered_by_type:
            # ä½¿ç”¨æ¨¡å‹ä¸­å·²è§£æå¥½çš„ season å­—æ®µè¿›è¡Œæ¯”è¾ƒ
            if item.season == season_to_filter:
                filtered_by_season.append(item)
        
        logger.info(f"æ ¹æ®æŒ‡å®šçš„å­£åº¦ ({season_to_filter}) è¿›è¡Œè¿‡æ»¤ï¼Œä» {original_count} ä¸ªç»“æœä¸­ä¿ç•™äº† {len(filtered_by_season)} ä¸ªã€‚")
        results = filtered_by_season

    # ä¿®æ­£ï¼šåœ¨è¿”å›ç»“æœå‰ï¼Œç¡®ä¿ currentEpisodeIndex ä¸æœ¬æ¬¡è¯·æ±‚çš„ episode_info ä¸€è‡´ã€‚
    # è¿™å¯ä»¥é˜²æ­¢å› ç¼“å­˜æˆ–å…¶ä»–åŸå› å¯¼è‡´çš„çŠ¶æ€æ³„éœ²ã€‚
    current_episode_index_for_this_request = episode_info.get("episode") if episode_info else None
    for item in results:
        item.currentEpisodeIndex = current_episode_index_for_this_request

    # æ–°å¢ï¼šæ ¹æ®æœç´¢æºçš„æ˜¾ç¤ºé¡ºåºå’Œæ ‡é¢˜ç›¸ä¼¼åº¦å¯¹ç»“æœè¿›è¡Œæ’åº
    source_settings = await crud.get_all_scraper_settings(session)
    source_order_map = {s['providerName']: s['displayOrder'] for s in source_settings}

    def sort_key(item: models.ProviderSearchInfo):
        provider_order = source_order_map.get(item.provider, 999)
        # ä½¿ç”¨ token_set_ratio æ¥è·å¾—æ›´é²æ£’çš„æ ‡é¢˜ç›¸ä¼¼åº¦è¯„åˆ†
        similarity_score = fuzz.token_set_ratio(search_title, item.title)
        # ä¸»æ’åºé”®ï¼šæºé¡ºåºï¼ˆå‡åºï¼‰ï¼›æ¬¡æ’åºé”®ï¼šç›¸ä¼¼åº¦ï¼ˆé™åºï¼‰
        return (provider_order, -similarity_score)

    timer.step_start("ç»“æœæ’åº")
    sorted_results = sorted(results, key=sort_key)
    timer.step_end(details=f"{len(sorted_results)}ä¸ªç»“æœ")

    # --- æ–°å¢ï¼šåœ¨è¿”å›å‰ç¼“å­˜æœ€ç»ˆç»“æœ ---
    timer.step_start("ç»“æœç¼“å­˜")
    # æˆ‘ä»¬ç¼“å­˜çš„æ˜¯æ•´å­£çš„ç»“æœï¼Œæ‰€ä»¥åœ¨å­˜å…¥å‰æ¸…é™¤ç‰¹å®šé›†æ•°çš„ä¿¡æ¯
    results_to_cache = []
    for item in sorted_results:
        item_copy = item.model_copy(deep=True)
        item_copy.currentEpisodeIndex = None
        results_to_cache.append(item_copy.model_dump())

    if sorted_results:
        await crud.set_cache(session, cache_key, results_to_cache, ttl_seconds=10800)
    # ç¼“å­˜è¡¥å……ç»“æœ
    if supplemental_results:
        await crud.set_cache(session, supplemental_cache_key, [item.model_dump() for item in supplemental_results], ttl_seconds=10800)
    timer.step_end()
    # --- ç¼“å­˜é€»è¾‘ç»“æŸ ---



    # ğŸš€ V2.1.6: ä½¿ç”¨ç»Ÿä¸€çš„AIç±»å‹å’Œå­£åº¦æ˜ å°„ä¿®æ­£å‡½æ•°
    if ai_matcher and metadata_manager:
        try:
            timer.step_start("AIæ˜ å°„ä¿®æ­£")
            logger.info("ğŸ”„ å¼€å§‹AIæ˜ å°„ä¿®æ­£...")
            # è·å–é¢„å–çš„å…ƒæ•°æ®ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            prefetched_metadata = None
            if metadata_prefetch_task:
                try:
                    prefetched_metadata = await metadata_prefetch_task
                except Exception:
                    pass

            mapping_result = await ai_type_and_season_mapping_and_correction(
                search_title=search_title,
                search_results=sorted_results,
                metadata_manager=metadata_manager,
                ai_matcher=ai_matcher,
                logger=logger,
                similarity_threshold=60.0,
                prefetched_metadata_results=prefetched_metadata
            )

            # åº”ç”¨ä¿®æ­£ç»“æœ
            if mapping_result['total_corrections'] > 0:
                logger.info(f"âœ“ ä¸»é¡µæœç´¢ ç»Ÿä¸€AIæ˜ å°„æˆåŠŸ: æ€»è®¡ä¿®æ­£äº† {mapping_result['total_corrections']} ä¸ªç»“æœ")
                logger.info(f"  - ç±»å‹ä¿®æ­£: {len(mapping_result['type_corrections'])} ä¸ª")
                logger.info(f"  - å­£åº¦ä¿®æ­£: {len(mapping_result['season_corrections'])} ä¸ª")

                # æ›´æ–°æœç´¢ç»“æœï¼ˆå·²ç»ç›´æ¥ä¿®æ”¹äº†sorted_resultsï¼‰
                sorted_results = mapping_result['corrected_results']
                timer.step_end(details=f"ä¿®æ­£{mapping_result['total_corrections']}ä¸ª")
            else:
                logger.info(f"â—‹ ä¸»é¡µæœç´¢ ç»Ÿä¸€AIæ˜ å°„: æœªæ‰¾åˆ°éœ€è¦ä¿®æ­£çš„ä¿¡æ¯")
                timer.step_end(details="æ— ä¿®æ­£")

        except Exception as e:
            logger.warning(f"ä¸»é¡µæœç´¢ ç»Ÿä¸€AIæ˜ å°„ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            timer.step_end(details=f"å¤±è´¥: {e}")

    timer.finish()  # æ‰“å°æœç´¢è®¡æ—¶æŠ¥å‘Š
    return UIProviderSearchResponse(
        results=[item.model_dump() for item in sorted_results],
        supplemental_results=[item.model_dump() for item in supplemental_results] if supplemental_results else [],
        search_season=season_to_filter,
        search_episode=episode_to_filter
    )



@router.get("/search/episodes", response_model=List[models.ProviderEpisodeInfo], summary="è·å–æœç´¢ç»“æœçš„åˆ†é›†åˆ—è¡¨")
async def get_episodes_for_search_result(
    provider: str = Query(...),
    media_id: str = Query(...),
    media_type: Optional[str] = Query(None), # Pass media_type to help scraper
    supplement_provider: Optional[str] = Query(None, description="è¡¥å……æºprovider"),
    supplement_media_id: Optional[str] = Query(None, description="è¡¥å……æºmediaId"),
    manager: ScraperManager = Depends(get_scraper_manager),
    metadata_manager: MetadataSourceManager = Depends(get_metadata_manager),
    current_user: models.User = Depends(security.get_current_user)
):
    """ä¸ºæŒ‡å®šçš„æœç´¢ç»“æœè·å–å®Œæ•´çš„åˆ†é›†åˆ—è¡¨ã€‚æ”¯æŒä»è¡¥å……æºè·å–åˆ†é›†URLã€‚"""
    try:
        episodes = []

        # å¦‚æœæä¾›äº†è¡¥å……æºå‚æ•°,ä»è¡¥å……æºè·å–åˆ†é›†URL
        if supplement_provider and supplement_media_id:
            logger.info(f"ä½¿ç”¨è¡¥å……æº {supplement_provider} è·å–åˆ†é›†åˆ—è¡¨")

            try:
                # è·å–è¡¥å……æºå®ä¾‹
                supplement_source = metadata_manager.sources.get(supplement_provider)
                if not supplement_source:
                    logger.warning(f"è¡¥å……æº {supplement_provider} ä¸å¯ç”¨")
                elif not getattr(supplement_source, 'supports_episode_urls', False):
                    logger.warning(f"è¡¥å……æº {supplement_provider} ä¸æ”¯æŒåˆ†é›†URLè·å–")
                else:
                    # ä½¿ç”¨è¡¥å……æºè·å–åˆ†é›†URLåˆ—è¡¨
                    episode_urls = await supplement_source.get_episode_urls(
                        supplement_media_id, provider  # ç›®æ ‡å¹³å°
                    )
                    logger.info(f"è¡¥å……æºè·å–åˆ° {len(episode_urls)} ä¸ªåˆ†é›†URL")

                    if episode_urls:
                        # è·å–ä¸»æºscraperç”¨äºè§£æURL
                        scraper = manager.get_scraper(provider)

                        # è§£æURLè·å–åˆ†é›†ä¿¡æ¯
                        for i, url in episode_urls:
                            try:
                                # ä»URLæå–episode_id
                                episode_id = await scraper.get_id_from_url(url)
                                if episode_id:
                                    episodes.append(models.ProviderEpisodeInfo(
                                        provider=provider,
                                        episodeId=episode_id,
                                        title=f"ç¬¬{i}é›†",
                                        episodeIndex=i,
                                        url=url
                                    ))
                            except Exception as e:
                                logger.warning(f"è§£æURLå¤±è´¥ (ç¬¬{i}é›†): {e}")

                        logger.info(f"è¡¥å……æºæˆåŠŸè§£æ {len(episodes)} ä¸ªåˆ†é›†")
            except Exception as e:
                logger.error(f"ä½¿ç”¨è¡¥å……æºè·å–åˆ†é›†å¤±è´¥: {e}", exc_info=True)
        else:
            # ä»ä¸»æºè·å–åˆ†é›†åˆ—è¡¨
            scraper = manager.get_scraper(provider)
            # å°† db_media_type ä¼ é€’ç»™ get_episodes ä»¥å¸®åŠ©éœ€è¦å®ƒçš„åˆ®å‰Šå™¨ï¼ˆå¦‚ mgtvï¼‰
            episodes = await scraper.get_episodes(media_id, db_media_type=media_type)

        return episodes
    except httpx.RequestError as e:
        # æ–°å¢ï¼šæ•è·ç½‘ç»œé”™è¯¯
        error_message = f"ä» {provider} è·å–åˆ†é›†åˆ—è¡¨æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}"
        logger.error(f"è·å–åˆ†é›†åˆ—è¡¨å¤±è´¥ (provider={provider}, media_id={media_id}): {error_message}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=error_message)
    except ValueError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except Exception as e:
        logger.error(f"è·å–åˆ†é›†åˆ—è¡¨å¤±è´¥ (provider={provider}, media_id={media_id}): {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="è·å–åˆ†é›†åˆ—è¡¨å¤±è´¥ã€‚")




