"""è‡ªåŠ¨æœç´¢å’Œå¯¼å…¥ä»»åŠ¡æ¨¡å—"""
import asyncio
import logging
import traceback
from typing import Callable, Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from thefuzz import fuzz

from src.db import crud, models, ConfigManager
from src.ai import AIMatcherManager
from src.services import ScraperManager, MetadataSourceManager, TaskManager, TaskSuccess, TitleRecognitionManager, unified_search
from src.rate_limiter import RateLimiter
from src.utils import (
    ai_type_and_season_mapping_and_correction,
    SearchTimer, SEARCH_TYPE_CONTROL_AUTO_IMPORT, convert_to_chinese_title
)

logger = logging.getLogger(__name__)


# å»¶è¿Ÿå¯¼å…¥è¾…åŠ©å‡½æ•°
def _get_parse_episode_ranges():
    from .utils import parse_episode_ranges
    return parse_episode_ranges

def _get_is_chinese_title():
    from .utils import is_chinese_title
    return is_chinese_title

def _get_is_tmdb_reverse_lookup_enabled():
    from .metadata import is_tmdb_reverse_lookup_enabled
    return is_tmdb_reverse_lookup_enabled

def _get_reverse_lookup_tmdb_chinese_title():
    from .metadata import reverse_lookup_tmdb_chinese_title
    return reverse_lookup_tmdb_chinese_title

def _get_generic_import_task():
    from .import_core import generic_import_task
    return generic_import_task


async def auto_search_and_import_task(
    payload: "models.ControlAutoImportRequest",
    progress_callback: Callable,
    session: AsyncSession,
    config_manager: ConfigManager,
    scraper_manager: ScraperManager,
    metadata_manager: MetadataSourceManager,
    task_manager: TaskManager,
    ai_matcher_manager: AIMatcherManager,
    rate_limiter: Optional[RateLimiter] = None,
    api_key: Optional[str] = None,
    title_recognition_manager: Optional[TitleRecognitionManager] = None,
):
    """
    å…¨è‡ªåŠ¨æœç´¢å¹¶å¯¼å…¥çš„æ ¸å¿ƒä»»åŠ¡é€»è¾‘ã€‚
    """
    parse_episode_ranges = _get_parse_episode_ranges()
    _is_chinese_title = _get_is_chinese_title()
    _is_tmdb_reverse_lookup_enabled = _get_is_tmdb_reverse_lookup_enabled()
    _reverse_lookup_tmdb_chinese_title = _get_reverse_lookup_tmdb_chinese_title()
    generic_import_task = _get_generic_import_task()
    
    # åˆå§‹åŒ–è®¡æ—¶å™¨å¹¶å¼€å§‹è®¡æ—¶
    timer = SearchTimer(SEARCH_TYPE_CONTROL_AUTO_IMPORT, payload.searchTerm, logger)
    timer.start()

    try:
        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿ rate_limiter å·²è¢«æ­£ç¡®ä¼ é€’ã€‚
        if rate_limiter is None:
            error_msg = "ä»»åŠ¡å¯åŠ¨å¤±è´¥ï¼šå†…éƒ¨é”™è¯¯ï¼ˆé€Ÿç‡é™åˆ¶å™¨æœªæä¾›ï¼‰ã€‚è¯·æ£€æŸ¥ä»»åŠ¡æäº¤å¤„çš„ä»£ç ã€‚"
            logger.error(f"auto_search_and_import_task was called without a rate_limiter. This is a bug. Payload: {payload}")
            raise ValueError(error_msg)

        search_type = payload.searchType
        search_term = payload.searchTerm
        media_type = payload.mediaType
        season = payload.season

        await progress_callback(5, f"å¼€å§‹å¤„ç†ï¼Œç±»å‹: {search_type}, æœç´¢è¯: {search_term}")

        aliases = {search_term}
        main_title = search_term
        image_url = None
        year: Optional[int] = None
        tmdb_id, bangumi_id, douban_id, tvdb_id, imdb_id = None, None, None, None, None

        # ä¸ºåå°ä»»åŠ¡åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç”¨æˆ·å¯¹è±¡
        user = models.User(id=1, username="admin")

        # 1. è·å–å…ƒæ•°æ®å’Œåˆ«å
        details: Optional[models.MetadataDetailsResponse] = None

        # æ™ºèƒ½æ£€æµ‹ï¼šå¦‚æœ searchType æ˜¯ keyword ä½† searchTerm æ˜¯æ•°å­—ï¼Œåˆ™å°è¯•å°†å…¶ä½œä¸º TMDB ID å¤„ç†
        effective_search_type = search_type.value
        if search_type == "keyword" and search_term.isdigit():
            logger.info(f"æ£€æµ‹åˆ°å…³é”®è¯ '{search_term}' ä¸ºæ•°å­—ï¼Œå°†å°è¯•ä½œä¸ºTMDB IDè¿›è¡Œå…ƒæ•°æ®è·å–...")
            effective_search_type = "tmdb"

        if effective_search_type != "keyword":
            provider_media_type = None
            if media_type:
                if effective_search_type == 'tmdb':
                    provider_media_type = 'tv' if media_type == 'tv_series' else 'movie'
                elif effective_search_type == 'tvdb':
                    provider_media_type = 'series' if media_type == 'tv_series' else 'movies'

            try:
                await progress_callback(10, f"æ­£åœ¨ä» {effective_search_type.upper()} è·å–å…ƒæ•°æ®...")

                # --- ä¿®æ­£ï¼šå½“ mediaType æœªæä¾›æ—¶ï¼Œæ™ºèƒ½åœ°å°è¯•ä¸¤ç§ç±»å‹ ---
                provider_media_type_to_try = None
                if media_type:
                    if effective_search_type == 'tmdb':
                        provider_media_type_to_try = 'tv' if media_type == 'tv_series' else 'movie'
                    elif effective_search_type == 'tvdb':
                        provider_media_type_to_try = 'series' if media_type == 'tv_series' else 'movies'

                if provider_media_type_to_try:
                    details = await metadata_manager.get_details(
                        provider=effective_search_type, item_id=search_term, user=user, mediaType=provider_media_type_to_try
                    )
                else:
                    # å¦‚æœæ— æ³•æ¨æ–­ï¼Œåˆ™ä¾æ¬¡å°è¯• TV å’Œ Movie
                    logger.info(f"æœªæä¾› mediaTypeï¼Œå°†ä¾æ¬¡å°è¯• TV å’Œ Movie ç±»å‹...")
                    tv_type = 'tv' if effective_search_type == 'tmdb' else 'series'
                    details = await metadata_manager.get_details(provider=effective_search_type, item_id=search_term, user=user, mediaType=tv_type)
                    if not details:
                        logger.info(f"ä½œä¸º TV/Series æœªæ‰¾åˆ°ï¼Œæ­£åœ¨å°è¯•ä½œä¸º Movie...")
                        movie_type = 'movie' if effective_search_type == 'tmdb' else 'movies'
                        details = await metadata_manager.get_details(provider=effective_search_type, item_id=search_term, user=user, mediaType=movie_type)
                # --- ä¿®æ­£ç»“æŸ ---
                if not details and search_type == "keyword":
                    logger.info(f"ä½œä¸ºTMDB IDè·å–å…ƒæ•°æ®å¤±è´¥ï¼Œå°†æŒ‰åŸæ ·ä½œä¸ºå…³é”®è¯å¤„ç†ã€‚")
            except Exception as e:
                logger.error(f"ä» {effective_search_type.upper()} è·å–å…ƒæ•°æ®å¤±è´¥: {e}\n{traceback.format_exc()}")
                if search_type == "keyword":
                    logger.warning(f"å°è¯•å°†å…³é”®è¯ä½œä¸ºTMDB IDå¤„ç†æ—¶å‡ºé”™ï¼Œå°†æŒ‰åŸæ ·ä½œä¸ºå…³é”®è¯å¤„ç†ã€‚")

        if details:
            main_title = details.title or main_title
            image_url = details.imageUrl
            aliases.add(main_title)
            aliases.update(details.aliasesCn or [])
            aliases.add(details.nameEn)
            aliases.add(details.nameJp)
            tmdb_id, bangumi_id, douban_id, tvdb_id, imdb_id = (
                details.tmdbId, details.bangumiId, details.doubanId,
                details.tvdbId, details.imdbId
            )

            # TMDBåæŸ¥åŠŸèƒ½ï¼šå¦‚æœæ ‡é¢˜ä¸æ˜¯ä¸­æ–‡ä¸”ä¸æ˜¯TMDBæœç´¢ï¼Œå°è¯•é€šè¿‡å…¶ä»–IDåæŸ¥TMDBè·å–ä¸­æ–‡æ ‡é¢˜
            logger.info(f"TMDBåæŸ¥æ£€æŸ¥: effective_search_type='{effective_search_type}', main_title='{main_title}', is_chinese={_is_chinese_title(main_title)}")
            if effective_search_type != 'tmdb' and main_title and not _is_chinese_title(main_title):
                # æ£€æŸ¥TMDBåæŸ¥æ˜¯å¦å¯ç”¨
                tmdb_reverse_enabled = await _is_tmdb_reverse_lookup_enabled(session, effective_search_type)
                logger.info(f"TMDBåæŸ¥é…ç½®æ£€æŸ¥: enabled={tmdb_reverse_enabled}, source_type='{effective_search_type}'")
                if tmdb_reverse_enabled:
                    logger.info(f"æ£€æµ‹åˆ°éä¸­æ–‡æ ‡é¢˜ '{main_title}'ï¼Œå°è¯•é€šè¿‡å…¶ä»–IDåæŸ¥TMDBè·å–ä¸­æ–‡æ ‡é¢˜...")
                    # å¦‚æœæ˜¯é€šè¿‡å¤–éƒ¨IDæœç´¢ï¼Œç›´æ¥ä½¿ç”¨æœç´¢çš„ID
                    lookup_tmdb_id = tmdb_id
                    lookup_imdb_id = imdb_id if effective_search_type != 'imdb' else search_term
                    lookup_tvdb_id = tvdb_id if effective_search_type != 'tvdb' else search_term
                    lookup_douban_id = douban_id if effective_search_type != 'douban' else search_term
                    lookup_bangumi_id = bangumi_id if effective_search_type != 'bangumi' else search_term

                    chinese_title = await _reverse_lookup_tmdb_chinese_title(
                        metadata_manager, user, effective_search_type, search_term,
                        lookup_tmdb_id, lookup_imdb_id, lookup_tvdb_id, lookup_douban_id, lookup_bangumi_id
                    )
                    if chinese_title:
                        logger.info(f"TMDBåæŸ¥æˆåŠŸï¼Œä½¿ç”¨ä¸­æ–‡æ ‡é¢˜: '{chinese_title}' (åŸæ ‡é¢˜: '{main_title}')")
                        main_title = chinese_title
                        aliases.add(chinese_title)
                    else:
                        logger.info(f"TMDBåæŸ¥æœªæ‰¾åˆ°ä¸­æ–‡æ ‡é¢˜ï¼Œç»§ç»­ä½¿ç”¨åŸæ ‡é¢˜: '{main_title}'")
                else:
                    logger.info(f"TMDBåæŸ¥åŠŸèƒ½æœªå¯ç”¨æˆ–ä¸æ”¯æŒæº '{effective_search_type}'ï¼Œç»§ç»­ä½¿ç”¨åŸæ ‡é¢˜: '{main_title}'")
            if hasattr(details, 'type') and details.type:
                media_type = models.AutoImportMediaType(details.type)
            if hasattr(details, 'year') and details.year:
                year = details.year

            logger.info(f"æ­£åœ¨ä¸º '{main_title}' ä»å…¶ä»–æºè·å–æ›´å¤šåˆ«å...")
            enriched_aliases = await metadata_manager.search_aliases_from_enabled_sources(main_title, user)
            if enriched_aliases:
                aliases.update(enriched_aliases)
                logger.info(f"åˆ«åå·²æ‰©å……: {aliases}")

        # 2. æ£€æŸ¥åª’ä½“åº“ä¸­æ˜¯å¦å·²å­˜åœ¨
        existing_anime: Optional[Dict[str, Any]] = None
        await progress_callback(20, "æ­£åœ¨æ£€æŸ¥åª’ä½“åº“...")

        # æ­¥éª¤ 2a: ä¼˜å…ˆé€šè¿‡å…ƒæ•°æ®IDå’Œå­£åº¦å·è¿›è¡Œç²¾ç¡®æŸ¥æ‰¾
        if search_type != "keyword" and season is not None:
            id_column_map = {
                "tmdb": "tmdbId", "tvdb": "tvdbId", "imdb": "imdbId",
                "douban": "doubanId", "bangumi": "bangumiId"
            }
            id_type = id_column_map.get(search_type.value)
            if id_type:
                logger.info(f"æ­£åœ¨é€šè¿‡ {search_type.upper()} ID '{search_term}' å’Œå­£åº¦ {season} ç²¾ç¡®æŸ¥æ‰¾...")
                existing_anime = await crud.find_anime_by_metadata_id_and_season(
                    session, id_type, search_term, season
                )
                if existing_anime:
                    logger.info(f"ç²¾ç¡®æŸ¥æ‰¾åˆ°å·²å­˜åœ¨çš„ä½œå“: {existing_anime['title']} (ID: {existing_anime['id']})")

        # å…³é”®ä¿®å¤ï¼šå¦‚æœåª’ä½“ç±»å‹æ˜¯ç”µå½±ï¼Œåˆ™å¼ºåˆ¶ä½¿ç”¨å­£åº¦1è¿›è¡ŒæŸ¥æ‰¾ï¼Œ
        # ä»¥åŒ¹é…UIå¯¼å…¥æ—¶ä¸ºç”µå½±è®¾ç½®çš„é»˜è®¤å­£åº¦ï¼Œä»è€Œé˜²æ­¢é‡å¤å¯¼å…¥ã€‚
        season_for_check = season
        if media_type == 'movie' and season_for_check is None:
            season_for_check = 1
            logger.info(f"æ£€æµ‹åˆ°åª’ä½“ç±»å‹ä¸ºç”µå½±ï¼Œå°†ä½¿ç”¨é»˜è®¤å­£åº¦ {season_for_check} è¿›è¡Œé‡å¤æ£€æŸ¥ã€‚")

        # æ­¥éª¤ 2b: å¦‚æœç²¾ç¡®æŸ¥æ‰¾æœªæ‰¾åˆ°ï¼Œåˆ™å›é€€åˆ°æŒ‰æ ‡é¢˜å’Œå­£åº¦æŸ¥æ‰¾
        if not existing_anime:
            if search_type != "keyword":
                logger.info("é€šè¿‡å…ƒæ•°æ®ID+å­£åº¦æœªæ‰¾åˆ°åŒ¹é…é¡¹ï¼Œå›é€€åˆ°æŒ‰æ ‡é¢˜æŸ¥æ‰¾...")

            # å¦‚æœé€šè¿‡IDæœªæ‰¾åˆ°ï¼Œæˆ–ä¸æ˜¯æŒ‰IDæœç´¢ï¼Œåˆ™å›é€€åˆ°æŒ‰æ ‡é¢˜å’Œå­£åº¦æŸ¥æ‰¾
            existing_anime = await crud.find_anime_by_title_season_year(
                session, main_title, season_for_check, year, title_recognition_manager, None  # sourceå‚æ•°æš‚æ—¶ä¸ºNoneï¼Œå› ä¸ºè¿™é‡Œæ˜¯æŸ¥æ‰¾ç°æœ‰æ¡ç›®
            )

        # å…³é”®ä¿®å¤ï¼šå¯¹äºå•é›†/å¤šé›†å¯¼å…¥ï¼Œéœ€è¦ä½¿ç”¨ç»è¿‡è¯†åˆ«è¯å¤„ç†åçš„é›†æ•°è¿›è¡Œæ£€æŸ¥
        if payload.episode is not None and existing_anime:
            # è§£æé›†æ•°å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨ (æ”¯æŒ "1,3,5,7,9,11-13" æ ¼å¼)
            requested_episodes = parse_episode_ranges(payload.episode)
            logger.info(f"æ£€æŸ¥åº“å†…æ˜¯å¦å­˜åœ¨è¯·æ±‚çš„é›†æ•°: {requested_episodes}")

            anime_id_to_use = existing_anime.get('id') or existing_anime.get('animeId')
            if anime_id_to_use:
                # æ£€æŸ¥æ‰€æœ‰è¯·æ±‚çš„é›†æ•°æ˜¯å¦éƒ½å·²å­˜åœ¨
                all_exist = True
                missing_episodes = []
                for ep in requested_episodes:
                    # åº”ç”¨è¯†åˆ«è¯è½¬æ¢è·å–å®é™…çš„é›†æ•°
                    episode_to_check = ep
                    if title_recognition_manager:
                        _, converted_episode, _, _, _ = await title_recognition_manager.apply_title_recognition(main_title, ep, season_for_check)
                        if converted_episode is not None:
                            episode_to_check = converted_episode
                            logger.info(f"è¯†åˆ«è¯è½¬æ¢: åŸå§‹é›†æ•° {ep} -> è½¬æ¢åé›†æ•° {episode_to_check}")

                    episode_exists = await crud.find_episode_by_index(session, anime_id_to_use, episode_to_check)
                    if not episode_exists:
                        all_exist = False
                        missing_episodes.append(ep)

                if all_exist:
                    final_message = f"ä½œå“ '{main_title}' çš„æ‰€æœ‰è¯·æ±‚é›†æ•° {requested_episodes} å·²åœ¨åª’ä½“åº“ä¸­ï¼Œæ— éœ€é‡å¤å¯¼å…¥ã€‚"
                    logger.info(f"è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡æ£€æµ‹åˆ°æ‰€æœ‰åˆ†é›†å·²å­˜åœ¨ï¼Œä»»åŠ¡æˆåŠŸç»“æŸ: {final_message}")
                    raise TaskSuccess(final_message)
                else:
                    logger.info(f"ä½œå“ '{main_title}' å·²å­˜åœ¨ï¼Œä½†éƒ¨åˆ†é›†æ•°ä¸å­˜åœ¨: {missing_episodes}ã€‚å°†ç»§ç»­æ‰§è¡Œå¯¼å…¥æµç¨‹ã€‚")
            # å¦‚æœåˆ†é›†ä¸å­˜åœ¨ï¼Œå³ä½¿ä½œå“å­˜åœ¨ï¼Œæˆ‘ä»¬ä¹Ÿè¦ç»§ç»­æ‰§è¡Œåç»­çš„æœç´¢å’Œå¯¼å…¥é€»è¾‘ã€‚
        # å…³é”®ä¿®å¤ï¼šä»…å½“è¿™æ˜¯ä¸€ä¸ªæ•´å­£å¯¼å…¥è¯·æ±‚æ—¶ï¼Œæ‰åœ¨æ‰¾åˆ°ä½œå“åç«‹å³åœæ­¢ã€‚
        # å¯¹äºå•é›†å¯¼å…¥ï¼Œå³ä½¿ä½œå“å­˜åœ¨ï¼Œä¹Ÿéœ€è¦ç»§ç»­æ‰§è¡Œä»¥æ£€æŸ¥å’Œå¯¼å…¥ç¼ºå¤±çš„å•é›†ã€‚
        if payload.episode is None and existing_anime:
            final_message = f"ä½œå“ '{main_title}' å·²åœ¨åª’ä½“åº“ä¸­ï¼Œæ— éœ€é‡å¤å¯¼å…¥æ•´å­£ã€‚"
            logger.info(f"è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡æ£€æµ‹åˆ°ä½œå“å·²å­˜åœ¨ï¼ˆæ•´å­£å¯¼å…¥ï¼‰ï¼Œä»»åŠ¡æˆåŠŸç»“æŸ: {final_message}")
            raise TaskSuccess(final_message)


        if existing_anime:
            # ä¿®æ­£ï¼šä» existing_anime å­—å…¸ä¸­å®‰å…¨åœ°è·å–IDã€‚
            # ä¸åŒçš„æŸ¥è¯¢è·¯å¾„å¯èƒ½è¿”å› 'id' æˆ– 'animeId' ä½œä¸ºé”®ã€‚
            # æ­¤æ›´æ”¹ç¡®ä¿æ— è®ºå“ªä¸ªé”®å­˜åœ¨ï¼Œæˆ‘ä»¬éƒ½èƒ½æ­£ç¡®è·å–IDã€‚
            anime_id_to_use = existing_anime.get('id') or existing_anime.get('animeId')
            if not anime_id_to_use:
                raise ValueError("åœ¨å·²å­˜åœ¨çš„ä½œå“è®°å½•ä¸­æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„IDã€‚")

            favorited_source = await crud.find_favorited_source_for_anime(session, anime_id_to_use)
            if favorited_source:
                source_to_use = favorited_source
                logger.info(f"åª’ä½“åº“ä¸­å·²å­˜åœ¨ä½œå“ï¼Œå¹¶æ‰¾åˆ°ç²¾ç¡®æ ‡è®°æº: {source_to_use['providerName']}")
            else:
                all_sources = await crud.get_anime_sources(session, anime_id_to_use)
                if all_sources:
                    ordered_settings = await crud.get_all_scraper_settings(session)
                    provider_order = {s['providerName']: s['displayOrder'] for s in ordered_settings}
                    all_sources.sort(key=lambda s: provider_order.get(s['providerName'], 999))
                    source_to_use = all_sources[0]
                    logger.info(f"åª’ä½“åº“ä¸­å·²å­˜åœ¨ä½œå“ï¼Œé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„æº: {source_to_use['providerName']}")
                else: source_to_use = None

            if source_to_use:
                # å…³é”®ä¿®å¤ï¼šå¦‚æœè¿™æ˜¯ä¸€ä¸ªå•é›†/å¤šé›†å¯¼å…¥ï¼Œå¹¶ä¸”æˆ‘ä»¬å·²ç»ç¡®è®¤äº†éƒ¨åˆ†åˆ†é›†ä¸å­˜åœ¨ï¼Œ
                # é‚£ä¹ˆæˆ‘ä»¬åº”è¯¥ä½¿ç”¨åº“å†…å·²æœ‰çš„æºç»§ç»­æ‰§è¡Œå¯¼å…¥ï¼Œè€Œä¸æ˜¯åœ¨è¿™é‡Œåœæ­¢ã€‚
                # åªæœ‰åœ¨æ•´å­£å¯¼å…¥æ—¶ï¼Œæˆ‘ä»¬æ‰åœ¨è¿™é‡Œåœæ­¢ã€‚
                if payload.episode is None:
                    final_message = f"ä½œå“ '{main_title}' å·²åœ¨åª’ä½“åº“ä¸­ï¼Œæ— éœ€é‡å¤å¯¼å…¥ã€‚"
                    logger.info(f"è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡æ£€æµ‹åˆ°ä½œå“å·²å­˜åœ¨ï¼ˆæ•´å­£å¯¼å…¥ï¼‰ï¼Œä»»åŠ¡æˆåŠŸç»“æŸ: {final_message}")
                    raise TaskSuccess(final_message)
                else:
                    # å¯¹äºå•é›†/å¤šé›†å¯¼å…¥ï¼Œä½¿ç”¨åº“å†…å·²æœ‰çš„æºåˆ›å»ºå¯¼å…¥ä»»åŠ¡
                    logger.info(f"ä½œå“ '{main_title}' å·²å­˜åœ¨ï¼Œä½¿ç”¨åº“å†…æº {source_to_use['providerName']} å¯¼å…¥ç¼ºå¤±çš„é›†æ•°ã€‚")

                    # è§£æå¤šé›†å‚æ•°
                    selected_episodes = parse_episode_ranges(payload.episode)
                    logger.info(f"è§£æé›†æ•°å‚æ•° '{payload.episode}' -> {selected_episodes}")

                    # è·å–å…ƒæ•°æ®ID
                    douban_id = existing_anime.get('doubanId')
                    tmdb_id = existing_anime.get('tmdbId')
                    imdb_id = existing_anime.get('imdbId')
                    tvdb_id = existing_anime.get('tvdbId')
                    bangumi_id = existing_anime.get('bangumiId')
                    image_url = existing_anime.get('imageUrl')

                    task_coro = lambda s, cb: generic_import_task(
                        provider=source_to_use['providerName'], mediaId=source_to_use['mediaId'],
                        animeTitle=existing_anime['title'], mediaType=existing_anime.get('type', 'tv_series'),
                        season=season_for_check, year=existing_anime.get('year'),
                        config_manager=config_manager, metadata_manager=metadata_manager,
                        currentEpisodeIndex=None, imageUrl=image_url,
                        doubanId=douban_id, tmdbId=tmdb_id, imdbId=imdb_id, tvdbId=tvdb_id, bangumiId=bangumi_id,
                        progress_callback=cb, session=s, manager=scraper_manager, task_manager=task_manager,
                        rate_limiter=rate_limiter,
                        title_recognition_manager=title_recognition_manager,
                        is_fallback=False,
                        preassignedAnimeId=anime_id_to_use,
                        selectedEpisodes=selected_episodes
                    )

                    # æ„å»ºä»»åŠ¡æ ‡é¢˜
                    title_parts = [f"è‡ªåŠ¨å¯¼å…¥ (åº“å†…): {existing_anime['title']}"]
                    if season_for_check is not None:
                        title_parts.append(f"S{season_for_check:02d}")
                    title_parts.append(f"E{payload.episode}")
                    task_title = " ".join(title_parts)

                    # æ„å»ºunique_key
                    unique_key_parts = ["import", source_to_use['providerName'], source_to_use['mediaId']]
                    if season_for_check is not None:
                        unique_key_parts.append(f"s{season_for_check}")
                    unique_key_parts.append(f"e{payload.episode}")
                    unique_key = "-".join(unique_key_parts)

                    # å‡†å¤‡ä»»åŠ¡å‚æ•°
                    task_parameters = {
                        "provider": source_to_use['providerName'],
                        "mediaId": source_to_use['mediaId'],
                        "animeTitle": existing_anime['title'],
                        "mediaType": existing_anime.get('type', 'tv_series'),
                        "season": season_for_check,
                        "year": existing_anime.get('year'),
                        "currentEpisodeIndex": None,
                        "selectedEpisodes": selected_episodes,
                        "imageUrl": image_url,
                        "doubanId": douban_id,
                        "tmdbId": tmdb_id,
                        "imdbId": imdb_id,
                        "tvdbId": tvdb_id,
                        "bangumiId": bangumi_id
                    }

                    execution_task_id, _ = await task_manager.submit_task(
                        task_coro,
                        task_title,
                        unique_key=unique_key,
                        task_type="generic_import",
                        task_parameters=task_parameters
                    )
                    final_message = f"å·²ä½¿ç”¨åº“å†…æºåˆ›å»ºå¯¼å…¥ä»»åŠ¡ã€‚æ‰§è¡Œä»»åŠ¡ID: {execution_task_id}"
                    raise TaskSuccess(final_message)

        # 3. å¦‚æœåº“ä¸­ä¸å­˜åœ¨ï¼Œåˆ™è¿›è¡Œå…¨ç½‘æœç´¢
        await progress_callback(40, "åª’ä½“åº“æœªæ‰¾åˆ°ï¼Œå¼€å§‹å…¨ç½‘æœç´¢...")
        # æ³¨æ„ï¼šæœç´¢é˜¶æ®µä¸ä¼ é€’episodeä¿¡æ¯ï¼Œå› ä¸ºscraperçš„searchæ–¹æ³•ä¸éœ€è¦å…·ä½“é›†æ•°
        # é›†æ•°ä¿¡æ¯åªåœ¨å¯¼å…¥é˜¶æ®µä½¿ç”¨
        episode_info = {"season": season}

        # ä½¿ç”¨WebUIç›¸åŒçš„æœç´¢é€»è¾‘ï¼šå…ˆè·å–å…ƒæ•°æ®æºåˆ«åï¼Œå†è¿›è¡Œå…¨ç½‘æœç´¢
        await progress_callback(30, "æ­£åœ¨è·å–å…ƒæ•°æ®æºåˆ«å...")

        # ä½¿ç”¨å…ƒæ•°æ®æºè·å–åˆ«åï¼ˆä¸WebUIç›¸åŒçš„é€»è¾‘ï¼‰
        if metadata_manager:
            try:
                # ä»æ•°æ®åº“è·å–adminç”¨æˆ·ï¼ˆä½¿ç”¨ä¼ å…¥çš„sessionï¼‰
                admin_user = await crud.get_user_by_username(session, "admin")
                if admin_user:
                    user_model = models.User.model_validate(admin_user)

                    logger.info("ä¸€ä¸ªæˆ–å¤šä¸ªå…ƒæ•°æ®æºå·²å¯ç”¨è¾…åŠ©æœç´¢ï¼Œå¼€å§‹æ‰§è¡Œ...")

                    # è°ƒç”¨æ­£ç¡®çš„æ–¹æ³•
                    supplemental_aliases, _ = await metadata_manager.search_supplemental_sources(main_title, user_model)
                    aliases.update(supplemental_aliases)

                    logger.info(f"æ‰€æœ‰è¾…åŠ©æœç´¢å®Œæˆï¼Œæœ€ç»ˆåˆ«åé›†å¤§å°: {len(aliases)}")
                    logger.info(f"ç”¨äºè¿‡æ»¤çš„åˆ«ååˆ—è¡¨: {list(aliases)}")
                else:
                    logger.warning("æœªæ‰¾åˆ°adminç”¨æˆ·ï¼Œè·³è¿‡å…ƒæ•°æ®æºè¾…åŠ©æœç´¢")
            except Exception as e:
                logger.warning(f"å…ƒæ•°æ®æºè¾…åŠ©æœç´¢å¤±è´¥: {e}")

        # ğŸš€ åç§°è½¬æ¢åŠŸèƒ½ - æ£€æµ‹éä¸­æ–‡æ ‡é¢˜å¹¶å°è¯•è½¬æ¢ä¸ºä¸­æ–‡ï¼ˆåœ¨é¢„å¤„ç†è§„åˆ™ä¹‹å‰æ‰§è¡Œï¼‰
        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç”¨æˆ·ç”¨äºå…ƒæ•°æ®è°ƒç”¨
        auto_import_user = models.User(id=0, username="auto_import")
        converted_title, conversion_applied = await convert_to_chinese_title(
            main_title,
            config_manager,
            metadata_manager,
            ai_matcher_manager,
            auto_import_user
        )
        if conversion_applied:
            logger.info(f"âœ“ å…¨è‡ªåŠ¨å¯¼å…¥ åç§°è½¬æ¢: '{main_title}' â†’ '{converted_title}'")
            main_title = converted_title  # æ›´æ–° main_title ç”¨äºåç»­å¤„ç†

        # åº”ç”¨æœç´¢é¢„å¤„ç†è§„åˆ™
        search_title = main_title
        search_season = season
        if title_recognition_manager:
            processed_title, processed_episode, processed_season, preprocessing_applied = await title_recognition_manager.apply_search_preprocessing(main_title, payload.episode, season)
            if preprocessing_applied:
                search_title = processed_title
                logger.info(f"âœ“ åº”ç”¨æœç´¢é¢„å¤„ç†: '{main_title}' -> '{search_title}'")
                # å¦‚æœé›†æ•°å‘ç”Ÿäº†å˜åŒ–ï¼Œæ›´æ–°episode_info
                if processed_episode != payload.episode:
                    logger.info(f"âœ“ é›†æ•°é¢„å¤„ç†: {payload.episode} -> {processed_episode}")
                    # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦æ›´æ–°episode_info
                # å¦‚æœå­£æ•°å‘ç”Ÿäº†å˜åŒ–ï¼Œæ›´æ–°æœç´¢å­£æ•°
                if processed_season != season:
                    search_season = processed_season
                    logger.info(f"âœ“ å­£åº¦é¢„å¤„ç†: {season} -> {search_season}")
                    # æ›´æ–°episode_infoä¸­çš„å­£æ•°ï¼ˆæœç´¢é˜¶æ®µä¸ä¼ é€’episodeï¼‰
                    episode_info = {"season": search_season}
            else:
                logger.info(f"â—‹ æœç´¢é¢„å¤„ç†æœªç”Ÿæ•ˆ: '{main_title}'")

        # åˆ›å»ºAIç±»å‹å’Œå­£åº¦æ˜ å°„ä»»åŠ¡(å¦‚æœå¯ç”¨) - ä¸æœç´¢å¹¶è¡Œè¿è¡Œ
        mapping_task = None
        auto_import_tmdb_enabled = await config_manager.get("autoImportEnableTmdbSeasonMapping", "false")
        if auto_import_tmdb_enabled.lower() == "true" and media_type != "movie":
            logger.info(f"â—‹ å…¨è‡ªåŠ¨å¯¼å…¥ AIæ˜ å°„: å¼€å§‹ä¸º '{search_title}' è¿›è¡Œç±»å‹å’Œå­£åº¦æ˜ å°„(å¹¶è¡Œ)...")

            # è·å–AIåŒ¹é…å™¨(å¦‚æœå¯ç”¨)
            ai_matcher = await ai_matcher_manager.get_matcher()
            if ai_matcher:
                logger.debug("å…¨è‡ªåŠ¨å¯¼å…¥ AIæ˜ å°„: ä½¿ç”¨AIåŒ¹é…å™¨")
            else:
                logger.debug("å…¨è‡ªåŠ¨å¯¼å…¥ AIæ˜ å°„: AIåŒ¹é…å™¨æœªå¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥")

            # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
            async def get_ai_mapping():
                try:
                    # å…ˆæ‰§è¡Œæœç´¢è·å–ç»“æœï¼Œç„¶åè¿›è¡ŒAIæ˜ å°„
                    # è¿™é‡Œå…ˆè¿”å›Noneï¼Œå®é™…æ˜ å°„åœ¨æœç´¢åè¿›è¡Œ
                    return None
                except Exception as e:
                    logger.warning(f"å…¨è‡ªåŠ¨å¯¼å…¥ AIæ˜ å°„å¤±è´¥: {e}")
                    return None

            mapping_task = asyncio.create_task(get_ai_mapping())
        else:
            if auto_import_tmdb_enabled.lower() != "true":
                logger.info("â—‹ å…¨è‡ªåŠ¨å¯¼å…¥ AIæ˜ å°„: åŠŸèƒ½æœªå¯ç”¨")
            elif media_type == "movie":
                logger.info("â—‹ å…¨è‡ªåŠ¨å¯¼å…¥ AIæ˜ å°„: ç”µå½±ç±»å‹,è·³è¿‡")

        logger.info(f"å°†ä½¿ç”¨æ ‡é¢˜ '{search_title}' è¿›è¡Œå…¨ç½‘æœç´¢...")

        timer.step_start("å¼¹å¹•æºæœç´¢")
        # ä½¿ç”¨ç»Ÿä¸€çš„æœç´¢å‡½æ•°ï¼ˆä¸ WebUI æœç´¢ä¿æŒä¸€è‡´ï¼‰
        # ä½¿ç”¨ä¸¥æ ¼è¿‡æ»¤æ¨¡å¼å’Œè‡ªå®šä¹‰åˆ«å
        # å¤–éƒ¨æ§åˆ¶APIå¯ç”¨AIåˆ«åæ‰©å±•ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
        all_results = await unified_search(
            search_term=search_title,
            session=session,
            scraper_manager=scraper_manager,
            metadata_manager=metadata_manager,  # ä¼ å…¥metadata_managerä»¥æ”¯æŒAIåˆ«åæ‰©å±•
            use_alias_expansion=True,  # å¯ç”¨AIåˆ«åæ‰©å±•ï¼ˆå¤–éƒ¨æ§åˆ¶APIä¸“ç”¨ï¼‰
            use_alias_filtering=False,
            use_title_filtering=True,  # å¯ç”¨æ ‡é¢˜è¿‡æ»¤
            use_source_priority_sorting=False,  # ä¸æ’åºï¼Œåé¢è‡ªå·±å¤„ç†
            strict_filtering=True,  # ä½¿ç”¨ä¸¥æ ¼è¿‡æ»¤æ¨¡å¼
            custom_aliases=aliases,  # ä¼ å…¥æ‰‹åŠ¨è·å–çš„åˆ«å
            progress_callback=None,
            episode_info=episode_info,  # ä¼ é€’åˆ†é›†ä¿¡æ¯ï¼ˆä¸ WebUI ä¸€è‡´ï¼‰
            alias_similarity_threshold=70,  # ä½¿ç”¨ 70% åˆ«åç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä¸ WebUI ä¸€è‡´ï¼‰
        )
        # æ”¶é›†å•æºæœç´¢è€—æ—¶ä¿¡æ¯
        from ..search_timer import SubStepTiming
        source_timing_sub_steps = [
            SubStepTiming(name=name, duration_ms=dur, result_count=cnt)
            for name, dur, cnt in scraper_manager.last_search_timing
        ]
        timer.step_end(details=f"{len(all_results)}ä¸ªç»“æœ", sub_steps=source_timing_sub_steps)

        logger.info(f"æœç´¢å®Œæˆï¼Œå…± {len(all_results)} ä¸ªç»“æœ")

        # ä½¿ç”¨ç»Ÿä¸€çš„AIç±»å‹å’Œå­£åº¦æ˜ å°„ä¿®æ­£å‡½æ•°
        if auto_import_tmdb_enabled.lower() == "true" and media_type != "movie":
            try:
                timer.step_start("AIæ˜ å°„ä¿®æ­£")
                # è·å–AIåŒ¹é…å™¨
                ai_matcher = await ai_matcher_manager.get_matcher()
                if ai_matcher:
                    logger.info(f"â—‹ å…¨è‡ªåŠ¨å¯¼å…¥ å¼€å§‹ç»Ÿä¸€AIæ˜ å°„ä¿®æ­£: '{search_title}' ({len(all_results)} ä¸ªç»“æœ)")

                    # ä½¿ç”¨æ–°çš„ç»Ÿä¸€å‡½æ•°è¿›è¡Œç±»å‹å’Œå­£åº¦ä¿®æ­£
                    mapping_result = await ai_type_and_season_mapping_and_correction(
                        search_title=search_title,
                        search_results=all_results,
                        metadata_manager=metadata_manager,
                        ai_matcher=ai_matcher,
                        logger=logger,
                        similarity_threshold=60.0
                    )

                    # åº”ç”¨ä¿®æ­£ç»“æœ
                    if mapping_result['total_corrections'] > 0:
                        logger.info(f"âœ“ å…¨è‡ªåŠ¨å¯¼å…¥ ç»Ÿä¸€AIæ˜ å°„æˆåŠŸ: æ€»è®¡ä¿®æ­£äº† {mapping_result['total_corrections']} ä¸ªç»“æœ")
                        logger.info(f"  - ç±»å‹ä¿®æ­£: {len(mapping_result['type_corrections'])} ä¸ª")
                        logger.info(f"  - å­£åº¦ä¿®æ­£: {len(mapping_result['season_corrections'])} ä¸ª")

                        # æ›´æ–°æœç´¢ç»“æœï¼ˆå·²ç»ç›´æ¥ä¿®æ”¹äº†all_resultsï¼‰
                        all_results = mapping_result['corrected_results']
                        timer.step_end(details=f"ä¿®æ­£{mapping_result['total_corrections']}ä¸ª")
                    else:
                        logger.info(f"â—‹ å…¨è‡ªåŠ¨å¯¼å…¥ ç»Ÿä¸€AIæ˜ å°„: æœªæ‰¾åˆ°éœ€è¦ä¿®æ­£çš„ä¿¡æ¯")
                        timer.step_end(details="æ— ä¿®æ­£")
                else:
                    logger.warning("â—‹ å…¨è‡ªåŠ¨å¯¼å…¥ AIæ˜ å°„: AIåŒ¹é…å™¨æœªå¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥")
                    timer.step_end(details="åŒ¹é…å™¨æœªå¯ç”¨")

            except Exception as e:
                logger.warning(f"å…¨è‡ªåŠ¨å¯¼å…¥ ç»Ÿä¸€AIæ˜ å°„ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                timer.step_end(details=f"å¤±è´¥: {e}")
        else:
            if auto_import_tmdb_enabled.lower() != "true":
                logger.info("â—‹ å…¨è‡ªåŠ¨å¯¼å…¥ ç»Ÿä¸€AIæ˜ å°„: åŠŸèƒ½æœªå¯ç”¨")
            elif media_type == "movie":
                logger.info("â—‹ å…¨è‡ªåŠ¨å¯¼å…¥ ç»Ÿä¸€AIæ˜ å°„: ç”µå½±ç±»å‹,è·³è¿‡")

        # æ ¹æ®æ ‡é¢˜å…³é”®è¯ä¿®æ­£åª’ä½“ç±»å‹ï¼ˆä¸ WebUI ä¸€è‡´ï¼‰
        def is_movie_by_title(title: str) -> bool:
            if not title:
                return False
            # å…³é”®è¯åˆ—è¡¨ï¼Œä¸åŒºåˆ†å¤§å°å†™
            movie_keywords = ["å‰§åœºç‰ˆ", "åŠ‡å ´ç‰ˆ", "movie", "æ˜ ç”»"]
            title_lower = title.lower()
            return any(keyword in title_lower for keyword in movie_keywords)

        for item in all_results:
            if item.type == "tv_series" and is_movie_by_title(item.title):
                logger.info(
                    f"Control API: æ ‡é¢˜ '{item.title}' åŒ…å«ç”µå½±å…³é”®è¯ï¼Œç±»å‹ä» 'tv_series' ä¿®æ­£ä¸º 'movie'ã€‚"
                )
                item.type = "movie"

        # æ·»åŠ WebUIçš„å­£åº¦è¿‡æ»¤é€»è¾‘
        if season and season > 0:
            original_count = len(all_results)
            # å½“æŒ‡å®šå­£åº¦æ—¶ï¼Œæˆ‘ä»¬åªå…³å¿ƒç”µè§†å‰§ç±»å‹
            filtered_by_type = [item for item in all_results if item.type == 'tv_series']

            # ç„¶ååœ¨ç”µè§†å‰§ç±»å‹ä¸­ï¼Œæˆ‘ä»¬æŒ‰å­£åº¦å·è¿‡æ»¤
            filtered_by_season = []
            filtered_out = []
            for item in filtered_by_type:
                # ä½¿ç”¨æ¨¡å‹ä¸­å·²è§£æå¥½çš„ season å­—æ®µè¿›è¡Œæ¯”è¾ƒ
                if item.season == season:
                    filtered_by_season.append(item)
                else:
                    filtered_out.append(item)

            logger.info(f"æ ¹æ®æŒ‡å®šçš„å­£åº¦ ({season}) è¿›è¡Œè¿‡æ»¤ï¼Œä» {original_count} ä¸ªç»“æœä¸­ä¿ç•™äº† {len(filtered_by_season)} ä¸ªã€‚")

            # æ‰“å°è¿‡æ»¤è¯¦æƒ…
            if filtered_out:
                logger.info("å­£åº¦è¿‡æ»¤ç»“æœ:")
                for item in filtered_out:
                    logger.info(f"  - å·²è¿‡æ»¤: {item.title} (Provider: {item.provider}, Season: {item.season})")
            if filtered_by_season:
                for item in filtered_by_season:
                    logger.info(f"  - {item.title} (Provider: {item.provider}, Season: {item.season})")

            all_results = filtered_by_season

        if all_results:
            logger.info("ä¿ç•™çš„ç»“æœåˆ—è¡¨:")
            for i, item in enumerate(all_results, 1):  # æ˜¾ç¤ºæ‰€æœ‰ç»“æœ
                logger.info(f"  - {item.title} (Provider: {item.provider}, Type: {item.type}, Season: {item.season})")
            logger.info(f"æ€»å…± {len(all_results)} ä¸ªç»“æœ")

        if not all_results:
            raise ValueError("å…¨ç½‘æœç´¢æœªæ‰¾åˆ°ä»»ä½•ç»“æœã€‚")

        # ç§»é™¤æå‰æ˜ å°„é€»è¾‘ï¼Œæ”¹ä¸ºåœ¨é€‰æ‹©æœ€ä½³åŒ¹é…ååº”ç”¨è¯†åˆ«è¯è½¬æ¢
        await progress_callback(50, "æ­£åœ¨å‡†å¤‡é€‰æ‹©æœ€ä½³æº...")

        # 4. é€‰æ‹©æœ€ä½³æº
        ordered_settings = await crud.get_all_scraper_settings(session)
        provider_order = {s['providerName']: s['displayOrder'] for s in ordered_settings}

        # ä¿®æ­£ï¼šä½¿ç”¨æ›´æ™ºèƒ½çš„æ’åºé€»è¾‘æ¥é€‰æ‹©æœ€ä½³åŒ¹é…
        # 1. åª’ä½“ç±»å‹æ˜¯å¦åŒ¹é… (æœ€ä¼˜å…ˆ)
        # 2. å¦‚æœè¯·æ±‚æŒ‡å®šäº†å­£åº¦ï¼Œå­£åº¦æ˜¯å¦åŒ¹é… (æ¬¡ä¼˜å…ˆ)
        # 3. æ ‡é¢˜ç›¸ä¼¼åº¦
        # 4. æ–°å¢ï¼šå¯¹å®Œå…¨åŒ¹é…æˆ–éå¸¸æ¥è¿‘çš„æ ‡é¢˜ç»™äºˆå·¨å¤§å¥–åŠ±
        # 5. æ ‡é¢˜é•¿åº¦æƒ©ç½š (æ ‡é¢˜è¶Šé•¿ï¼Œè¶Šå¯èƒ½æ˜¯ç‰¹åˆ«ç¯‡ï¼Œå¾—åˆ†è¶Šä½)
        # 6. ç”¨æˆ·è®¾ç½®çš„æºä¼˜å…ˆçº§ (æœ€å)
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        logger.info(f"æ’åºå‰çš„åª’ä½“ç±»å‹: media_type='{media_type}', å‰5ä¸ªç»“æœ:")
        for i, item in enumerate(all_results[:5]):
            logger.info(f"  {i+1}. '{item.title}' (Provider: {item.provider}, Type: {item.type})")

        # ç®€åŒ–æ’åºé€»è¾‘ï¼šç”±äºå·²ç»æœ‰å­£åº¦è¿‡æ»¤å’Œæ ‡é¢˜æ˜ å°„ï¼Œä¸»è¦æŒ‰æºä¼˜å…ˆçº§æ’åº
        # æ–°å¢ï¼šå¹´ä»½åŒ¹é…ä¼˜å…ˆçº§
        all_results.sort(
            key=lambda item: (
                # ä¼˜å…ˆçº§1ï¼šå¹´ä»½åŒ¹é…ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼Œé¿å…ä¸‹è½½é”™è¯¯å¹´ä»½çš„ç‰ˆæœ¬ï¼‰
                10000 if year is not None and item.year is not None and item.year == year else 0,
                # ä¼˜å…ˆçº§2ï¼šå®Œå…¨åŒ¹é…çš„æ ‡é¢˜
                1000 if item.title.strip() == main_title.strip() else 0,
                # ä¼˜å…ˆçº§3ï¼šæ ‡é¢˜ç›¸ä¼¼åº¦
                fuzz.token_set_ratio(main_title, item.title),
                # ä¼˜å…ˆçº§4ï¼šæƒ©ç½šå¹´ä»½ä¸åŒ¹é…çš„ç»“æœ
                -1000 if year is not None and item.year is not None and item.year != year else 0,
                # ä¼˜å…ˆçº§5ï¼šæºä¼˜å…ˆçº§
                -provider_order.get(item.provider, 999)
            ),
            reverse=True # æŒ‰å¾—åˆ†ä»é«˜åˆ°ä½æ’åº
        )

        # æ·»åŠ æ’åºåçš„è°ƒè¯•æ—¥å¿—
        logger.info(f"æ’åºåçš„å‰5ä¸ªç»“æœ:")
        for i, item in enumerate(all_results[:5]):
            title_match = "âœ“" if item.title.strip() == main_title.strip() else "âœ—"
            year_match = "âœ“" if year is not None and item.year is not None and item.year == year else ("âœ—" if year is not None and item.year is not None else "-")
            similarity = fuzz.token_set_ratio(main_title, item.title)
            year_info = f"å¹´ä»½: {item.year}" if item.year else "å¹´ä»½: æœªçŸ¥"
            logger.info(f"  {i+1}. '{item.title}' (Provider: {item.provider}, Type: {item.type}, {year_info}, å¹´ä»½åŒ¹é…: {year_match}, æ ‡é¢˜åŒ¹é…: {title_match}, ç›¸ä¼¼åº¦: {similarity}%)")
        # å€™é€‰é¡¹é€‰æ‹©ï¼šæ£€æŸ¥æ˜¯å¦å¯ç”¨é¡ºå»¶æœºåˆ¶
        if not all_results:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æœç´¢ç»“æœ")

        # ä½¿ç”¨AIMatcherManagerè¿›è¡ŒAIåŒ¹é…
        ai_selected_index = None
        if await ai_matcher_manager.is_enabled():
            try:
                # æ„å»ºæŸ¥è¯¢ä¿¡æ¯
                query_info = {
                    "title": main_title,
                    "season": payload.season,
                    "episode": payload.episode,
                    "year": year,  # ä¿®æ­£ï¼šä½¿ç”¨ä»å…ƒæ•°æ®è·å–çš„yearå˜é‡ï¼Œè€Œä¸æ˜¯payload.year
                    "type": media_type
                }

                # è·å–ç²¾ç¡®æ ‡è®°ä¿¡æ¯
                favorited_info = {}
                async with scraper_manager._session_factory() as ai_session:
                    from ..orm_models import AnimeSource
                    from sqlalchemy import select

                    for result in all_results:
                        # æŸ¥æ‰¾æ˜¯å¦æœ‰ç›¸åŒproviderå’ŒmediaIdçš„æºè¢«æ ‡è®°
                        stmt = (
                            select(AnimeSource.isFavorited)
                            .where(
                                AnimeSource.providerName == result.provider,
                                AnimeSource.mediaId == result.mediaId
                            )
                            .limit(1)
                        )
                        result_row = await ai_session.execute(stmt)
                        is_favorited = result_row.scalar_one_or_none()
                        if is_favorited:
                            key = f"{result.provider}:{result.mediaId}"
                            favorited_info[key] = True

                # ä½¿ç”¨AIMatcherManagerè¿›è¡ŒåŒ¹é…
                ai_selected_index = await ai_matcher_manager.select_best_match(
                    query_info, all_results, favorited_info
                )

                if ai_selected_index is not None:
                    logger.info(f"AIåŒ¹é…æˆåŠŸé€‰æ‹©: ç´¢å¼• {ai_selected_index}")
                else:
                    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ä¼ ç»ŸåŒ¹é…å…œåº•
                    ai_fallback_enabled = (await config_manager.get("aiFallbackEnabled", "true")).lower() == 'true'
                    if ai_fallback_enabled:
                        logger.info("AIåŒ¹é…æœªæ‰¾åˆ°åˆé€‚ç»“æœï¼Œé™çº§åˆ°ä¼ ç»ŸåŒ¹é…")
                    else:
                        logger.warning("AIåŒ¹é…æœªæ‰¾åˆ°åˆé€‚ç»“æœï¼Œä¸”ä¼ ç»ŸåŒ¹é…å…œåº•å·²ç¦ç”¨ï¼Œå°†ä¸ä½¿ç”¨ä»»ä½•ç»“æœ")

            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨ä¼ ç»ŸåŒ¹é…å…œåº•
                ai_fallback_enabled = (await config_manager.get("aiFallbackEnabled", "true")).lower() == 'true'
                if ai_fallback_enabled:
                    logger.error(f"AIåŒ¹é…å¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»ŸåŒ¹é…: {e}", exc_info=True)
                else:
                    logger.error(f"AIåŒ¹é…å¤±è´¥ï¼Œä¸”ä¼ ç»ŸåŒ¹é…å…œåº•å·²ç¦ç”¨: {e}", exc_info=True)
                ai_selected_index = None

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¤–éƒ¨æ§åˆ¶APIé¡ºå»¶æœºåˆ¶
        fallback_enabled = (await config_manager.get("externalApiFallbackEnabled", "false")).lower() == 'true'

        best_match = None

        # å¦‚æœAIé€‰æ‹©äº†ç»“æœï¼Œä½¿ç”¨AIé€‰æ‹©çš„ç»“æœ
        if ai_selected_index is not None:
            best_match = all_results[ai_selected_index]
            logger.info(f"ä½¿ç”¨AIé€‰æ‹©çš„æœ€ä½³åŒ¹é…: {best_match.title} (Provider: {best_match.provider})")
        # å¦åˆ™ï¼Œå¦‚æœå¯ç”¨äº†AIåŒ¹é…ï¼Œæ£€æŸ¥ä¼ ç»ŸåŒ¹é…å…œåº•
        elif await ai_matcher_manager.is_enabled():
            ai_fallback_enabled = (await config_manager.get("aiFallbackEnabled", "true")).lower() == 'true'
            if ai_fallback_enabled and all_results:
                # ä¼ ç»ŸåŒ¹é…ï¼šéªŒè¯ç¬¬ä¸€ä¸ªç»“æœæ˜¯å¦æ»¡è¶³æ¡ä»¶
                first_result = all_results[0]
                type_matched = first_result.type == media_type.value if hasattr(media_type, 'value') else first_result.type == str(media_type)
                similarity = fuzz.token_set_ratio(main_title, first_result.title)

                # å¿…é¡»æ»¡è¶³ï¼šç±»å‹åŒ¹é… AND ç›¸ä¼¼åº¦ >= 70%
                if type_matched and similarity >= 70:
                    best_match = first_result
                    logger.info(f"AIåŒ¹é…æœªé€‰æ‹©ç»“æœï¼Œä½¿ç”¨ä¼ ç»ŸåŒ¹é…: {first_result.title} (Provider: {first_result.provider}, "
                               f"ç±»å‹åŒ¹é…: âœ“, ç›¸ä¼¼åº¦: {similarity}%)")
                else:
                    best_match = None
                    logger.warning(f"ä¼ ç»ŸåŒ¹é…å¤±è´¥: ç¬¬ä¸€ä¸ªç»“æœä¸æ»¡è¶³æ¡ä»¶ (ç±»å‹åŒ¹é…: {'âœ“' if type_matched else 'âœ—'}, "
                                 f"ç›¸ä¼¼åº¦: {similarity}%, è¦æ±‚: â‰¥70%)")
            else:
                logger.warning("AIåŒ¹é…æœªé€‰æ‹©ç»“æœï¼Œä¸”ä¼ ç»ŸåŒ¹é…å…œåº•å·²ç¦ç”¨ï¼Œå°†ä¸ä½¿ç”¨ä»»ä½•ç»“æœ")
                best_match = None
        # å¦‚æœæœªå¯ç”¨AIåŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨ä¼ ç»ŸåŒ¹é…
        else:
            if all_results:
                # ä¼ ç»ŸåŒ¹é…ï¼šéªŒè¯ç¬¬ä¸€ä¸ªç»“æœæ˜¯å¦æ»¡è¶³æ¡ä»¶
                first_result = all_results[0]
                type_matched = first_result.type == media_type.value if hasattr(media_type, 'value') else first_result.type == str(media_type)
                similarity = fuzz.token_set_ratio(main_title, first_result.title)

                # å¿…é¡»æ»¡è¶³ï¼šç±»å‹åŒ¹é… AND ç›¸ä¼¼åº¦ >= 70%
                if type_matched and similarity >= 70:
                    best_match = first_result
                    logger.info(f"ä½¿ç”¨ä¼ ç»ŸåŒ¹é…: {first_result.title} (Provider: {first_result.provider}, "
                               f"ç±»å‹åŒ¹é…: âœ“, ç›¸ä¼¼åº¦: {similarity}%)")
                else:
                    best_match = None
                    logger.warning(f"ä¼ ç»ŸåŒ¹é…å¤±è´¥: ç¬¬ä¸€ä¸ªç»“æœä¸æ»¡è¶³æ¡ä»¶ (ç±»å‹åŒ¹é…: {'âœ“' if type_matched else 'âœ—'}, "
                                 f"ç›¸ä¼¼åº¦: {similarity}%, è¦æ±‚: â‰¥70%)")
            else:
                best_match = None
                logger.warning("ä¼ ç»ŸåŒ¹é…å¤±è´¥: æ²¡æœ‰æœç´¢ç»“æœ")

        # å¦‚æœæ²¡æœ‰é€‰æ‹©ä»»ä½•ç»“æœï¼ŒæŠ›å‡ºé”™è¯¯
        if not best_match:
            raise ValueError("æœªèƒ½é€‰æ‹©åˆé€‚çš„æœç´¢ç»“æœ")

        # å¦‚æœæ²¡æœ‰ä»å…ƒæ•°æ®è·å–åˆ°imageUrl,ä½¿ç”¨æœç´¢ç»“æœçš„imageUrl
        if not image_url and best_match.imageUrl:
            image_url = best_match.imageUrl
            logger.info(f"ä½¿ç”¨æœç´¢ç»“æœçš„æµ·æŠ¥: {image_url}")

        # é¡ºå»¶æœºåˆ¶ï¼šå¦‚æœå¯ç”¨ï¼Œå°è¯•éªŒè¯ç¬¬ä¸€é›†æ˜¯å¦å¯ç”¨
        if fallback_enabled:
            logger.info("å¤–éƒ¨æ§åˆ¶APIé¡ºå»¶æœºåˆ¶å·²å¯ç”¨ï¼Œæ­£åœ¨éªŒè¯ç¬¬ä¸€é›†å¯ç”¨æ€§...")
            await progress_callback(60, "æ­£åœ¨éªŒè¯ç¬¬ä¸€é›†å¯ç”¨æ€§...")

            # è·å–ç¬¬ä¸€é›†çš„å¼¹å¹•
            try:
                # ä½¿ç”¨ scraper_manager è·å–ç¬¬ä¸€é›†çš„å¼¹å¹•
                scraper = scraper_manager.get_scraper(best_match.provider)
                if not scraper:
                    raise ValueError(f"æœªæ‰¾åˆ° {best_match.provider} çš„ scraper")

                # è·å–åˆ†é›†åˆ—è¡¨
                episodes_list = await scraper.get_episodes(best_match.mediaId, db_media_type=media_type)

                # åº”ç”¨è¯†åˆ«è¯è½¬æ¢
                current_episode_index = 1
                if title_recognition_manager:
                    _, converted_episode, _, _, _ = await title_recognition_manager.apply_title_recognition(best_match.title, current_episode_index, season)
                    if converted_episode is not None:
                        current_episode_index = converted_episode
                        logger.info(f"è¯†åˆ«è¯è½¬æ¢: åŸå§‹é›†æ•° 1 -> è½¬æ¢åé›†æ•° {current_episode_index}")

                # æŸ¥æ‰¾ç¬¬ä¸€é›†
                first_episode = None
                for ep in episodes_list:
                    if ep.episodeIndex == current_episode_index:
                        first_episode = ep
                        break

                if not first_episode:
                    logger.warning(f"æœªæ‰¾åˆ°ç¬¬ä¸€é›† (ç´¢å¼•: {current_episode_index})ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå€™é€‰æº...")
                    # å°è¯•ä¸‹ä¸€ä¸ªå€™é€‰æº
                    for idx, candidate in enumerate(all_results[1:], start=1):
                        logger.info(f"å°è¯•å€™é€‰æº {idx}: {candidate.title} (Provider: {candidate.provider})")
                        try:
                            scraper = scraper_manager.get_scraper(candidate.provider)
                            if not scraper:
                                logger.warning(f"æœªæ‰¾åˆ° {candidate.provider} çš„ scraperï¼Œè·³è¿‡")
                                continue

                            episodes_list = await scraper.get_episodes(candidate.mediaId, api_key=api_key)

                            # åº”ç”¨è¯†åˆ«è¯è½¬æ¢
                            current_episode_index = 1
                            if title_recognition_manager:
                                _, converted_episode, _, _, _ = await title_recognition_manager.apply_title_recognition(candidate.title, current_episode_index, season)
                                if converted_episode is not None:
                                    current_episode_index = converted_episode
                                    logger.info(f"è¯†åˆ«è¯è½¬æ¢: åŸå§‹é›†æ•° 1 -> è½¬æ¢åé›†æ•° {current_episode_index}")

                            first_episode = None
                            for ep in episodes_list:
                                if ep.episodeIndex == current_episode_index:
                                    first_episode = ep
                                    break

                            if first_episode:
                                logger.info(f"å€™é€‰æº {idx} æ‰¾åˆ°ç¬¬ä¸€é›†ï¼Œä½¿ç”¨è¯¥æº")
                                best_match = candidate
                                break
                            else:
                                logger.warning(f"å€™é€‰æº {idx} æœªæ‰¾åˆ°ç¬¬ä¸€é›†ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª")
                        except Exception as e:
                            logger.error(f"éªŒè¯å€™é€‰æº {idx} æ—¶å‡ºé”™: {e}")
                            continue

                    if not first_episode:
                        raise ValueError("æ‰€æœ‰å€™é€‰æºå‡æœªæ‰¾åˆ°ç¬¬ä¸€é›†ï¼Œæ— æ³•å¯¼å…¥")

                # è·å–ç¬¬ä¸€é›†çš„å¼¹å¹•
                comments = await scraper.get_comments(first_episode.episodeId)
                if not comments:
                    logger.warning(f"ç¬¬ä¸€é›† (ç´¢å¼•: {current_episode_index}) æ²¡æœ‰å¼¹å¹•ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå€™é€‰æº...")
                    # å°è¯•ä¸‹ä¸€ä¸ªå€™é€‰æº
                    for idx, candidate in enumerate(all_results[1:], start=1):
                        logger.info(f"å°è¯•å€™é€‰æº {idx}: {candidate.title} (Provider: {candidate.provider})")
                        try:
                            scraper = scraper_manager.get_scraper(candidate.provider)
                            if not scraper:
                                logger.warning(f"æœªæ‰¾åˆ° {candidate.provider} çš„ scraperï¼Œè·³è¿‡")
                                continue

                            episodes_list = await scraper.get_episodes(candidate.mediaId, db_media_type=media_type)

                            # åº”ç”¨è¯†åˆ«è¯è½¬æ¢
                            current_episode_index = 1
                            if title_recognition_manager:
                                _, converted_episode, _, _, _ = await title_recognition_manager.apply_title_recognition(candidate.title, current_episode_index, season)
                                if converted_episode is not None:
                                    current_episode_index = converted_episode
                                    logger.info(f"è¯†åˆ«è¯è½¬æ¢: åŸå§‹é›†æ•° 1 -> è½¬æ¢åé›†æ•° {current_episode_index}")

                            first_episode = None
                            for ep in episodes_list:
                                if ep.episodeIndex == current_episode_index:
                                    first_episode = ep
                                    break

                            if not first_episode:
                                logger.warning(f"å€™é€‰æº {idx} æœªæ‰¾åˆ°ç¬¬ä¸€é›†ï¼Œè·³è¿‡")
                                continue

                            comments = await scraper.get_comments(first_episode.episodeId)
                            if comments:
                                logger.info(f"å€™é€‰æº {idx} æ‰¾åˆ°ç¬¬ä¸€é›†å¼¹å¹•ï¼Œä½¿ç”¨è¯¥æº")
                                best_match = candidate
                                break
                            else:
                                logger.warning(f"å€™é€‰æº {idx} ç¬¬ä¸€é›†æ²¡æœ‰å¼¹å¹•ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª")
                        except Exception as e:
                            logger.error(f"éªŒè¯å€™é€‰æº {idx} æ—¶å‡ºé”™: {e}")
                            continue

                    if not comments:
                        raise ValueError("æ‰€æœ‰å€™é€‰æºçš„ç¬¬ä¸€é›†å‡æ²¡æœ‰å¼¹å¹•ï¼Œæ— æ³•å¯¼å…¥")

                logger.info(f"ç¬¬ä¸€é›†éªŒè¯æˆåŠŸï¼Œå¼¹å¹•æ•°é‡: {len(comments)}")
            except Exception as e:
                logger.error(f"éªŒè¯ç¬¬ä¸€é›†æ—¶å‡ºé”™: {e}")
                raise ValueError(f"éªŒè¯ç¬¬ä¸€é›†å¤±è´¥: {str(e)}")

        # 5. åˆ›å»ºå¯¼å…¥ä»»åŠ¡
        await progress_callback(70, "æ­£åœ¨åˆ›å»ºå¯¼å…¥ä»»åŠ¡...")

        # è§£æå¤šé›†å‚æ•°
        selected_episodes = None
        if payload.episode:
            selected_episodes = parse_episode_ranges(payload.episode)
            logger.info(f"è§£æé›†æ•°å‚æ•° '{payload.episode}' -> {selected_episodes}")

        # åº”ç”¨å­˜å‚¨åå¤„ç†è§„åˆ™
        final_title = best_match.title
        if title_recognition_manager:
            processed_title, _, _, postprocessing_applied = await title_recognition_manager.apply_storage_postprocessing(best_match.title, season)
            if postprocessing_applied:
                final_title = processed_title
                logger.info(f"âœ“ åº”ç”¨å­˜å‚¨åå¤„ç†: '{best_match.title}' -> '{final_title}'")
            else:
                logger.info(f"â—‹ å­˜å‚¨åå¤„ç†æœªç”Ÿæ•ˆ: '{best_match.title}'")

        task_coro = lambda s, cb: generic_import_task(
            provider=best_match.provider, mediaId=best_match.mediaId,
            animeTitle=final_title, mediaType=best_match.type,
            season=season, year=best_match.year,
            config_manager=config_manager, metadata_manager=metadata_manager,
            currentEpisodeIndex=None, imageUrl=image_url,
            doubanId=douban_id, tmdbId=tmdb_id, imdbId=imdb_id, tvdbId=tvdb_id, bangumiId=bangumi_id,
            progress_callback=cb, session=s, manager=scraper_manager, task_manager=task_manager,
            rate_limiter=rate_limiter,
            title_recognition_manager=title_recognition_manager,
            is_fallback=False,
            selectedEpisodes=selected_episodes
        )

        # æ„å»ºä»»åŠ¡æ ‡é¢˜
        title_parts = [f"è‡ªåŠ¨å¯¼å…¥: {final_title}"]
        if season is not None:
            title_parts.append(f"S{season:02d}")
        if payload.episode:
            title_parts.append(f"E{payload.episode}")
        task_title = " ".join(title_parts)

        # æ„å»ºunique_key
        unique_key_parts = ["import", best_match.provider, best_match.mediaId]
        if season is not None:
            unique_key_parts.append(f"s{season}")
        if payload.episode:
            unique_key_parts.append(f"e{payload.episode}")
        unique_key = "-".join(unique_key_parts)

        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        task_parameters = {
            "provider": best_match.provider,
            "mediaId": best_match.mediaId,
            "animeTitle": final_title,
            "mediaType": best_match.type,
            "season": season,
            "year": best_match.year,
            "currentEpisodeIndex": None,
            "selectedEpisodes": selected_episodes,
            "imageUrl": image_url,
            "doubanId": douban_id,
            "tmdbId": tmdb_id,
            "imdbId": imdb_id,
            "tvdbId": tvdb_id,
            "bangumiId": bangumi_id
        }

        execution_task_id, _ = await task_manager.submit_task(
            task_coro,
            task_title,
            unique_key=unique_key,
            task_type="generic_import",
            task_parameters=task_parameters
        )
        timer.finish()  # æ‰“å°è®¡æ—¶æŠ¥å‘Š
        final_message = f"å·²ä¸ºæœ€ä½³åŒ¹é…æºåˆ›å»ºå¯¼å…¥ä»»åŠ¡ã€‚æ‰§è¡Œä»»åŠ¡ID: {execution_task_id}"
        raise TaskSuccess(final_message)
    finally:
        if api_key:
            await scraper_manager.release_search_lock(api_key)
            logger.info(f"è‡ªåŠ¨å¯¼å…¥ä»»åŠ¡å·²ä¸º API key é‡Šæ”¾æœç´¢é”ã€‚")

